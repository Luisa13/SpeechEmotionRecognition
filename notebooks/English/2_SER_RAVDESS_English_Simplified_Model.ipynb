{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_SER_RAVDESS_English_Simplified_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3fNJXlKigbWQ"},"source":["\n","# **EMOTIONAL SPEECH RECOGNITION ANALYSIS**\n","\n","## English\n","El siguiente analisis presenta una clasificacion emocional del discurso, reconociendo 7 emociones (enfado, sorpresa, asco, miedo, felicidad, tristeza y neutral) para el idioma **ingles**.\n","\n","Luisa Sanchez Avivar\n","    _luisasanavi@gmail.com_"]},{"cell_type":"code","metadata":{"id":"W0EqYek8gFl8","executionInfo":{"status":"ok","timestamp":1618094178918,"user_tz":-120,"elapsed":4835,"user":{"displayName":"Luisa Sánchez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtRqDcfvLGMFQUrZnMWjIFyBNK4uoa7Hlh1L2O8w=s64","userId":"11510390526428860545"}}},"source":["# IMPORT LIBRARIES\n","# Processing\n","import librosa\n","import librosa.display\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","\n","# Files\n","import os\n","\n","# Machine Learning\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from sklearn.metrics import confusion_matrix\n","import keras\n","from keras.utils import np_utils, to_categorical\n","from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","from keras.models import Sequential, Model, model_from_json\n","from keras.layers import Dense, Embedding, LSTM\n","from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n","\n","# ####### TEST ####### \n","# Scipy\n","from scipy import signal\n","from scipy.io import wavfile\n","\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgN2moypkFrm","executionInfo":{"status":"ok","timestamp":1618094178919,"user_tz":-120,"elapsed":4825,"user":{"displayName":"Luisa Sánchez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtRqDcfvLGMFQUrZnMWjIFyBNK4uoa7Hlh1L2O8w=s64","userId":"11510390526428860545"}}},"source":["AUDIO_DATA_PATH = 'data/'\n","GPATH = '/content/drive/My Drive/Master/Asignaturas/2 Cuatrimestre/Proyecto/Code/'\n","SAMPLE_FILE = \"03-01-01-01-01-01-01.wav\"\n","\n","# Maps\n","EMOTION_MAP = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}\n","INTENSITY_MAP = {1:'normal', 2:'strong'}\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GRdYkmCiUDF"},"source":["## **1. CARGA DE DATOS**\n","Para este analisis utilizare el dataset de [RAVDESS](https://zenodo.org/record/1188976.) (Ryerson Audio-Visual Database of Emotional Speech and Song), el cual contiene 7356 archivos (24.8 GB) entre los cuales podemos encontrar 3 modalidades: **solo audio** (en 16 bit, 48 kHz y en formato .wav), **audio-video** (720p H.264, AAC 48kHz, .mp4) y **solo video** sin sonido.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"C7yFtFm7uj7q"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVzDzuPsiXfS"},"source":["dir_list = os.listdir(GPATH + AUDIO_DATA_PATH)\n","dir_list.sort()\n","\n","print(dir_list)\n","\n","emotion = []\n","gender = []\n","intensity = []\n","path = []\n","\n","# Extraemos de cada archivo de sonido sus datos\n","for dir in dir_list:\n","  path_dir = os.listdir(GPATH + AUDIO_DATA_PATH + dir) # todos los archivos de audios asociados a un directorio\n","  for filepath in path_dir:\n","    info_vector = filepath.split('.')[0].split('-')\n","    n_emotion = int(info_vector[2])\n","    n_gender = int(info_vector[6])\n","    n_intensity = int(info_vector[3])\n","    str_path = GPATH + AUDIO_DATA_PATH + dir + '/' + str(filepath)\n","    path.append(str_path)\n","    emotion.append(n_emotion)\n","    intensity.append(n_intensity)\n","    if n_gender%2 == 0:\n","      gender.append('female')\n","    else:\n","      gender.append('male')\n","\n","# Construimos el data frame\n","EnglishSpeech_df = pd.DataFrame(columns=['emotion', 'gender', 'intensity', 'path'])\n","EnglishSpeech_df['emotion'] = emotion\n","EnglishSpeech_df['gender'] = gender\n","EnglishSpeech_df['intensity'] = intensity\n","EnglishSpeech_df['path'] = path\n","EnglishSpeech_df['emotion'] = EnglishSpeech_df['emotion'].map(EMOTION_MAP) \n","EnglishSpeech_df['intensity'] = EnglishSpeech_df['intensity'].map(INTENSITY_MAP)\n","\n","\n","print(\"Size of the dataset: {} \\n\".format(len(EnglishSpeech_df)))\n","class_distribution = EnglishSpeech_df['emotion'].value_counts()\n","print(class_distribution)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RwUa-1DN8em9"},"source":["# Imprimimos la distribucion de las clases\n","\n","df_aux = pd.DataFrame()\n","df_aux['emotion'] = list(class_distribution.keys())\n","df_aux['count']  = list(class_distribution)\n","fig, axs = plt.subplots(figsize=(15, 5))\n","axs = sns.barplot(x = 'emotion', y = 'count', color = '#2962FF', data = df_aux)\n","axs.set_title('Distribucion')\n","axs.set_xticklabels(axs.get_xticklabels(),rotation=45, fontsize = 12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U-6BFxg4otYl"},"source":["# Imprimimos una muestra de 10 filas aleatorias\n","EnglishSpeech_df.sample(n = 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LGoJBn6_iWQi"},"source":["# **2. EXPLORACION DE LOS DATOS**"]},{"cell_type":"markdown","metadata":{"id":"g5RyAVX_MOg4"},"source":["### **2.1 EXPOSICION DE UNA MUESTRA ALEATORIA**\n","\n"]},{"cell_type":"code","metadata":{"id":"JFJ3VgYKteHO"},"source":["\n","def plot_audio_waveform(audio_sample):\n","  '''\n","  Muestra la forma de la onda sonora a partir de una muestra.\n","\n","  Arguments\n","  ---------\n","  audio_sample: dataFrame\n","    Muestra de audio \n","  \n","  '''\n","  sample, sampling_rate = librosa.load(audio_sample)\n","  plt.figure(figsize=(12, 4))\n","  librosa.display.waveplot(sample, sr=sampling_rate)\n","  print(len(sample))\n","  \n","\n","def log_specgram(audio_sample):\n","  '''\n","  Muestra el espectograma a partir de una muestra de audio\n","\n","  Arguments\n","  ---------\n","  audio_sample: dataFrame\n","    Muestra de audio\n","  '''\n","  sample, sampling_rate = librosa.load(audio_sample)\n","  return  __log_specgram(sample, sampling_rate)\n","\n","\n","def __log_specgram(audio, sample_rate, window_size=20,\n","                 step_size=10, eps=1e-10):\n","  '''\n","  Imprime el especograma de una muestra de audio en una ventana de tiempo\n","  \n","  Arguments\n","  ---------\n","  audio: np.ndarray\n","    Muestra de audio\n","  sample_rate: int, optional\n","    Frecuencia de muestreo de la muestra de audio\n","  window_size: int, optional\n","\n","  step_size: int, optional\n","  \n","  eps: int, optional\n","  '''\n","  nperseg = int(round(window_size * sample_rate / 1e3))\n","  noverlap = int(round(step_size * sample_rate / 1e3))\n","  freqs, times, spec = signal.spectrogram(audio,\n","                                    fs=sample_rate,\n","                                    window='hann',\n","                                    nperseg=nperseg,\n","                                    noverlap=noverlap,\n","                                    detrend=False)\n","  return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YfKeAyHqkDC5"},"source":["random_sample = EnglishSpeech_df.path[random.randint(0, len(EnglishSpeech_df))]\n","print(\"Random audio sample: {}\".format(random_sample))\n","# Dibuja la grafica de la onda sonora\n","plot_audio_waveform(random_sample)\n","\n","# Dibuja el espectograma\n","freqs, times, spectrogram = log_specgram(random_sample)\n","\n","# Muestra la firgura\n","plt.figure(figsize=(30, 2))\n","plt.imshow(spectrogram)\n","plt.xlabel('Seg')\n","plt.ylabel('Hz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JmMpt9hvWqS"},"source":["\n","# samplingFrequency, signalData = wavfile.read(random_sample)\n","# plt.plot(sample)\n","# plt.specgram(sample, Fs=sampling_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4mMFHOpOMVSj"},"source":["### **2.2 COMPARATIVA DE TODAS LAS EMOCIONES**\n","Ahora que hemos conseguido extraer la grafica de una determinada muestra de sonido, vamos a comparar la graficas correspondientes a las muestras de sonido de distintas emociones respectivamente. Para ello vamos a distinguir por genero, ya que podria suponer una diferencia."]},{"cell_type":"code","metadata":{"id":"fJ34cCuf4hpn"},"source":["def plot_all_emotion_waveforms(gender, rows = 3, cols = 3):\n","  '''\n","  Muestra los graficos para todas las emociones con su correspondiente \n","  etiqueta del dataset para un mismo genero (female/ male)\n","  \n","  Arguments\n","  --------- \n","    gender: str\n","      Genero del actor en el audio\n","    rows:  int, optional\n","      Filas en las que se muestra. 5 por defecto\n","    cols: int, optional\n","      Columnas en las que se muestra. 2 por defecto\n","    \n","  '''\n","  labels = list(EnglishSpeech_df['emotion'].unique())\n","  files = dict()\n","  if not gender:\n","    return -1\n","\n","  # Seleccionamos una muestra aleatoria correspondiente a cada emocion\n","  for label in labels:\n","    # Escogemos un archivo de audio al azar que cumpla estas dos condiciones\n","    index = EnglishSpeech_df[(EnglishSpeech_df['emotion'] == label) & \n","                            (EnglishSpeech_df['gender'] == gender)].sample(n = 1).index[0]\n","    emotion_file = EnglishSpeech_df.iloc[index].path\n","    files[label] = emotion_file\n","\n","  # Mostramos las diferentes waveforms\n","  fig = plt.figure(figsize=(15,15))\n","  fig.subplots_adjust(hspace=1, wspace=0.4)\n","  for i, label in enumerate(labels):\n","    wfigure = files[label]\n","    fig.add_subplot(rows, cols, i+1)\n","    plt.title(label.capitalize())\n","    data_sample, sample_rate = librosa.load(wfigure)\n","    librosa.display.waveplot(data_sample, sr= sample_rate)\n","\n","    ## TODO: return image figure\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7dMgx4YlF3tn"},"source":["### 2.2.1 COMPARATIVA DE EMOCIONES PARA HOMBRE\n","En primer lugar vemos que aspecto tienen las emociones para la **voz masculina**"]},{"cell_type":"code","metadata":{"id":"ptPieGG9LFNa"},"source":["plot_all_emotion_waveforms('male')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d43RhozjGDsg"},"source":["\n","### 2.2.2 COMPARATIVA DE EMOCIONES DE MUJER\n","Vemos el aspecto que tienen las emociones para la **voz femenina**\n"]},{"cell_type":"code","metadata":{"id":"S6TC2shSGEC5"},"source":["plot_all_emotion_waveforms('female')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ud2E03YLveT"},"source":["Se puede observar en los resultados, que la se;al asociada a cada una de las emociones presenta diferencias con respecto a la voz femenina y la masculina. A simple vista no podemos hablar de unos cambios constantes en determinados puntos. Vemos que la *felicidad* masculina y femennina es expresada de una manera similar, al contrario, por ejemplo, de lo que ocurre con el *miedo*, donde en su version masculina presenta mas saltos entre sus picos de frecuencia. por lo que es algo que deberemos tener en cuenta en este analisis"]},{"cell_type":"markdown","metadata":{"id":"_xyfeqdHoV1d"},"source":["# **3. EXTRACCION DE CARACTERISTICAS**"]},{"cell_type":"markdown","metadata":{"id":"5pOjXM6UM7Gu"},"source":["### **3.1 EXTRACCION DE CARACTERISTICAS CON MFCC**"]},{"cell_type":"code","metadata":{"id":"-Ur-eq3SoVI_"},"source":["def get_features(df):\n","  '''\n","  Extrae las caracteristicas de un conjunto de pistas de audio a \n","  partir de un dataframe usando librosa\n","\n","  Aguments\n","  ---------\n","    df : dataframe\n","    Dataframe que contiene el path donde se encuentra la pista de audio\n","\n","  Return\n","  -------\n","   data: np.array \n","   Caracteristicas extraidas\n","\n","  '''\n","  bar_data_range = tqdm(range(len(df)))\n","  data = pd.DataFrame(columns = ['data'])\n","  for index in bar_data_range:\n","    data_features = get_features_single_file(df.path[index])\n","    data.loc[index] = [data_features]\n","\n","  return data\n","\n","\n","def get_features_single_file(pathfile):\n","  '''\n","  Extrae las caracteristicas  de una unica pista de audio usando MFCC \n","  a traves de librosa.\n","  \n","  Aguments\n","  ---------\n","    pathfile: str \n","      Path del archivo del que se extraeran las caracteristicas\n","\n","  Return\n","  -------\n","    data_features\n","\n","  '''\n","  X, sample_rate = librosa.load(pathfile, duration=2.5, sr=22050*2, offset=0.5)\n","  mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n","  data_features = np.mean(mfcc, axis = 0)\n","\n","  return data_features\n","\n","\n","\n","def get_random_emotion(df, emotion):\n","  '''\n","  Devuelve el path de un archivo de audio aleatorio a partir de un dataframe.\n","\n","  Aguments\n","  ---------\n","  df: dataframe\n","    Caracteristicas de la muestra de audio organizadas por emociones\n","  emotion: str\n","    Nombre de la emocion \n","    \n","  Return\n","  -------\n","  '''\n","  if 'emotion' not in df:\n","    return -1\n","\n","  aux_df = df[df['emotion'] == emotion]\n","  item = random.choice(aux_df.index.to_list())\n","  path = aux_df.path[item]\n","\n","  return path\n","\n","def plot_waves_comparative(df1, df2, df1_title = 'Wave 1', df2_title = 'Wave 2', title_ = 'Title'):\n","  '''\n","  Imprime la grafica de dos waveforms a partir de sus caracteristicas.\n","\n","  Aguments\n","  ---------\n","  df1: dataframe\n","    Caracteristicas de la primera muestra de audio\n","  df2: dataframe\n","    Caracteristicas de la segunda muestra de audio\n","  df1_title: str, optional\n","    Titulo para la primera grafica\n","  df2_title: str, optional\n","    Titulo para la segunda grafica \n","  title_: str, optional\n","    Titulo para la figura\n","\n","  '''\n","  plt.figure(figsize=(20, 15))\n","  plt.subplot(3,1,1)\n","  plt.title(title_)\n","  plt.plot(df1, label= df1_title)\n","  plt.plot(df2, label= df2_title)\n","  plt.legend()\n","\n","\n","def plot_all_comparative_waveforms(rows = 3, cols = 3):\n","  '''\n","\n","\n","  Aguments\n","  ---------\n","  Return\n","  -------\n","  '''\n","\n","  labels = list(EnglishSpeech_df['emotion'].unique())\n","  features_dict = dict()\n","  for label in labels:\n","    # Female\n","    path = get_random_emotion(EnglishSpeech_df[EnglishSpeech_df['gender'] == 'female'], label)\n","    # female_feat = get_features_single_file(path)\n","    key = 'female_' + label\n","    features_dict[key] = get_features_single_file(path)\n","    # Male\n","    path = get_random_emotion(EnglishSpeech_df[EnglishSpeech_df['gender'] == 'male'], label)\n","    # male_feat = get_features_single_file(path)\n","    key = 'male_' + label\n","    features_dict[key] = get_features_single_file(path)\n","\n","  # Mostramos las diferentes waveforms\n","  fig = plt.figure(figsize=(15,15))\n","  fig.subplots_adjust(hspace=0.4, wspace=0.4)\n","  for i, label in enumerate(labels):\n","    key = 'female_' + label\n","    df_fem = features_dict[key]\n","    key = 'male_' + label\n","    df_mal = features_dict[key]\n","    fig.add_subplot(rows, cols, i+1)\n","    plt.title(label)\n","    plt.plot(df_fem, label= 'female')\n","    plt.plot(df_mal, label= 'male')\n","    plt.legend()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIQ3mUy_6agy"},"source":["# Guardamos las caracteristicas a partir de la estructura que hemos construido antes\n","features_data = get_features(EnglishSpeech_df)\n","print(features_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E37M9px4O5Ha"},"source":["# features_data.head()\n","f_df = pd.DataFrame(features_data['data'].values.tolist())\n","f_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S78fodTazvOV"},"source":["features_complete_df = pd.concat((f_df, EnglishSpeech_df['gender'], EnglishSpeech_df['emotion']), axis = 1)\n","features_complete_df = features_complete_df.fillna(0)\n","\n","# Barajamos las filas para imrpimir una muestra \n","random_aux = shuffle(features_complete_df)\n","random_aux.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSySO6rS-viY"},"source":["### Division del dataframe\n","Vamos a dividir el dataframe por el genero, para comprobar como afecta este a la prosodia en la voz femenina y masculina por separadas, dadas las mismas emociones en el analisis"]},{"cell_type":"code","metadata":{"id":"wd2WIaCm9tbb"},"source":["male_features_df = features_complete_df[(features_complete_df['gender'] == 'male')]\n","female_features_df = features_complete_df[(features_complete_df['gender'] == 'female')]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RVOZFXA67mSf"},"source":["### Comparativa\n","Ahora que podemos extraer las caracteristicas de las señales, vamos a comparar la misma emocion, (por ejemplo *enfado*) en la voz femenina y la masculina.\n"]},{"cell_type":"code","metadata":{"id":"3eTJLV26_aeS"},"source":["# Extraemos una muestra aleatoria para la emocion: enfado \n","# Female\n","path = get_random_emotion(EnglishSpeech_df[EnglishSpeech_df['gender'] == 'female'], 'angry')\n","female_feat = get_features_single_file(path)\n","# print(len(female_feat))\n","# Male\n","path = get_random_emotion(EnglishSpeech_df[EnglishSpeech_df['gender'] == 'male'], 'angry')\n","male_feat = get_features_single_file(path)\n","# print(len(male_feat))\n","\n","plot_waves_comparative(female_feat, male_feat, \"Female\", \"Male\", \"angry\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhBq4ykHTOvj"},"source":["plot_all_comparative_waveforms()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l2_hafW8NCs1"},"source":["### **3.2 PREPARACION DE LOS DATOS**\n"]},{"cell_type":"code","metadata":{"id":"oPYzQW0lD8mO"},"source":["def split_training_test(df, n_splits_=1, test_size_=0.25, train_size_=None):\n","  '''\n","  Divide el dataset en entrenamieto y test utilizando StratifiedShuffleSplit\n","  Aguments\n","  ---------\n","  Return\n","  -------\n","  '''\n","  X = df.drop(['gender', 'emotion'], axis=1)\n","  Y = df.emotion\n","  test_train_stratified = StratifiedShuffleSplit(n_splits = n_splits_, test_size = test_size_, random_state=12)\n","  for train_index, test_index in test_train_stratified.split(X, Y):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n","\n","  return X_train, X_test, Y_train, Y_test\n","  # return train_test_split(X, Y, stratify = Y, test_size=0.25)#X_train, X_test, Y_train, Y_test\n","\n","\n","def data_normalization(val_train, val_test):\n","  '''\n","  Normaliza los datos mejorando la precision y la velocidad del proceso de entrenamiento\n","  Aguments\n","  ---------\n","  Return\n","  -------\n","  '''\n","  # Now because we are mixing up a few different data sources, it would be wise to normalise the data. \n","  # This is proven to improve the accuracy and speed up the training process. Prior to the discovery of this solution in the embrionic years of neural network, \n","  # the problem used to be know as \"exploding gradients\".\n","  mean = np.mean(val_train, axis=0)\n","  std = np.std(val_train, axis=0)\n","\n","  X_train = (val_train - mean)/std\n","  X_test = (val_test - mean)/std\n","\n","  return X_train, X_test\n","\n","\n","def data_to_categorical(x_train_norm, y_train, x_test_norm, y_test):\n","  '''\n","  Categoriza los datos y los formatea para su uso con keras. Asume que x_train y x_test estan normalizados.\n","  Aguments\n","  ---------\n","  Return\n","  -------\n","  '''\n","  # Lets few preparation steps to get it into the correct format for Keras \n","  # Preparamos los datos para la categorizacion\n","  x_train_norm = np.array(x_train_norm)\n","  y_train = np.array(y_train)\n","  x_test_norm = np.array(x_test_norm)\n","  y_test = np.array(y_test)\n","\n","  # One hot encode \n","  label_encoder = LabelEncoder()\n","  y_train = np_utils.to_categorical(label_encoder.fit_transform(y_train))\n","  y_test = np_utils.to_categorical(label_encoder.fit_transform(y_test))\n","\n","  # # Pickel the lb object for future use \n","  # filename = 'labels'\n","  # outfile = open(filename,'wb')\n","  # pickle.dump(label_encoder, outfile)\n","  # outfile.close()\n","\n","  # print(x_train_norm.shape)\n","  # print(label_encoder.classes_)\n","\n","  return x_train_norm, y_train, x_test_norm, y_test, label_encoder\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93-_lq4_Kr0s"},"source":["### 3.2.1 DIVISION DE LOS DATOS EN ENTRENAMIENTO Y TEST"]},{"cell_type":"code","metadata":{"id":"dCPx5on3RgON"},"source":["# NOTA: Aqui estoy comprobando los datos que me da si divido entrenamiento y test con split de toda la vida\n","\n","# # female_X_train, female_X_test, female_Y_train, female_Y_test =split_training_test(female_features_df)\n","# female_X_train.head()\n","# print(918 in female_X_train.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j11yvzKOSCjK"},"source":["X_train, X_test, Y_train, Y_test = split_training_test(features_complete_df)\n","X_train.head()\n","print(X_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izvRvtm1bEFS"},"source":["# Comprobamos la distribución de las clases (emociones en nuestro caso)\n","Y_train.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YFFV5tx4bwOX"},"source":["# Comprobamos que no hay valores NaN\n","X_train.isna().sum().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtwPktkPwWtf"},"source":["### 3.2.2 NORMALIZACION DE LOS DATOS\n"]},{"cell_type":"code","metadata":{"id":"xHnViPw2wnd8"},"source":["X_train_norm, X_test_norm = data_normalization(X_train, X_test)\n","print(features_complete_df.shape)\n","\n","# Comprobamos imprimiendo una muestra de los datos\n","indx = random.randint(0, len(features_complete_df))\n","X_train[indx:indx+10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8LDPcBZb_79"},"source":["### 3.2.3 CATEGORIZACION DE LOS DATOS"]},{"cell_type":"code","metadata":{"id":"oroOdrFPcCAR"},"source":["X_train, Y_train, X_test, Y_test, labels_whole = data_to_categorical(X_train_norm, Y_train, X_test_norm, Y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hsUhE126I1wb"},"source":["print(labels_whole.classes_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Hbje1-V2mWU"},"source":["### 3.2.4 CAMBIO DE DIMENSION"]},{"cell_type":"code","metadata":{"id":"3y0blgUt2mcg"},"source":["X_train = np.expand_dims(X_train, axis=2)\n","X_test = np.expand_dims(X_test, axis=2)\n","print(X_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2jQURR6UgCh"},"source":["Comprobamos que despues de los cambios hay unas dimensiones coherentes"]},{"cell_type":"code","metadata":{"id":"s1xxY-0rTb-k"},"source":["print(\"X Train: {} --> Y Train: {}\".format(X_train.shape, Y_train.shape))\n","print(\"\\nX Test: {} --> Y Test: {}\".format(X_test.shape, Y_test.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ayIZhTj72toS"},"source":["### **3.3 PREPARACION DE LOS DATOS PARA _Female_**"]},{"cell_type":"code","metadata":{"id":"aS2zBonP2twt"},"source":["# 1. Division de los datos: Entrenamiento y Test\n","female_X_train, female_X_test, female_Y_train, female_Y_test =split_training_test(female_features_df) # with StratifiedShuffleSplit\n","\n","# 2. Normalizacion\n","x_train_female_norm, x_test_female_norm = data_normalization(female_X_train, female_X_test)\n","print(x_train_female_norm.shape)\n","\n","# 3. Categorización\n","female_X_train, female_Y_train, female_X_test, female_Y_test, labels_female = data_to_categorical(x_train_female_norm, female_Y_train, x_test_female_norm, female_Y_test)\n","print(labels_female.classes_)\n","\n","# 4. Cambio de Dimensión\n","X_train_female = np.expand_dims(female_X_train, axis=2)\n","X_test_female = np.expand_dims(female_X_test, axis=2)\n","\n","print(\"\\nTrain size: {}\\nTest size:{}\".format(X_train_female.shape, X_test_female.shape))\n","print(\"\\nTrain size: {}\\nTest size:{}\".format(female_Y_train.shape, female_Y_test.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-e0jKPiW2t_H"},"source":["### **3.4 PREPARACION DE LOS DATOS PARA _Male_**\n"]},{"cell_type":"code","metadata":{"id":"JgYdI-_z2uHX"},"source":["# 1. Division de los datos: Entrenamiento y Test\n","male_X_train, male_X_test, male_Y_train, male_Y_test =split_training_test(male_features_df)\n","male_X_train.head()\n","\n","# 2. Normalizacion\n","x_train_male_norm, x_test_male_norm = data_normalization(male_X_train, male_X_test)\n","print(x_train_male_norm.shape)\n","\n","# 3. Categorización\n","male_X_train, male_X_test, male_Y_train, male_Y_test, labels_male = data_to_categorical(x_train_male_norm, male_Y_train, x_test_male_norm, male_Y_test)\n","print(labels_male.classes_)\n","\n","# 4. Cambio de Dimensión\n","X_train_male = np.expand_dims(X_train, axis=2)\n","X_test_male = np.expand_dims(X_test, axis=2)\n","\n","print(\"\\nTrain size: {}\\nTest size:{}\".format(X_train_female.shape, X_test_female.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BR987F-kQtLF"},"source":["# **4. EL MODELO**\n","## 4.1 Diseño del modelo"]},{"cell_type":"code","metadata":{"id":"zfFk-ukQPI5E"},"source":["def model_cnn(x_train, n_classes):\n","  model = Sequential()\n","\n","  model.add(Conv1D(128, 5,padding='same',\n","                  input_shape=(X_train.shape[1],1) ))\n","  model.add(Activation('relu'))\n","  model.add(Dropout(0.1))\n","  model.add(MaxPooling1D(pool_size=(8)))\n","  model.add(Conv1D(128, 5,padding='same',))\n","  model.add(Activation('relu'))\n","  model.add(Dropout(0.1))\n","  model.add(Flatten())\n","  # Numero de clases\n","  model.add(Dense(n_classes))\n","  model.add(Activation('softmax'))\n","\n","  return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"daikhBWxQwHl"},"source":["## Compilación del modelo"]},{"cell_type":"code","metadata":{"id":"1rNv86m1bgbt"},"source":["# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n","# X_train_female , X_test_female\n","model = model_cnn(X_train_female, female_Y_test.shape[1])\n","model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eEDa2jFlK0bT"},"source":["# opt = keras.optimizers.Adam(lr=0.0001)\n","# opt = keras.optimizers.RMSprop(lr=0.00001, decay=1e-6)\n","#opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n","opt = keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n","\n","model.compile(loss='categorical_crossentropy', \n","              optimizer = opt, \n","              metrics=['accuracy'])\n","\n","\n","# Callbacks para tratar el overfitting\n","rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=20, min_lr=0.000001)\n","# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 15)\n","\n","history = model.fit(X_train_female, female_Y_train, \n","                        batch_size=16, \n","                        epochs=100, \n","                        validation_data=(X_test_female, female_Y_test),\n","                        callbacks=[rlrp])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pne1D4dEQhAC"},"source":["### Persistencia del modelo\n","Guardamos el modelo para un uso posterior sin necesidad de volverlo a entrenar\n"]},{"cell_type":"code","metadata":{"id":"AoCinwR_QhH4"},"source":["# detectar que modelo acaba de ejecutarse para guardarlo automaticamente?\n","model.save('model_english_female_SER.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"47kQyvMZrKu_"},"source":["## Validación del modelo"]},{"cell_type":"code","metadata":{"id":"RPOBSbv8rK1d"},"source":["\n","# Mostramos la grafica loss \n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","ax1.plot(history.history['loss'])\n","ax1.plot(history.history['val_loss'])\n","ax1.set_title('Loss')\n","ax1.set(xlabel='epoch', ylabel='loss')\n","ax1.legend(['train', 'test'], loc='upper right')\n","\n","# Mostramos la grafica accuracy\n","ax2.plot(history.history['accuracy'])\n","ax2.plot(history.history['val_accuracy'])\n","ax2.set_title('Validation')\n","ax2.set(xlabel='epoch', ylabel='acc')\n","\n","ax2.legend(['train', 'test'], loc='lower right')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ouilxF6TGUrM"},"source":["# Evaluamos contra test\n","score = model.evaluate(X_test_female, female_Y_test, batch_size=128)\n","print(\"Loss: {} \\nAccuracy: {}%\".format(score[0], score[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5M6A-A7rRPor"},"source":["## Predicción"]},{"cell_type":"code","metadata":{"id":"57xSv-QQwoe6"},"source":["# Predecimos los valores a partir de los datos de test\n","y_pred = model.predict(X_test_female)\n","y_pred_class = np.argmax(y_pred, axis = 1)\n","y_true = np.argmax(female_Y_test, axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qw0ZRov3j_bc"},"source":["# labels_female\n","y_pred_class.astype(int).flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LHmCd9TQ6350"},"source":["y_true"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HgFlW3rSrR8a"},"source":["cm = confusion_matrix(y_true, y_pred_class)\n","sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n","plt.title('Confusion Matrix', size=20)\n","plt.xlabel('Predicted Labels', size=14)\n","plt.ylabel('Actual Labels', size=14)\n","plt.show()"],"execution_count":null,"outputs":[]}]}