@article{Langari2020,
abstract = {One of the most important issues in human-computer interaction is to create a system that can hear and respond correctly like a human. This has led to the design of the Automatic Speech Emotion Recognition system (SER) that is able to identify different emotional classes by extracting and selecting effective features from speech signals. For this reason, in this study, we propose a novel feature extraction method based on adaptive time-frequency coefficients to improve the SER. The simulations are performed using the Berlin Emotional Speech Database (EMO-DB), the Surrey Audio-Visual Expressed Emotion Database (SAVEE), and the Persian Drama Radio Emotional Corpus (PDREC). The main contribution of our work is to extract novel features, called Adaptive Time-Frequency features, based on the Fractional Fourier Transform and to combine them with Cepstral features. Experimental results show that the proposed method effectively identifies different emotional classes in EMO-DB (97.57{\%} accuracy), SAVEE (80{\%} accuracy), and PDREC (91.46{\%} accuracy) data sets.},
author = {Langari, Shadi and Marvi, Hossein and Zahedi, Morteza},
doi = {10.1016/j.imu.2020.100424},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Langari, Marvi, Zahedi - 2020 - Efficient speech emotion recognition using modified feature extraction.pdf:pdf},
issn = {23529148},
journal = {Informatics in Medicine Unlocked},
keywords = {Evolutionary algorithms,Feature extraction,Feature selection,Human-computer interaction,Speech emotion recognition},
pages = {100424},
publisher = {Elsevier Ltd},
title = {{Efficient speech emotion recognition using modified feature extraction}},
url = {https://doi.org/10.1016/j.imu.2020.100424},
volume = {20},
year = {2020}
}
@article{Hellbernd2016,
	abstract = {Action-theoretic views of language posit that the recognition of others' intentions is key to successful interpersonal communication. Yet, speakers do not always code their intentions literally, raising the question of which mechanisms enable interlocutors to exchange communicative intents. The present study investigated whether and how prosody-the vocal tone-contributes to the identification of "unspoken" intentions. Single (non-)words were spoken with six intonations representing different speech acts-as carriers of communicative intentions. This corpus was acoustically analyzed (Experiment 1), and behaviorally evaluated in two experiments (Experiments 2 and 3). The combined results show characteristic prosodic feature configurations for different intentions that were reliably recognized by listeners. Interestingly, identification of intentions was not contingent on context (single words), lexical information (non-words), and recognition of the speaker's emotion (valence and arousal). Overall, the data demonstrate that speakers' intentions are represented in the prosodic signal which can, thus, determine the success of interpersonal communication.},
	author = {Hellbernd, Nele and Sammler, Daniela},
	doi = {10.1016/j.jml.2016.01.001},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hellbernd, Sammler - 2016 - Prosody conveys speaker's intentions Acoustic cues for speech act perception.pdf:pdf},
	issn = {0749596X},
	journal = {Journal of Memory and Language},
	keywords = {Acoustics,Intention,Pragmatics,Prosody,Speech acts},
	pages = {70--86},
	title = {{Prosody conveys speaker's intentions: Acoustic cues for speech act perception}},
	volume = {88},
	year = {2016}
}
@incollection{Rashid2018,
	abstract = {Speech is a complex naturally acquired human motor ability. It is characterized in adults with the production of about 14 different sounds per second via the harmonized actions of roughly 100 muscles. Speaker recognition is the capability of a software or hardware to receive speech signal, identify the speaker present in the speech signal and recognize the speaker afterwards. Feature extraction is accomplished by changing the speech waveform to a form of parametric representation at a relatively minimized data rate for subsequent processing and analysis. Therefore, acceptable classification is derived from excellent and quality features. Mel Frequency Cepstral Coefficients (MFCC), Linear Prediction Coeffi- cients (LPC), Linear Prediction Cepstral Coefficients (LPCC), Line Spectral Frequencies (LSF), Discrete Wavelet Transform (DWT) and Perceptual Linear Prediction (PLP) are the speech feature extraction techniques that were discussed in these chapter. These methods have been tested in a wide variety of applications, giving them high level of reliability and acceptability. Researchers have made several modifications to the above discussed tech- niques to make them less susceptible to noise, more robust and consume less time. In conclusion, none of the methods is superior to the other, the area of application would determine which method to select.},
	author = {Rashid, Sabur Ajibola Alim and Nahrul Khair Alang},
	booktitle = {From Natural to Artificial Intelligence: Algorithms and Applications},
	chapter = {Chapter 1},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rashid - 2018 - Some Commonly Used Speech Feature Extraction Algorithms.pdf:pdf},
	keywords = {discrete wavelet transform (DWT),human speech,line spectral frequencies (LSF),linear prediction cepstral coefficients (LPCC),linear prediction coefficients (LPC),mel frequency cepstral coefficients (MFCC),perceptual linear prediction (PLP),speech features},
	number = {tourism},
	pages = {13},
	title = {{Some Commonly Used Speech Feature Extraction Algorithms}},
	url = {https://www.intechopen.com/books/advanced-biometric-technologies/liveness-detection-in-biometrics},
	year = {2018}
}
@article{Farooq2020,
	abstract = {Speech emotion recognition (SER) plays a significant role in human-machine interaction. Emotion recognition from speech and its precise classification is a challenging task because a machine is unable to understand its context. For an accurate emotion classification, emotionally relevant features must be extracted from the speech data. Traditionally, handcrafted features were used for emotional classification from speech signals; however, they are not efficient enough to accurately depict the emotional states of the speaker. In this study, the benefits of a deep convolutional neural network (DCNN) for SER are explored. For this purpose, a pretrained network is used to extract features from state-of-the-art speech emotional datasets. Subsequently, a correlation-based feature selection technique is applied to the extracted features to select the most appropriate and discriminative features for SER. For the classification of emotions, we utilize support vector machines, random forests, the k-nearest neighbors algorithm, and neural network classifiers. Experiments are performed for speaker-dependent and speaker-independent SER using four publicly available datasets: the Berlin Dataset of Emotional Speech (Emo-DB), Surrey Audio Visual Expressed Emotion (SAVEE), Interactive Emotional Dyadic Motion Capture (IEMOCAP), and the Ryerson Audio Visual Dataset of Emotional Speech and Song (RAVDESS). Our proposed method achieves an accuracy of 95.10% for Emo-DB, 82.10% for SAVEE, 83.80% for IEMOCAP, and 81.30% for RAVDESS, for speaker-dependent SER experiments. Moreover, our method yields the best results for speaker-independent SER with existing handcrafted features-based SER approaches.},
	author = {Farooq, Misbah and Hussain, Fawad and Baloch, Naveed Khan and Raja, Fawad Riasat and Yu, Heejung and Zikria, Yousaf Bin},
	doi = {10.3390/s20216008},
	file = {:home/luisa/Downloads/sensors-20-06008.pdf:pdf},
	issn = {14248220},
	journal = {Sensors (Switzerland)},
	keywords = {Correlation-based feature selection,Deep convolutional neural network,Speech emotion recognition},
	number = {21},
	pages = {1--18},
	pmid = {33113907},
	title = {{Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network}},
	volume = {20},
	year = {2020}
}
@article{AbdulQayyum2019,
	abstract = {Speech is considered as the widest and most natural medium of communication. Speech can convey a plethora of information regarding one's mental, behavioral, emotional traits. Besides, speech-emotion recognition related work can aid in averting cyber crimes. Research on speech-emotion recognition exploiting concurrent machine learning techniques has been on the peak for some time. Numerous techniques like Recurrent Neural Network (RNN), Deep Neural Network (DNN), spectral feature extraction and many more have been applied on different datasets. This paper presents a unique Convolutional Neural Network (CNN) based speech-emotion recognition system. A model is developed and fed with raw speech from specific dataset for training, classification and testing purposes with the help of high end GPU. Finally, it comes out with a convincing accuracy of 83.61% which is better compared to any other similar task on this dataset by a large margin. This work will be influential in developing conversational and social robots and allocating all the nuances of their sentiments.},
	author = {{Abdul Qayyum}, Alif Bin and Arefeen, Asiful and Shahnaz, Celia},
	doi = {10.1109/SPICSCON48833.2019.9065172},
	file = {:home/luisa/Downloads/ConvolutionalNeuralNetworkCNNBasedSpeech-EmotionRecognition.pdf:pdf},
	isbn = {9781728142944},
	journal = {2019 IEEE International Conference on Signal Processing, Information, Communication and Systems, SPICSCON 2019},
	keywords = {Convolutional Neural Network (CNN),Deep Neural Network (DNN),Discrete Cosine Transform (DCT),Emotion,Emotional state,Modulation Spectral Features (MSF),Multi class,Multivariate Linear Regression Classification (MLR),Recurrent Neural Network (RNN),SAVEE dataset,Speech Processing,Speech-emotion,Support Vector Machines (SVM)},
	number = {November},
	pages = {122--125},
	title = {{Convolutional Neural Network (CNN) Based Speech-Emotion Recognition}},
	year = {2019}
}


