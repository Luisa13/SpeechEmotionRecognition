@article{Langari2020,
abstract = {One of the most important issues in human-computer interaction is to create a system that can hear and respond correctly like a human. This has led to the design of the Automatic Speech Emotion Recognition system (SER) that is able to identify different emotional classes by extracting and selecting effective features from speech signals. For this reason, in this study, we propose a novel feature extraction method based on adaptive time-frequency coefficients to improve the SER. The simulations are performed using the Berlin Emotional Speech Database (EMO-DB), the Surrey Audio-Visual Expressed Emotion Database (SAVEE), and the Persian Drama Radio Emotional Corpus (PDREC). The main contribution of our work is to extract novel features, called Adaptive Time-Frequency features, based on the Fractional Fourier Transform and to combine them with Cepstral features. Experimental results show that the proposed method effectively identifies different emotional classes in EMO-DB (97.57{\%} accuracy), SAVEE (80{\%} accuracy), and PDREC (91.46{\%} accuracy) data sets.},
author = {Langari, Shadi and Marvi, Hossein and Zahedi, Morteza},
doi = {10.1016/j.imu.2020.100424},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Langari, Marvi, Zahedi - 2020 - Efficient speech emotion recognition using modified feature extraction.pdf:pdf},
issn = {23529148},
journal = {Informatics in Medicine Unlocked},
keywords = {Evolutionary algorithms,Feature extraction,Feature selection,Human-computer interaction,Speech emotion recognition},
pages = {100424},
publisher = {Elsevier Ltd},
title = {{Efficient speech emotion recognition using modified feature extraction}},
url = {https://doi.org/10.1016/j.imu.2020.100424},
volume = {20},
year = {2020}
}
@article{Hellbernd2016,
	abstract = {Action-theoretic views of language posit that the recognition of others' intentions is key to successful interpersonal communication. Yet, speakers do not always code their intentions literally, raising the question of which mechanisms enable interlocutors to exchange communicative intents. The present study investigated whether and how prosody-the vocal tone-contributes to the identification of "unspoken" intentions. Single (non-)words were spoken with six intonations representing different speech acts-as carriers of communicative intentions. This corpus was acoustically analyzed (Experiment 1), and behaviorally evaluated in two experiments (Experiments 2 and 3). The combined results show characteristic prosodic feature configurations for different intentions that were reliably recognized by listeners. Interestingly, identification of intentions was not contingent on context (single words), lexical information (non-words), and recognition of the speaker's emotion (valence and arousal). Overall, the data demonstrate that speakers' intentions are represented in the prosodic signal which can, thus, determine the success of interpersonal communication.},
	author = {Hellbernd, Nele and Sammler, Daniela},
	doi = {10.1016/j.jml.2016.01.001},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hellbernd, Sammler - 2016 - Prosody conveys speaker's intentions Acoustic cues for speech act perception.pdf:pdf},
	issn = {0749596X},
	journal = {Journal of Memory and Language},
	keywords = {Acoustics,Intention,Pragmatics,Prosody,Speech acts},
	pages = {70--86},
	title = {{Prosody conveys speaker's intentions: Acoustic cues for speech act perception}},
	volume = {88},
	year = {2016}
}
@incollection{Rashid2018,
	abstract = {Speech is a complex naturally acquired human motor ability. It is characterized in adults with the production of about 14 different sounds per second via the harmonized actions of roughly 100 muscles. Speaker recognition is the capability of a software or hardware to receive speech signal, identify the speaker present in the speech signal and recognize the speaker afterwards. Feature extraction is accomplished by changing the speech waveform to a form of parametric representation at a relatively minimized data rate for subsequent processing and analysis. Therefore, acceptable classification is derived from excellent and quality features. Mel Frequency Cepstral Coefficients (MFCC), Linear Prediction Coeffi- cients (LPC), Linear Prediction Cepstral Coefficients (LPCC), Line Spectral Frequencies (LSF), Discrete Wavelet Transform (DWT) and Perceptual Linear Prediction (PLP) are the speech feature extraction techniques that were discussed in these chapter. These methods have been tested in a wide variety of applications, giving them high level of reliability and acceptability. Researchers have made several modifications to the above discussed tech- niques to make them less susceptible to noise, more robust and consume less time. In conclusion, none of the methods is superior to the other, the area of application would determine which method to select.},
	author = {Rashid, Sabur Ajibola Alim and Nahrul Khair Alang},
	booktitle = {From Natural to Artificial Intelligence: Algorithms and Applications},
	chapter = {Chapter 1},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rashid - 2018 - Some Commonly Used Speech Feature Extraction Algorithms.pdf:pdf},
	keywords = {discrete wavelet transform (DWT),human speech,line spectral frequencies (LSF),linear prediction cepstral coefficients (LPCC),linear prediction coefficients (LPC),mel frequency cepstral coefficients (MFCC),perceptual linear prediction (PLP),speech features},
	number = {tourism},
	pages = {13},
	title = {{Some Commonly Used Speech Feature Extraction Algorithms}},
	url = {https://www.intechopen.com/books/advanced-biometric-technologies/liveness-detection-in-biometrics},
	year = {2018}
}
@article{Farooq2020,
	abstract = {Speech emotion recognition (SER) plays a significant role in human-machine interaction. Emotion recognition from speech and its precise classification is a challenging task because a machine is unable to understand its context. For an accurate emotion classification, emotionally relevant features must be extracted from the speech data. Traditionally, handcrafted features were used for emotional classification from speech signals; however, they are not efficient enough to accurately depict the emotional states of the speaker. In this study, the benefits of a deep convolutional neural network (DCNN) for SER are explored. For this purpose, a pretrained network is used to extract features from state-of-the-art speech emotional datasets. Subsequently, a correlation-based feature selection technique is applied to the extracted features to select the most appropriate and discriminative features for SER. For the classification of emotions, we utilize support vector machines, random forests, the k-nearest neighbors algorithm, and neural network classifiers. Experiments are performed for speaker-dependent and speaker-independent SER using four publicly available datasets: the Berlin Dataset of Emotional Speech (Emo-DB), Surrey Audio Visual Expressed Emotion (SAVEE), Interactive Emotional Dyadic Motion Capture (IEMOCAP), and the Ryerson Audio Visual Dataset of Emotional Speech and Song (RAVDESS). Our proposed method achieves an accuracy of 95.10% for Emo-DB, 82.10% for SAVEE, 83.80% for IEMOCAP, and 81.30% for RAVDESS, for speaker-dependent SER experiments. Moreover, our method yields the best results for speaker-independent SER with existing handcrafted features-based SER approaches.},
	author = {Farooq, Misbah and Hussain, Fawad and Baloch, Naveed Khan and Raja, Fawad Riasat and Yu, Heejung and Zikria, Yousaf Bin},
	doi = {10.3390/s20216008},
	file = {:home/luisa/Downloads/sensors-20-06008.pdf:pdf},
	issn = {14248220},
	journal = {Sensors (Switzerland)},
	keywords = {Correlation-based feature selection,Deep convolutional neural network,Speech emotion recognition},
	number = {21},
	pages = {1--18},
	pmid = {33113907},
	title = {{Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network}},
	volume = {20},
	year = {2020}
}
@article{AbdulQayyum2019,
	abstract = {Speech is considered as the widest and most natural medium of communication. Speech can convey a plethora of information regarding one's mental, behavioral, emotional traits. Besides, speech-emotion recognition related work can aid in averting cyber crimes. Research on speech-emotion recognition exploiting concurrent machine learning techniques has been on the peak for some time. Numerous techniques like Recurrent Neural Network (RNN), Deep Neural Network (DNN), spectral feature extraction and many more have been applied on different datasets. This paper presents a unique Convolutional Neural Network (CNN) based speech-emotion recognition system. A model is developed and fed with raw speech from specific dataset for training, classification and testing purposes with the help of high end GPU. Finally, it comes out with a convincing accuracy of 83.61% which is better compared to any other similar task on this dataset by a large margin. This work will be influential in developing conversational and social robots and allocating all the nuances of their sentiments.},
	author = {{Abdul Qayyum}, Alif Bin and Arefeen, Asiful and Shahnaz, Celia},
	doi = {10.1109/SPICSCON48833.2019.9065172},
	file = {:home/luisa/Downloads/ConvolutionalNeuralNetworkCNNBasedSpeech-EmotionRecognition.pdf:pdf},
	isbn = {9781728142944},
	journal = {2019 IEEE International Conference on Signal Processing, Information, Communication and Systems, SPICSCON 2019},
	keywords = {Convolutional Neural Network (CNN),Deep Neural Network (DNN),Discrete Cosine Transform (DCT),Emotion,Emotional state,Modulation Spectral Features (MSF),Multi class,Multivariate Linear Regression Classification (MLR),Recurrent Neural Network (RNN),SAVEE dataset,Speech Processing,Speech-emotion,Support Vector Machines (SVM)},
	number = {November},
	pages = {122--125},
	title = {{Convolutional Neural Network (CNN) Based Speech-Emotion Recognition}},
	year = {2019}
}
@article{Sarkania2013,
	abstract = {Automatic emotion of detection in speech is a latest research area in the field of human machine interaction and speech processing. The aim of this paper is to enable a very natural interaction among human and machine. This dissertation proposes an approach to recognize the user's emotional state by analysing signal of human speech. To achieve the good extraction of the feature from the signal the propose technique uses the high pass filter before the feature extraction process. High pass filter uses to reduce the noise. High pass filter pass only high frequency and attenuates the lower frequency. This paper uses the Neural Network as a classifier to classify the different emotional states such as happy, sad, anger etc from emotional speech database. For the performance of classification use the speech feature such as Mel Frequency cepstrum coefficient (MFCC). The result shows that the Neural Network used as a classifier is a feasible technique for the emotional classification. By using the high pass filter performance should be increase.},
	author = {Sarkania, Vaibhav Kumar and Bhalla, Vinod Kumar},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarkania, Bhalla - 2013 - International Journal of Advanced Research in.pdf:pdf},
	journal = {Android Internals},
	keywords = {- android boot up,activity lifecycle,dalvik vm,zygote},
	number = {6},
	pages = {143--147},
	title = {{Emotion Recognition through Speech Using Neural Network}},
	volume = {3},
	year = {2015}
}
@article{Lim2017,
	abstract = {With rapid developments in the design of deep architecture models and learning algorithms, methods referred to as deep learning have come to be widely used in a variety of research areas such as pattern recognition, classification, and signal processing. Deep learning methods are being applied in various recognition tasks such as image, speech, and music recognition. Convolutional Neural Networks (CNNs) especially show remarkable recognition performance for computer vision tasks. In addition, Recurrent Neural Networks (RNNs) show considerable success in many sequential data processing tasks. In this study, we investigate the result of the Speech Emotion Recognition (SER) algorithm based on CNNs and RNNs trained using an emotional speech database. The main goal of our work is to propose a SER method based on concatenated CNNs and RNNs without using any traditional hand-crafted features. By applying the proposed methods to an emotional speech database, the classification result was verified to have better accuracy than that achieved using conventional classification methods.},
	author = {Lim, Wootaek and Jang, Daeyoung and Lee, Taejin},
	doi = {10.1109/APSIPA.2016.7820699},
	file = {:C$\backslash$:/Users/Usuario/Documents/Downloads/137.pdf:pdf},
	isbn = {9789881476821},
	journal = {2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016},
	title = {{Speech emotion recognition using convolutional and Recurrent Neural Networks}},
	year = {2017}
}
@article{Lee2015,
	abstract = {This paper presents a speech emotion recognition system using a recurrent neural network (RNN) model trained by an efficient learning algorithm. The proposed system takes into account the long-range contextual effect and the uncertainty of emotional label expressions. To extract high-level representation of emotional states with regard to its temporal dynamics, a powerful learning method with a bidirectional long short-term memory (BLSTM) structure is adopted. To overcome the uncertainty of emotional labels, such that all frames in the same utterance are mapped to the same emotional label, it is assumed that the label of each frame is regarded as a sequence of random variables. The sequences are then trained by the proposed learning algorithm. The weighted accuracy of the proposed emotion recognition system is improved up to 12{\%} compared to the DNNELM- based emotion recognition system used as a baseline.},
	author = {Lee, Jinkyu and Tashev, Ivan},
	file = {:C$\backslash$:/Users/Usuario/Documents/Downloads/Lee-Tashev{\_}Emotion{\_}detection{\_}Interspeech2015.pdf:pdf},
	issn = {19909772},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	keywords = {Long short-term memory,Recurrent neural network,Speech emotion recognition},
	pages = {1537--1540},
	title = {{High-level feature representation using recurrent neural network for speech emotion recognition}},
	volume = {2015-January},
	year = {2015}
}
@article{Harar2017,
	abstract = {This paper describes a method for Speech Emotion Recognition (SER) using Deep Neural Network (DNN) architecture with convolutional, pooling and fully connected layers. We used 3 class subset (angry, neutral, sad) of German Corpus (Berlin Database of Emotional Speech) containing 271 labeled recordings with total length of 783 seconds. Raw audio data were standardized so every audio file has zero mean and unit variance. Every file was split into 20 millisecond segments without overlap. We used Voice Activity Detection (VAD) algorithm to eliminate silent segments and divided all data into TRAIN (80{\%}) VALIDATION (10{\%}) and TESTING (10{\%}) sets. DNN is optimized using Stochastic Gradient Descent. As input we used raw data without and feature selection. Our trained model achieved overall test accuracy of 96.97{\%} on whole-file classification.},
	author = {Harar, Pavol and Burget, Radim and Dutta, Malay Kishore},
	doi = {10.1109/SPIN.2017.8049931},
	file = {:C$\backslash$:/Users/Usuario/Downloads/Speech-Emotion-Recognition-with-Deep-Learning.pdf:pdf},
	isbn = {9781509027972},
	journal = {2017 4th International Conference on Signal Processing and Integrated Networks, SPIN 2017},
	number = {February 2017},
	pages = {137--140},
	title = {{Speech emotion recognition with deep learning}},
	year = {2017}
}
@article{Wang2020,
	abstract = {Speech Emotion Recognition (SER) has emerged as a critical component of the next generation of human-machine interfacing technologies. In this work, we propose a new duallevel model that predicts emotions based on both MFCC features and mel-spectrograms produced from raw audio signals. Each utterance is preprocessed into MFCC features and two mel-spectrograms at different time-frequency resolutions. A standard LSTM processes the MFCC features, while a novel LSTM architecture, denoted as Dual-Sequence LSTM (DSLSTM), processes the two mel-spectrograms simultaneously. The outputs are later averaged to produce a final classification of the utterance. Our proposed model achieves, on average, a weighted accuracy of 72.7{\%} and an unweighted accuracy of 73.3{\%}-a 6{\%} improvement over current state-of-the-art unimodal models-and is comparable with multimodal models that leverage textual information as well as audio signals.},
	archivePrefix = {arXiv},
	arxivId = {1910.08874},
	author = {Wang, Jianyou and Xue, Michael and Culhane, Ryan and Diao, Enmao and Ding, Jie and Tarokh, Vahid},
	doi = {10.1109/ICASSP40776.2020.9054629},
	eprint = {1910.08874},
	file = {:C$\backslash$:/Users/Usuario/Documents/Downloads/1910.08874.pdf:pdf},
	isbn = {9781509066315},
	issn = {15206149},
	journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	keywords = {Dual-Level Model,Dual-Sequence LSTM,LSTM,Mel-Spectrogram,Speech Emotion Recognition},
	pages = {6474--6478},
	title = {{Speech emotion recognition with dual-sequence LSTM architecture}},
	volume = {2020-May},
	year = {2020}
}





