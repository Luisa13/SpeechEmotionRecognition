@article{Wang2020,
abstract = {Speech Emotion Recognition (SER) has emerged as a critical component of the next generation of human-machine interfacing technologies. In this work, we propose a new duallevel model that predicts emotions based on both MFCC features and mel-spectrograms produced from raw audio signals. Each utterance is preprocessed into MFCC features and two mel-spectrograms at different time-frequency resolutions. A standard LSTM processes the MFCC features, while a novel LSTM architecture, denoted as Dual-Sequence LSTM (DSLSTM), processes the two mel-spectrograms simultaneously. The outputs are later averaged to produce a final classification of the utterance. Our proposed model achieves, on average, a weighted accuracy of 72.7{\%} and an unweighted accuracy of 73.3{\%}-a 6{\%} improvement over current state-of-the-art unimodal models-and is comparable with multimodal models that leverage textual information as well as audio signals.},
archivePrefix = {arXiv},
arxivId = {1910.08874},
author = {Wang, Jianyou and Xue, Michael and Culhane, Ryan and Diao, Enmao and Ding, Jie and Tarokh, Vahid},
doi = {10.1109/ICASSP40776.2020.9054629},
eprint = {1910.08874},
file = {:C$\backslash$:/Users/Usuario/Documents/Downloads/1910.08874.pdf:pdf},
isbn = {9781509066315},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Dual-Level Model,Dual-Sequence LSTM,LSTM,Mel-Spectrogram,Speech Emotion Recognition},
pages = {6474--6478},
title = {{Speech emotion recognition with dual-sequence LSTM architecture}},
volume = {2020-May},
year = {2020}
}
