@article{Pell2001,
abstract = {Preliminary data were collected on how emotional qualities of the voice (sad, happy, angry) influence the acoustic underpinnings of neutral sentences varying in location of intra-sentential focus (initial, final, no) and utterance "modality" (statement, question). Short (six syllable) and long (ten syllable) utterances exhibiting varying combinations of emotion, focus, and modality characteristics were analyzed for eight elderly speakers following administration of a controlled elicitation paradigm (story completion) and a speaker evaluation procedure. Duration and fundamental frequency (f0) parameters of recordings were scrutinized for "keyword" vowels within each token and for whole utterances. Results generally re-affirmed past accounts of how duration and f0 are encoded on key content words to mark linguistic focus in affectively neutral statements and questions for English. Acoustic data on three "global" parameters of the stimuli (speech rate, mean f0, f0 range) were also largely supportive of previous descriptions of how happy, sad, angry, and neutral utterances are differentiated in the speech signal. Important interactions between emotional and linguistic properties of the utterances emerged which were predominantly (although not exclusively) tied to the modulation of f0; speakers were notably constrained in conditions which required them to manipulate f0 parameters to express emotional and nonemotional intentions conjointly. Sentence length also had a meaningful impact on some of the measures gathered.},
author = {Pell, Marc D.},
doi = {10.1121/1.1352088},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Influence{\_}of{\_}emotion{\_}and{\_}focus{\_}location.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {1668--1680},
title = {{Influence of emotion and focus location on prosody in matched statements and questions}},
volume = {109},
year = {2001}
}
@article{Pell2011,
abstract = {To inform how emotions in speech are implicitly processed and registered in memory, we compared how emotional prosody, emotional semantics, and both cues in tandem prime decisions about conjoined emotional faces. Fifty-two participants rendered facial affect decisions (Pell, 2005a), indicating whether a target face represented an emotion (happiness or sadness) or not (a facial grimace), after passively listening to happy, sad, or neutral prime utterances. Emotional information from primes was conveyed by: (1) prosody only; (2) semantic cues only; or (3) combined prosody and semantic cues. Results indicated that prosody, semantics, and combined prosody-semantic cues facilitate emotional decisions about target faces in an emotion-congruent manner. However, the magnitude of priming did not vary across tasks. Our findings highlight that emotional meanings of prosody and semantic cues are systematically registered during speech processing, but with similar effects on associative knowledge about emotions, which is presumably shared by prosody, semantics, and faces. {\textcopyright} 2010 Psychology Press.},
author = {Pell, Marc D. and Jaywant, Abhishek and Monetta, Laura and Kotz, Sonja A.},
doi = {10.1080/02699931.2010.516915},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Emotional{\_}speech{\_}processing{\_}Disentanglin.pdf:pdf},
issn = {02699931},
journal = {Cognition and Emotion},
keywords = {Audio-visual priming,Emotions,Facial expression,Speech perception,Vocal cues},
number = {5},
pages = {834--853},
title = {{Emotional speech processing: Disentangling the effects of prosody and semantic cues}},
volume = {25},
year = {2011}
}
@article{Pell2009,
abstract = {To understand how language influences the vocal communication of emotion, we investigated how discrete emotions are recognized and acoustically differentiated in four language contexts-English, German, Hindi, and Arabic. Vocal expressions of six emotions (anger, disgust, fear, sadness, happiness, pleasant surprise) and neutral expressions were elicited from four native speakers of each language. Each speaker produced pseudo-utterances ("nonsense speech") which resembled their native language to express each emotion type, and the recordings were judged for their perceived emotional meaning by a group of native listeners in each language condition. Emotion recognition and acoustic patterns were analyzed within and across languages. Although overall recognition rates varied by language, all emotions could be recognized strictly from vocal cues in each language at levels exceeding chance. Anger, sadness, and fear tended to be recognized most accurately irrespective of language. Acoustic and discriminant function analyses highlighted the importance of speaker fundamental frequency (i.e., relative pitch level and variability) for signalling vocal emotions in all languages. Our data emphasize that while emotional communication is governed by display rules and other social variables, vocal expressions of 'basic' emotion in speech exhibit modal tendencies in their acoustic and perceptual attributes which are largely unaffected by language or linguistic similarity. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Pell, Marc D. and Paulmann, Silke and Dara, Chinar and Alasseri, Areej and Kotz, Sonja A.},
doi = {10.1016/j.wocn.2009.07.005},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Factors{\_}in{\_}the{\_}recognition{\_}of{\_}emotional.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
number = {4},
pages = {417--435},
title = {{Factors in the recognition of vocally expressed emotions: A comparison of four languages}},
volume = {37},
year = {2009}
}
@article{Sarkania2013,
abstract = {Android is a linux based operating system which uses linux kernel. In this paper we will see how the boot up process of android is different from linux and how the different applications in an Android System communicate with each other. As an Android Application is made up of different activities so through this paper we will also come to know about how the activity changes their states and the whole lifecycle of an activity.},
author = {Sarkania, Vaibhav Kumar and Bhalla, Vinod Kumar},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarkania, Bhalla - 2013 - International Journal of Advanced Research in.pdf:pdf},
journal = {Android Internals},
keywords = {- android boot up,activity lifecycle,dalvik vm,zygote},
number = {6},
pages = {143--147},
title = {{International Journal of Advanced Research in}},
volume = {3},
year = {2013}
}
@article{Cheang2011,
abstract = {The goal of the present research was to determine whether certain speaker intentions conveyed through prosody in an unfamiliar language can be accurately recognized. English and Cantonese utterances expressing sarcasm, sincerity, humorous irony, or neutrality through prosody were presented to English and Cantonese listeners unfamiliar with the other language. Listeners identified the communicative intent of utterances in both languages in a crossed design. Participants successfully identified sarcasm spoken in their native language but identified sarcasm at near-chance levels in the unfamiliar language. Both groups were relatively more successful at recognizing the other attitudes when listening to the unfamiliar language (in addition to the native language). Our data suggest that while sarcastic utterances in Cantonese and English share certain acoustic features, these cues are insufficient to recognize sarcasm between languages; rather, this ability depends on (native) language experience.},
author = {Cheang, Henry S and Pell, Marc D},
doi = {10.1075/pc.19.2.02che},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Recognizing{\_}sarcasm{\_}without{\_}language{\_}A{\_}c.pdf:pdf},
issn = {0929-0907},
journal = {Pragmatics {\&} CognitionPragmatics and Cognition},
keywords = {cantonese,communicative intentions,cross-linguistic,sarcasm},
number = {2},
pages = {203--223},
title = {{Recognizing sarcasm without language: A cross-linguistic study of English and Cantonese}},
volume = {19},
year = {2011}
}
@article{BAKIR2018,
abstract = {In several application, emotion recognition from the speech signal has been research topic since many years. To determine the emotions from the speech signal, many systems have been developed. To solve the speaker emotion recognition problem, hybrid model is proposed to classify five speech emotions, including anger, sadness, fear, happiness and neutral. The aim this study of was to actualize automatic voice and speech emotion recognition system using hybrid model taking Turkish sound forms and properties into consideration. Approximately 3000 Turkish voice samples of words and clauses with differing lengths have been collected from 25 males and 25 females. In this study, an authentic and unique Turkish database has been used. Features of these voice samples have been obtained using Mel Frequency Cepstral Coefficients (MFCC) and Mel Frequency Discrete Wavelet Coefficients (MFDWC). Moreover, spectral features of these voice samples have been obtained using Support Vector Machine (SVM). Feature vectors of the voice samples obtained have been trained with such methods as Gauss Mixture Model( GMM), Artifical Neural Network (ANN), Dynamic Time Warping (DTW), Hidden Markov Model (HMM) and hybrid model(GMM with combined SVM). This hybrid model has been carried out by combining with SVM and GMM. In first stage of this model, with SVM has been performed subsets obtained vector of spectral features. In the second phase, a set of training and tests have been formed from these spectral features. In the test phase, owner of a given voice sample has been identified taking the trained voice samples into consideration. Results and performances of the algorithms employed in the study for classification have been also demonstrated in a comparative manner.},
author = {BAKIR, CIGDEM and YUZKAT, MECIT},
doi = {10.17694/bajece.419557},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Speech Emotion Classification and Recognition with different methods for Turkish Language, .pdf:pdf},
issn = {2147-284X},
journal = {Balkan Journal of Electrical and Computer Engineering},
number = {2},
pages = {54--60},
title = {{Speech Emotion Classification and Recognition with different methods for Turkish Language}},
volume = {6},
year = {2018}
}
@article{Kaminska2012,
abstract = {Machine recognition of human emotional states is an essential part in improving man-machine interaction. During expressive speech the voice conveys semantic message as well as the information about emotional state of the speaker. The pitch contour is one of the most significant properties of speech, which is affected by the emotional state. Therefore pitch features have been commonly used in systems for automatic emotion detection. In this work different intensities of emotions and their influence on pitch features have been studied. This understanding is important to develop such a system. Intensities of emotions are presented on Plutchik's cone-shaped 3D model. The k Nearest Neighbor algorithm has been used for classification. The classification has been divided into two parts. First, the primary emotion has been detected, then its intensity has been specified. The results show that the recognition accuracy of the system is over 50{\%} for primary emotions, and over 70{\%} for its intensities.},
author = {Kami{\'{n}}ska, Dorota and Pelikant, Adam},
doi = {10.2478/v10177-012-0024-4},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kami{\'{n}}ska, Pelikant - 2012 - Recognition of human emotion from a speech signal based on plutchik's model.pdf:pdf},
issn = {20818491},
journal = {International Journal of Electronics and Telecommunications},
keywords = {Plutchik's wheel of emotion,emotion detection,speech signal},
number = {2},
pages = {165--170},
title = {{Recognition of human emotion from a speech signal based on plutchik's model}},
volume = {58},
year = {2012}
}
@article{Pell2009a,
abstract = {Expressions of basic emotions (joy, sadness, anger, fear, disgust) can be recognized pan-culturally from the face and it is assumed that these emotions can be recognized from a speaker's voice, regardless of an individual's culture or linguistic ability. Here, we compared how monolingual speakers of Argentine Spanish recognize basic emotions from pseudo-utterances ("nonsense speech") produced in their native language and in three foreign languages (English, German, Arabic). Results indicated that vocal expressions of basic emotions could be decoded in each language condition at accuracy levels exceeding chance, although Spanish listeners performed significantly better overall in their native language ("in-group advantage"). Our findings argue that the ability to understand vocally-expressed emotions in speech is partly independent of linguistic ability and involves universal principles, although this ability is also shaped by linguistic and cultural variables.},
author = {Pell, Marc D. and Monetta, Laura and Paulmann, Silke and Kotz, Sonja A.},
doi = {10.1007/s10919-008-0065-7},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Recognizing{\_}Emotions{\_}in{\_}a{\_}Foreign{\_}Langua.pdf:pdf},
issn = {01915886},
journal = {Journal of Nonverbal Behavior},
keywords = {Affective prosody,Cross-linguistic group study,Cultural factors,Emotional speech processing,Vocal expression},
number = {2},
pages = {107--120},
title = {{Recognizing Emotions in a Foreign Language}},
volume = {33},
year = {2009}
}
@article{Lugovic2016,
abstract = {Affective computing opens a new area of research in computer science with the aim to improve the way how humans and machines interact. Recognition of human emotions by machines is becoming a significant focus in recent research in different disciplines related to information sciences and Human-Computer Interaction (HCI). In particular, emotion recognition in human speech is important, as it is the primary communication tool of humans. This paper gives a brief overview of the current state of the research in this area with the aim to underline different techniques that are being used for detecting emotional states in vocal expressions. Furthermore, approaches for extracting speech features from speech datasets and machine learning methods with special emphasis on classifiers are analysed. In addition to the mentioned techniques, this paper also gives an outline of the areas where emotion recognition could be utilised such as healthcare, psychology, cognitive sciences and marketing.},
author = {Lugovic, S. and Dunder, I. and Horvat, M.},
doi = {10.1109/MIPRO.2016.7522336},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lugovic, Dunder, Horvat - 2016 - Techniques and applications of emotion recognition in speech.pdf:pdf},
isbn = {9789532330885},
journal = {2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2016 - Proceedings},
keywords = {acoustic signal processing,affective computing,emotion recognition,human-computer interaction,linguistic speech features,machine learning,speech analysis},
number = {November 2017},
pages = {1278--1283},
title = {{Techniques and applications of emotion recognition in speech}},
year = {2016}
}
@article{Pell2015,
abstract = {This study used event-related brain potentials (ERPs) to compare the time course of emotion processing from non-linguistic vocalizations versus speech prosody, to test whether vocalizations are treated preferentially by the neurocognitive system. Participants passively listened to vocalizations or pseudo-utterances conveying anger, sadness, or happiness as the EEG was recorded. Simultaneous effects of vocal expression type and emotion were analyzed for three ERP components (N100, P200, late positive component). Emotional vocalizations and speech were differentiated very early (N100) and vocalizations elicited stronger, earlier, and more differentiated P200 responses than speech. At later stages (450-700. ms), anger vocalizations evoked a stronger late positivity (LPC) than other vocal expressions, which was similar but delayed for angry speech. Individuals with high trait anxiety exhibited early, heightened sensitivity to vocal emotions (particularly vocalizations). These data provide new neurophysiological evidence that vocalizations, as evolutionarily primitive signals, are accorded precedence over speech-embedded emotions in the human voice.},
author = {Pell, M. D. and Rothermich, K. and Liu, P. and Paulmann, S. and Sethi, S. and Rigoulot, S.},
doi = {10.1016/j.biopsycho.2015.08.008},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pell et al. - 2015 - Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody.pdf:pdf},
issn = {18736246},
journal = {Biological Psychology},
keywords = {Anxiety,ERPs,Emotional communication,Speech prosody,Vocal expression},
pages = {14--25},
pmid = {26307467},
publisher = {Elsevier B.V.},
title = {{Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody}},
url = {http://dx.doi.org/10.1016/j.biopsycho.2015.08.008},
volume = {111},
year = {2015}
}
@article{Davletcharova2015,
abstract = {Recognizing emotion from speech has become one the active research themes in speech processing and in applications based on human-computer interaction. This paper conducts an experimental study on recognizing emotions from human speech. The emotions considered for the experiments include neutral, anger, joy and sadness. The distinuishability of emotional features in speech were studied first followed by emotion classification performed on a custom dataset. The classification was performed for different classifiers. One of the main feature attribute considered in the prepared dataset was the peak-to-peak distance obtained from the graphical representation of the speech signals. After performing the classification tests on a dataset formed from 30 different subjects, it was found that for getting better accuracy, one should consider the data collected from one person rather than considering the data from a group of people.},
author = {Davletcharova, Assel and Sugathan, Sherin and Abraham, Bibia and James, Alex Pappachen},
doi = {10.1016/j.procs.2015.08.032},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/1-s2.0-S1877050915021432-main.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Emotion Analysis,Emotion Classification,Mel-Frequency Cepstral Coefficients,Speech Processing},
pages = {91--96},
publisher = {Elsevier Masson SAS},
title = {{Detection and Analysis of Emotion from Speech Signals}},
url = {http://dx.doi.org/10.1016/j.procs.2015.08.032},
volume = {58},
year = {2015}
}
@article{Smith1975,
abstract = {Using the voices of six subjects, representing various social and educational backgrounds, fifty-four synthetic voices were generated by computer. Each normal voice was both increased and decreased in rate by 121/2, 25, 371/2, and 50 per cent. Judges evaluated the fifty-four voices using a series of adjectives representing two major personality factors of “competence” and “benevolence”. Several statistical analyses were performed, and it was found that the competence factor was much more sensitive to rate manipulations than was the benevolence factor. Ratings of competence were found to increase as rate increases and decrease as rate decreases, in a linear fashion. Benevolence had an inverted U-relationship with speech rate; the highest benevolence ratings occurred with normal speech rate. {\textcopyright} 1975, Sage Publications. All rights reserved.},
author = {Smith, Bruce L. and Brown, Bruce L. and Strong, William J. and Rencher, Alvin C.},
doi = {10.1177/002383097501800203},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/EffectsofSpeechRateonPersonalityPerception.pdf:pdf},
issn = {00238309},
journal = {Language and Speech},
number = {2},
pages = {145--152},
title = {{Effects of speech rate on personality perception}},
volume = {18},
year = {1975}
}
@article{Anggraeni2018,
abstract = {In this paper describe an implementation of speech recognition to pick and place an object using Robot Arm. To get the feature extraction of speech signal used Mel-Frequency Cepstrum Coefficients (MFCC) method and to learn the database of speech recognition used Support Vector Machine (SVM) method, the algorithm based on Python 2.7. The data learning which used to SVM process are 12 features, then the system tested using trained and not trained data show the best agreement to identifying the speech recognition. The speech recognition system has been implemented for control the 5 DoF Robot Arm based Arduino microcontroller to doing task pick and place the object.},
author = {Anggraeni, D. and Sanjaya, W. S.M. and Nurasyidiek, M. Y.S. and Munawwaroh, M.},
doi = {10.1088/1757-899X/288/1/012042},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Anggraeni{\_}2018{\_}IOP{\_}Conf.{\_}Ser.{\_}{\_}Mater.{\_}Sci.{\_}Eng.{\_}288{\_}012042.pdf:pdf},
issn = {1757899X},
journal = {IOP Conference Series: Materials Science and Engineering},
number = {1},
title = {{The Implementation of Speech Recognition using Mel-Frequency Cepstrum Coefficients (MFCC) and Support Vector Machine (SVM) method based on Python to Control Robot Arm}},
volume = {288},
year = {2018}
}
@article{Processing2015,
author = {Processing, Social Signal},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Emotion{\_}recognition{\_}through{\_}voice{\_}analys.pdf:pdf},
title = {{Research paper Social Signal Processing !}},
year = {2015}
}
@article{Dasgupta2017,
abstract = {The ability to modulate vocal sounds and generate speech is one of the features which set humans apart from other living beings. The human voice can be characterized by several attributes such as pitch, timbre, loudness, and vocal tone. It has often been observed that humans express their emotions by varying different vocal attributes during speech generation. Hence, deduction of human emotions through voice and speech analysis has a practical plausibility and could potentially be beneficial for improving human conversational and persuasion skills. This paper presents an algorithmic approach for detection and analysis of human emotions with the help of voice and speech processing. The proposed approach has been developed with the objective of incorporation with futuristic artificial intelligence systems for improving human-computer interactions.},
author = {Dasgupta, Poorna Banerjee},
doi = {10.14445/22312803/ijctt-v52p101},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/1710.10198.pdf:pdf},
journal = {International Journal of Computer Trends and Technology},
keywords = {artificial intelligence,emotions,human,speech processing,voice processing},
number = {1},
pages = {1--3},
title = {{Detection and Analysis of Human Emotions through Voice and Speech Pattern Processing}},
volume = {52},
year = {2017}
}
@article{Pell2008,
abstract = {To test ideas about the universality and time course of vocal emotion processing, 50 English listeners performed an emotional priming task to determine whether they implicitly recognize emotional meanings of prosody when exposed to a foreign language. Arabic pseudo-utterances produced in a happy, sad, or neutral prosody acted as primes for a happy, sad, or 'false' (i.e., non-emotional) face target and participants judged whether the facial expression represents an emotion. The prosody-face relationship (congruent, incongruent) and the prosody duration (600 or 1000 ms) were independently manipulated in the same experiment. Results indicated that English listeners automatically detect the emotional significance of prosody when expressed in a foreign language, although activation of emotional meanings in a foreign language may require increased exposure to prosodic information than when listening to the native language. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Pell, Marc D. and Skorup, Vera},
doi = {10.1016/j.specom.2008.03.006},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Implicit{\_}processing{\_}of{\_}emotional{\_}prosody.pdf:pdf},
issn = {01676393},
journal = {Speech Communication},
keywords = {Cross-linguistic,Cultural factors,Semantic priming,Speech processing,Vocal expression},
number = {6},
pages = {519--530},
title = {{Implicit processing of emotional prosody in a foreign versus native language}},
volume = {50},
year = {2008}
}
@article{Juslin2003,
abstract = {Many authors have speculated about a close relationship between vocal expression of emotions and musical expression of emotions, but evidence bearing on this relationship has unfortunately been, lacking. This review of 104 studies of vocal expression and 41 studies of music performance reveals similarities between the 2 channels concerning (a) the accuracy with which discrete emotions were communicated to listeners and (b) the emotion-specific patterns of acoustic cues used to communicate each emotion. The patterns are generally consistent with K. R. Scherer's (1986) theoretical predictions. The results can explain why music is perceived as expressive of emotion, and they are consistent with an evolutionary perspective on vocal expression of emotions. Discussion focuses on theoretical accounts and directions for future research.},
author = {Juslin, Patrik N. and Laukka, Petri},
doi = {10.1037/0033-2909.129.5.770},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/c{\_}510552-l{\_}1-k{\_}juslin{\_}emotion2003.pdf:pdf},
issn = {00332909},
journal = {Psychological Bulletin},
number = {5},
pages = {770--814},
title = {{Communication of Emotions in Vocal Expression and Music Performance: Different Channels, Same Code?}},
volume = {129},
year = {2003}
}

@article{Chen2017,
	author = {Chen, Si and Zhu, Yiqing and Wayland, Ratree},
	year = {2017},
	month = {07},
	pages = {e0180656},
	title = {Effects of stimulus duration and vowel quality in cross-linguistic categorical perception of pitch directions},
	volume = {12},
	journal = {PLoS ONE},
	doi = {10.1371/journal.pone.0180656}
}

@article{Langari2020,
	abstract = {One of the most important issues in human-computer interaction is to create a system that can hear and respond correctly like a human. This has led to the design of the Automatic Speech Emotion Recognition system (SER) that is able to identify different emotional classes by extracting and selecting effective features from speech signals. For this reason, in this study, we propose a novel feature extraction method based on adaptive time-frequency coefficients to improve the SER. The simulations are performed using the Berlin Emotional Speech Database (EMO-DB), the Surrey Audio-Visual Expressed Emotion Database (SAVEE), and the Persian Drama Radio Emotional Corpus (PDREC). The main contribution of our work is to extract novel features, called Adaptive Time-Frequency features, based on the Fractional Fourier Transform and to combine them with Cepstral features. Experimental results show that the proposed method effectively identifies different emotional classes in EMO-DB (97.57{\%} accuracy), SAVEE (80{\%} accuracy), and PDREC (91.46{\%} accuracy) data sets.},
	author = {Langari, Shadi and Marvi, Hossein and Zahedi, Morteza},
	doi = {10.1016/j.imu.2020.100424},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Langari, Marvi, Zahedi - 2020 - Efficient speech emotion recognition using modified feature extraction.pdf:pdf},
	issn = {23529148},
	journal = {Informatics in Medicine Unlocked},
	keywords = {Evolutionary algorithms,Feature extraction,Feature selection,Human-computer interaction,Speech emotion recognition},
	pages = {100424},
	publisher = {Elsevier Ltd},
	title = {{Efficient speech emotion recognition using modified feature extraction}},
	url = {https://doi.org/10.1016/j.imu.2020.100424},
	volume = {20},
	year = {2020}
}
@article{Hellbernd2016,
	abstract = {Action-theoretic views of language posit that the recognition of others' intentions is key to successful interpersonal communication. Yet, speakers do not always code their intentions literally, raising the question of which mechanisms enable interlocutors to exchange communicative intents. The present study investigated whether and how prosody-the vocal tone-contributes to the identification of "unspoken" intentions. Single (non-)words were spoken with six intonations representing different speech acts-as carriers of communicative intentions. This corpus was acoustically analyzed (Experiment 1), and behaviorally evaluated in two experiments (Experiments 2 and 3). The combined results show characteristic prosodic feature configurations for different intentions that were reliably recognized by listeners. Interestingly, identification of intentions was not contingent on context (single words), lexical information (non-words), and recognition of the speaker's emotion (valence and arousal). Overall, the data demonstrate that speakers' intentions are represented in the prosodic signal which can, thus, determine the success of interpersonal communication.},
	author = {Hellbernd, Nele and Sammler, Daniela},
	doi = {10.1016/j.jml.2016.01.001},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hellbernd, Sammler - 2016 - Prosody conveys speaker's intentions Acoustic cues for speech act perception.pdf:pdf},
	issn = {0749596X},
	journal = {Journal of Memory and Language},
	keywords = {Acoustics,Intention,Pragmatics,Prosody,Speech acts},
	pages = {70--86},
	title = {{Prosody conveys speaker's intentions: Acoustic cues for speech act perception}},
	volume = {88},
	year = {2016}
}
@article{Rashid2018,
	abstract = {Speech is a complex naturally acquired human motor ability. It is characterized in adults with the production of about 14 different sounds per second via the harmonized actions of roughly 100 muscles. Speaker recognition is the capability of a software or hardware to receive speech signal, identify the speaker present in the speech signal and recognize the speaker afterwards. Feature extraction is accomplished by changing the speech waveform to a form of parametric representation at a relatively minimized data rate for subsequent processing and analysis. Therefore, acceptable classification is derived from excellent and quality features. Mel Frequency Cepstral Coefficients (MFCC), Linear Prediction Coeffi- cients (LPC), Linear Prediction Cepstral Coefficients (LPCC), Line Spectral Frequencies (LSF), Discrete Wavelet Transform (DWT) and Perceptual Linear Prediction (PLP) are the speech feature extraction techniques that were discussed in these chapter. These methods have been tested in a wide variety of applications, giving them high level of reliability and acceptability. Researchers have made several modifications to the above discussed tech- niques to make them less susceptible to noise, more robust and consume less time. In conclusion, none of the methods is superior to the other, the area of application would determine which method to select.},
	author = {Rashid, Sabur Ajibola Alim and Nahrul Khair Alang},
	booktitle = {From Natural to Artificial Intelligence: Algorithms and Applications},
	chapter = {Chapter 1},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rashid - 2018 - Some Commonly Used Speech Feature Extraction Algorithms.pdf:pdf},
	keywords = {discrete wavelet transform (DWT),human speech,line spectral frequencies (LSF),linear prediction cepstral coefficients (LPCC),linear prediction coefficients (LPC),mel frequency cepstral coefficients (MFCC),perceptual linear prediction (PLP),speech features},
	number = {tourism},
	pages = {13},
	title = {{Some Commonly Used Speech Feature Extraction Algorithms}},
	url = {https://www.intechopen.com/books/advanced-biometric-technologies/liveness-detection-in-biometrics},
	year = {2018}
}
@article{Farooq2020,
	abstract = {Speech emotion recognition (SER) plays a significant role in human-machine interaction. Emotion recognition from speech and its precise classification is a challenging task because a machine is unable to understand its context. For an accurate emotion classification, emotionally relevant features must be extracted from the speech data. Traditionally, handcrafted features were used for emotional classification from speech signals; however, they are not efficient enough to accurately depict the emotional states of the speaker. In this study, the benefits of a deep convolutional neural network (DCNN) for SER are explored. For this purpose, a pretrained network is used to extract features from state-of-the-art speech emotional datasets. Subsequently, a correlation-based feature selection technique is applied to the extracted features to select the most appropriate and discriminative features for SER. For the classification of emotions, we utilize support vector machines, random forests, the k-nearest neighbors algorithm, and neural network classifiers. Experiments are performed for speaker-dependent and speaker-independent SER using four publicly available datasets: the Berlin Dataset of Emotional Speech (Emo-DB), Surrey Audio Visual Expressed Emotion (SAVEE), Interactive Emotional Dyadic Motion Capture (IEMOCAP), and the Ryerson Audio Visual Dataset of Emotional Speech and Song (RAVDESS). Our proposed method achieves an accuracy of 95.10% for Emo-DB, 82.10% for SAVEE, 83.80% for IEMOCAP, and 81.30% for RAVDESS, for speaker-dependent SER experiments. Moreover, our method yields the best results for speaker-independent SER with existing handcrafted features-based SER approaches.},
	author = {Farooq, Misbah and Hussain, Fawad and Baloch, Naveed Khan and Raja, Fawad Riasat and Yu, Heejung and Zikria, Yousaf Bin},
	doi = {10.3390/s20216008},
	file = {:home/luisa/Downloads/sensors-20-06008.pdf:pdf},
	issn = {14248220},
	journal = {Sensors (Switzerland)},
	keywords = {Correlation-based feature selection,Deep convolutional neural network,Speech emotion recognition},
	number = {21},
	pages = {1--18},
	pmid = {33113907},
	title = {{Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network}},
	volume = {20},
	year = {2020}
}
@article{AbdulQayyum2019,
	abstract = {Speech is considered as the widest and most natural medium of communication. Speech can convey a plethora of information regarding one's mental, behavioral, emotional traits. Besides, speech-emotion recognition related work can aid in averting cyber crimes. Research on speech-emotion recognition exploiting concurrent machine learning techniques has been on the peak for some time. Numerous techniques like Recurrent Neural Network (RNN), Deep Neural Network (DNN), spectral feature extraction and many more have been applied on different datasets. This paper presents a unique Convolutional Neural Network (CNN) based speech-emotion recognition system. A model is developed and fed with raw speech from specific dataset for training, classification and testing purposes with the help of high end GPU. Finally, it comes out with a convincing accuracy of 83.61{\%} which is better compared to any other similar task on this dataset by a large margin. This work will be influential in developing conversational and social robots and allocating all the nuances of their sentiments.},
	author = {{Abdul Qayyum}, Alif Bin and Arefeen, Asiful and Shahnaz, Celia},
	doi = {10.1109/SPICSCON48833.2019.9065172},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abdul Qayyum, Arefeen, Shahnaz - 2019 - Convolutional Neural Network (CNN) Based Speech-Emotion Recognition.pdf:pdf},
	isbn = {9781728142944},
	journal = {2019 IEEE International Conference on Signal Processing, Information, Communication and Systems, SPICSCON 2019},
	keywords = {Convolutional Neural Network (CNN),Deep Neural Network (DNN),Discrete Cosine Transform (DCT),Emotion,Emotional state,Modulation Spectral Features (MSF),Multi class,Multivariate Linear Regression Classification (MLR,Recurrent Neural Network (RNN),SAVEE dataset,Speech Processing,Speech-emotion,Support Vector Machines (SVM)},
	number = {November},
	pages = {122--125},
	title = {{Convolutional Neural Network (CNN) Based Speech-Emotion Recognition}},
	year = {2019}
}
@article{Sarkania2013,
	abstract = {Automatic emotion of detection in speech is a latest research area in the field of human machine interaction and speech processing. The aim of this paper is to enable a very natural interaction among human and machine. This dissertation proposes an approach to recognize the user's emotional state by analysing signal of human speech. To achieve the good extraction of the feature from the signal the propose technique uses the high pass filter before the feature extraction process. High pass filter uses to reduce the noise. High pass filter pass only high frequency and attenuates the lower frequency. This paper uses the Neural Network as a classifier to classify the different emotional states such as happy, sad, anger etc from emotional speech database. For the performance of classification use the speech feature such as Mel Frequency cepstrum coefficient (MFCC). The result shows that the Neural Network used as a classifier is a feasible technique for the emotional classification. By using the high pass filter performance should be increase.},
	author = {Sarkania, Vaibhav Kumar and Bhalla, Vinod Kumar},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarkania, Bhalla - 2013 - International Journal of Advanced Research in.pdf:pdf},
	journal = {Android Internals},
	keywords = {- android boot up,activity lifecycle,dalvik vm,zygote},
	number = {6},
	pages = {143--147},
	title = {{Emotion Recognition through Speech Using Neural Network}},
	volume = {3},
	year = {2015}
}
@article{Lim2017,
	abstract = {With rapid developments in the design of deep architecture models and learning algorithms, methods referred to as deep learning have come to be widely used in a variety of research areas such as pattern recognition, classification, and signal processing. Deep learning methods are being applied in various recognition tasks such as image, speech, and music recognition. Convolutional Neural Networks (CNNs) especially show remarkable recognition performance for computer vision tasks. In addition, Recurrent Neural Networks (RNNs) show considerable success in many sequential data processing tasks. In this study, we investigate the result of the Speech Emotion Recognition (SER) algorithm based on CNNs and RNNs trained using an emotional speech database. The main goal of our work is to propose a SER method based on concatenated CNNs and RNNs without using any traditional hand-crafted features. By applying the proposed methods to an emotional speech database, the classification result was verified to have better accuracy than that achieved using conventional classification methods.},
	author = {Lim, Wootaek and Jang, Daeyoung and Lee, Taejin},
	doi = {10.1109/APSIPA.2016.7820699},
	file = {:C$\backslash$:/Users/Usuario/Documents/Downloads/137.pdf:pdf},
	isbn = {9789881476821},
	journal = {2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016},
	title = {{Speech emotion recognition using convolutional and Recurrent Neural Networks}},
	year = {2017}
}
@article{Lee2015,
	abstract = {This paper presents a speech emotion recognition system using a recurrent neural network (RNN) model trained by an efficient learning algorithm. The proposed system takes into account the long-range contextual effect and the uncertainty of emotional label expressions. To extract high-level representation of emotional states with regard to its temporal dynamics, a powerful learning method with a bidirectional long short-term memory (BLSTM) structure is adopted. To overcome the uncertainty of emotional labels, such that all frames in the same utterance are mapped to the same emotional label, it is assumed that the label of each frame is regarded as a sequence of random variables. The sequences are then trained by the proposed learning algorithm. The weighted accuracy of the proposed emotion recognition system is improved up to 12{\%} compared to the DNNELM- based emotion recognition system used as a baseline.},
	author = {Lee, Jinkyu and Tashev, Ivan},
	file = {:C$\backslash$:/Users/Usuario/Documents/Downloads/Lee-Tashev{\_}Emotion{\_}detection{\_}Interspeech2015.pdf:pdf},
	issn = {19909772},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	keywords = {Long short-term memory,Recurrent neural network,Speech emotion recognition},
	pages = {1537--1540},
	title = {{High-level feature representation using recurrent neural network for speech emotion recognition}},
	volume = {2015-January},
	year = {2015}
}
@article{Harar2017,
	abstract = {This paper describes a method for Speech Emotion Recognition (SER) using Deep Neural Network (DNN) architecture with convolutional, pooling and fully connected layers. We used 3 class subset (angry, neutral, sad) of German Corpus (Berlin Database of Emotional Speech) containing 271 labeled recordings with total length of 783 seconds. Raw audio data were standardized so every audio file has zero mean and unit variance. Every file was split into 20 millisecond segments without overlap. We used Voice Activity Detection (VAD) algorithm to eliminate silent segments and divided all data into TRAIN (80{\%}) VALIDATION (10{\%}) and TESTING (10{\%}) sets. DNN is optimized using Stochastic Gradient Descent. As input we used raw data without and feature selection. Our trained model achieved overall test accuracy of 96.97{\%} on whole-file classification.},
	author = {Harar, Pavol and Burget, Radim and Dutta, Malay Kishore},
	doi = {10.1109/SPIN.2017.8049931},
	file = {:C$\backslash$:/Users/Usuario/Downloads/Speech-Emotion-Recognition-with-Deep-Learning.pdf:pdf},
	isbn = {9781509027972},
	journal = {2017 4th International Conference on Signal Processing and Integrated Networks, SPIN 2017},
	number = {February 2017},
	pages = {137--140},
	title = {{Speech emotion recognition with deep learning}},
	year = {2017}
}
@article{Wang2020,
	abstract = {Speech Emotion Recognition (SER) has emerged as a critical component of the next generation of human-machine interfacing technologies. In this work, we propose a new duallevel model that predicts emotions based on both MFCC features and mel-spectrograms produced from raw audio signals. Each utterance is preprocessed into MFCC features and two mel-spectrograms at different time-frequency resolutions. A standard LSTM processes the MFCC features, while a novel LSTM architecture, denoted as Dual-Sequence LSTM (DSLSTM), processes the two mel-spectrograms simultaneously. The outputs are later averaged to produce a final classification of the utterance. Our proposed model achieves, on average, a weighted accuracy of 72.7{\%} and an unweighted accuracy of 73.3{\%}-a 6{\%} improvement over current state-of-the-art unimodal models-and is comparable with multimodal models that leverage textual information as well as audio signals.},
	archivePrefix = {arXiv},
	arxivId = {1910.08874},
	author = {Wang, Jianyou and Xue, Michael and Culhane, Ryan and Diao, Enmao and Ding, Jie and Tarokh, Vahid},
	doi = {10.1109/ICASSP40776.2020.9054629},
	eprint = {1910.08874},
	file = {:C$\backslash$:/Users/Usuario/Documents/Downloads/1910.08874.pdf:pdf},
	isbn = {9781509066315},
	issn = {15206149},
	journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	keywords = {Dual-Level Model,Dual-Sequence LSTM,LSTM,Mel-Spectrogram,Speech Emotion Recognition},
	pages = {6474--6478},
	title = {{Speech emotion recognition with dual-sequence LSTM architecture}},
	volume = {2020-May},
	year = {2020}
}

@article{Pell2008,
	abstract = {To test ideas about the universality and time course of vocal emotion processing, 50 English listeners performed an emotional priming task to determine whether they implicitly recognize emotional meanings of prosody when exposed to a foreign language. Arabic pseudo-utterances produced in a happy, sad, or neutral prosody acted as primes for a happy, sad, or 'false' (i.e., non-emotional) face target and participants judged whether the facial expression represents an emotion. The prosody-face relationship (congruent, incongruent) and the prosody duration (600 or 1000 ms) were independently manipulated in the same experiment. Results indicated that English listeners automatically detect the emotional significance of prosody when expressed in a foreign language, although activation of emotional meanings in a foreign language may require increased exposure to prosodic information than when listening to the native language. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
	author = {Pell, Marc D. and Skorup, Vera},
	doi = {10.1016/j.specom.2008.03.006},
	file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Implicit{\_}processing{\_}of{\_}emotional{\_}prosody.pdf:pdf},
	issn = {01676393},
	journal = {Speech Communication},
	keywords = {Cross-linguistic,Cultural factors,Semantic priming,Speech processing,Vocal expression},
	number = {6},
	pages = {519--530},
	title = {{Implicit processing of emotional prosody in a foreign versus native language}},
	volume = {50},
	year = {2008}
}

@article{Varshney,
	author = {Varshney, Lav and Sun, John},
	year = {2013},
	month = {02},
	pages = {},
	title = {Why do we perceive logarithmically?},
	volume = {10},
	journal = {Significance},
	doi = {10.1111/j.1740-9713.2013.00636.x}
}

@article{StevensVolkmann,
	ISSN = {00029556},
	URL = {http://www.jstor.org/stable/1417526},
	author = {S. S. Stevens and J. Volkmann},
	journal = {The American Journal of Psychology},
	number = {3},
	pages = {329--353},
	publisher = {University of Illinois Press},
	title = {The Relation of Pitch to Frequency: A Revised Scale},
	volume = {53},
	year = {1940}
}

@dataset{SAVEEdataset,
	author = {Jackson, Philip and ul haq, Sana},
	year = {2011},
	month = {04},
	pages = {},
	title = {Surrey Audio-Visual Expressed Emotion (SAVEE) database}
}

@inproceedings{librosa082,
	author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel and Mcvicar, Matt and Battenberg, Eric and Nieto, Oriol},
	year = {2015},
	month = {01},
	pages = {18-24},
	title = {librosa: Audio and Music Signal Analysis in Python},
	doi = {10.25080/Majora-7b98e3ed-003}
}

@misc{Nair,
	author = {Nair, Pratheeksha},
	title = {{The dummy's guide to MFCC. Disclaimer 1 : This article is only an{\ldots} | by Pratheeksha Nair | prathena | Medium}},
	url = {https://medium.com/prathena/the-dummys-guide-to-mfcc-aceab2450fd},
	urldate = {2021-06-03}
}










