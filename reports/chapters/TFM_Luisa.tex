\documentclass[11pt,a4paper,spanish]{book}
\usepackage{estilo_unir}
\usepackage[style=apa, natbib=true, backend=biber]{biblatex}
\usepackage{graphicx, float, hyperref, comment, multicol}
\graphicspath{ {./.images/} }
% Formato
%\usepackage[utf8]{inputenc}

% Define el nivel de profundidad de las secciones para numerarlas
\setcounter{secnumdepth}{3}
\addbibresource{./bibLib/referenceLib.bib}


%---------------------------
%título del trabajo y autor
%---------------------------
\title{Reconocimiento de Emociones en la Lengua no Aprendida}
\author{Luisa Sánchez Avivar}
\director{Ciro Rodríguez León}
\date{Junio del 2021}

%---------------------------
%marges
%---------------------------
%\usepackage[margin=1.9cm]{geometry}
%---------------------------
%---------------------------
%---------------------------
%---------------------------
\begin{document}
\renewcommand{\listfigurename}{Índice de Ilustraciones}
\renewcommand{\listtablename}{Índice de Tablas}
\renewcommand{\contentsname}{Índice de Contenidos}
\renewcommand{\figurename}{Figura}
\renewcommand{\tablename}{Tabla} 

\maketitle


\chapter{Resumen}
{\bf Nota:} En este apartado se introducirá un breve resumen en español del trabajo realizado (extensión máxima: 150 palabras). Este resumen debe incluir el objetivo o propósito de la investigación, la metodología, los resultados y las conclusiones.


{\bf Palabras Clave:} Se deben incluir de 3 a 5 palabras claves en español

\chapter{Abstract}
{\bf Nota:} En este apartado se introducirá un breve resumen en español del trabajo realizado (extensión máxima: 150 palabras). Este resumen debe incluir el objetivo o propósito de la investigación, la metodología, los resultados y las conclusiones.


{\bf Palabras Clave:} Se deben incluir de 3 a 5 palabras claves en inglés




\mainmatter
\chapter{Introducción}


	\section{Motivación}
	Es indudable el impacto que ha creado la inteligencia artificial en la forma en la que nos comunicamos con las máquinas a día de hoy. La importancia de la interacción con las máquinas a través de comandos de voz, se ha visto acentuada gracias a la aparición de asistentes inteligentes como Siri (Apple) o Alexa (Amazon), que han explotado las diferentes áreas del análisis de la voz con el objetivo de mejorar la experiencia de usuario. Otras compañías como OTO han desarrollado modelos de análisis de voz capaces de detectar atributos únicos en la voz del interlocutor, lo que es usando por centros de asistencia telefónica para potenciar y mejorar sus sistemas automáticos.
	En definitiva, desde el primer software de reconocimiento por voz que fue presentado por IBM en 1961 reconociendo 16 palabras y dígitos, hasta la aparición de Google Home en 2017, ente tipo de asistentes han ido mejorando su alcance y capacidades.\\
	
	El uso de estos asistentes no sólo se limita al ámbito doméstico, por ejemplo, la industria del videojuego creció un 23\% durante la pandemia del Covid-19 en 2020 más que en el año anterior, 2019. La tendencia en el uso de este dominio empieza a extenderse hasta en la aplicación de asistentes personalizados para coches automáticos y asistencia de ayuda telefónica. 
	
	Sin embargo, a pesar de los avances tecnológicos, estos asistentes de voz normalmente carecen de la habilidad de reconocer el estado emocional del usuario, y cerrar esta brecha podría ser un gran avance en las industrias ya mencionadas. Por ejemplo, Facebook usa inteligencia emocional para monitorizar signos de depresión en los usuarios.
	
	Cabe pensar que en este tipo de tecnología haya un potencial interés para la asistencia sanitaria, o incluso, para la industria automovilística. Visualicemos por ejemplo, un conductor tratando de resolver una incidencia mientras conduce. Esta incidencia puede variar desde buscar una ruta alternativa a un hospital o servicio de emergencia, y el estado emocional en el que se encuentre, puede afectar limitando su habilidad para resolver el problema. 
	
	De la misma manera el reconocimiento de emociones puede ocupar un lugar en los asistentes virtuales de cualquier servicio al integrarlo con técnicas del procesamiento del lenguaje natural, permitiendo mayor eficiencia del procesamiento de la conversación al detectar -por ejemplo-  irritabilidad o frustración en el usuario. 
	
	El espectro emocional que una persona esconde en su discurso es un factor esencial de la comunicación humana y ofrece información sin necesidad de alterar el contenido lingüístico. Es aquí donde reside el potencial de este área y por lo que cabe preguntarse si ese reconocimiento emocional a través de la voz, está fuertemente ligado al idioma y la cultura, o hay emociones que podemos detectar independientemente de este. Por ejemplo, hay áreas con una diversidad cultura y lingüística muy diversa, sólo en Zimbabwe hay 16 lenguas oficiales, o 4 en Suiza. Yace aquí la necesidad de desvincular esta dependencia, lo que podría crear un impulso en el desarrollo de estos sistemas sin la necesidad de un corpus específico, y al mismo tiempo ayudarnos a entender la relación entre la expresión de emociones y la lengua.
	La idea de este proyecto nació por la motivación de crear un sistema capaz de crear una respuesta, no sólo coherente en el plano semántico, si no también sensible al estado emocional del usuario. Dado el alcance ambicioso con el que se partía, y teniendo en cuenta lo anterior, hemos preferido centrarnos en el estudio del reconocimiento de esas emociones, aplicándolo a la lengua extranjera.
	
	
	
	\section{Planteamiento del Trabajo}
	Desde hace años, el reconocimiento de emociones a través de la voz ha sido motivo de interés para la investigación, sin embargo siempre se ha estudiado sobre un mismo lenguaje debatiendo la habilidad de reconocer y clasificar las emociones oralmente expresadas. Esta habilidad ha sido respaldada por numerosos artículos donde se concluye que es posible distinguir e identificar entre al menos cuatro emociones básicas (felicidad, tristeza, y enfado) a través de la voz (sin necesidad del procesamiento del lenguaje natural y por lo tanto de un contexto).
	
	Atendiendo al estudio de las emociones expresadas según la lengua existen estudios donde se demuestra que individuos de diferentes culturas pueden reconocer emociones básicas en diferentes niveles, pero es menos abundante la evidencia de un acuerdo en cómo las emociones básicas son reconocidas desde la expresión vocal de un interlocutor.% \cite{Pell2009a}
	Análogamente el debate del reconocimiento de emociones en un plano intercultural también se ha enfocado a través del estudio de los gestos faciales en conjunto con la expresión vocal, donde se concluye los factores sociales tienen un gran impacto, ya que la identificación de las emociones es más fácil para los miembros de la misma cultura que para los de otra distinta \citep{Pell2009a} y \citep{Pell2009}. A pesar de ello hay una gran carencia de comparativas con respecto a la voz donde se demuestre una sólida influencia cultural, sin embargo parece claro que las dimensiones socio culturales que engloban nuestras interacciones pueden tener un gran impacto en nuestra comunicación dentro de un marco emocional.
	
	Este trabajo de fin de máster se centra en el uso de técnicas basadas en redes neuronales para la clasificación de emociones en el tracto vocal en la lengua extranjera. Para acercarnos a este escenario, se parte del supuesto que dado un modelo entrenado en un lenguaje capaz de reconocer emociones en este, se evalúa en un idioma distinto que nunca ha formado parte del anterior conjunto de datos. 
	
	Con este estudio se pretende entender mejor la relación entre emoción e idioma y arrojar luz a preguntas como qué emociones son más fácilmente reconocibles indistintamente del lenguaje. 
	
	\chapter{Estado del Arte}
		\section{Contexto}
		El reconocimiento de emociones en el habla es una disciplina en inteligencia artificial que trata de reconocer y clasificar emociones a través de la señal de voz. Este campo de estudio se ha hecho cada vez más popular, pero su origen se remonta a 1996, desde que se presentara el primer trabajo defendible sobre el tema "Reconociendo emociones en el habla" de la mano de F.Daellert \citep{Dellaert1996}.
		Desde entonces, el reconocimiento de emociones a través de la voz ha sido motivo de interés para la investigación, sin embargo en su gran mayoría, se ha estudiado sobre un mismo lenguaje debatiendo la habilidad de reconocer y clasificar las emociones oralmente expresadas. 
		
		
		\subsection{Temas fonéticos}
		El objetivo del reconocimiento de emociones en el habla es reconocer el trasfondo emocional del mensaje a través de la voz. Esta manifestación sonora posee factores clave para la comunicación humana que ayudan en su interacción sin alterar el contexto del mensaje.\\
		
		La expresión de las emociones están íntimamente relacionadas con las propiedades fonéticas en el habla donde se observan señales y patrones para marcar contrastes lingüísticos en un idioma \cite{Pell2001} por lo tanto, los efectos del lenguaje en la comunicación emocional son evidentes al haber sido observadas y medidas, las variaciones en el rango tonal y la frecuencia para expresarlas, cambiando no sólo el tono si no también el patrón lingüístico asociado \citep{Davletcharova2015}.
		Por otro lado tanto la proporción de consonantes y vocales (que hacen variar la presión de aire que se necesita) como el ratio de sílabas por palabra en cada idioma, caracterizan la expresión oral de las emociones. Existen muchos factores relacionados con el lenguaje como la morfología o la duración del estímulo que podrían ser un impacto en la decodificación de los matices en la señal vocal, tal y como se explica en \citep{Chen2017}.
		Existe una clasificación dependiendo de la velocidad silábica en la expresión de dichos idiomas, sin embargo poco se conoce acerca de los efectos en las medidas respiratorias en el habla. Esta observación puede llevar a que se pregunte si en lenguajes tan dispares, las emociones expresadas mediante la voz puedan ser reconocidas desde el punto de vista del otro idioma.
		Normalmente estos estudios se llevan a cabo en un único lenguaje, lo que en el caso que nos acontece, se traduciría como el reconocimiento de emociones llevado a cabo en la lengua materna; Mientras este ejercicio puede llegar a ser intuitivo, distinguir las mismas emociones en la lengua extranjera supone un reto ya que implicaría importantes matices culturales. Por ejemplo, no sería lo mismo entender qué emociones intenta expresar un italo parlante desde el punto de vista de una persona que entiende el español (ambas lenguas latinas), que comprender las mismas emociones del discurso desde un germano hablante. Así bien, es importante definir qué idioma se está reconociendo y desde cuál, por lo que analizar las raíces lingüísticas y fonéticas de los idiomas a estudiar es esencial. \hfill \break
		
		\subsection{Reconocimiento del habla}
		Atendiendo a la manera de como modelar las emociones, según la literatura, la clasificación de emociones se ha tratado desde dos enfoques principales:
		\begin{itemize}
			\item Las emociones como categorías discretas
			\item Las emociones vistas a través de un modelo dimensional
		\end{itemize}
		
		En el primer punto, a todos los humanos se les atribuye un conjunto de emociones básicas que pueden ser reconocidas interculturalmente. El debate se centra en la definición de dichas emociones, y fue Paul Elkman y su equipo en 1992 \citep{Ekman1992} quien estableció que estas eran 6: enfado, asco, miedo , felicidad, tristeza y sorpresa.
		En el segundo punto, las emociones se definen respecto a una o más dimensiones, donde normalmente las dimensiones que se comprenden tienden a ser la afectividad, la excitación o la intensidad. Aquí la discusión se centra en encontrar el número de dimensiones que de lugar a un modelo coherente y pueda incluir las emociones conocidas. El modelo de Plutchik \citep{Plutchik2001} sea quizá el más conocido en este enfoque, y propone un modelo tridimensional que organiza las emociones en círculos concéntricos situando la más básicas en el centro. \hfill \break
		
		Sin embargo la mayoría de los estudios centrados en la clasificación de emociones se centran en las emociones vistas como categorías discretas.
		
		\subsubsection{Extracción de Características}
		La extracción de características es una de las secciones más importantes en el reconocimiento de emociones a través de la voz debido a la ambigüedad de las características y la variedad vocal. La extracción de características es el paso principal en el procesamiento del diálogo, y se lleva a cabo para centrarse en la información contenida en la señal y mejorar el grado de similitud y/o diferenciación entre las clases \citep{Hellbernd2016}. Hasta ahora, por lo general hay dos enfoques principales  con respecto al tipo de características usadas en el Reconocimiento de Emociones en el Discurso:\\
		Los rasgos prosódicos, los cuales extraen información de la prosodia, en concreto, tono, energía y duración, y por otro lado, las características del tracto vocal que normalmente indican la distribución de la energía en la frecuencia del rango vocal (conocidos como Coeficientes Cepstrales).
		La mayoría de los estudios centrados en este tema usan rasgos espectrales como la información extraída del tracto vocal, lo que supone obtener la información derivada del espectro de la señal de la voz y se usan para modelar los patrones de entonación y frecuencia del hablante \citep{Langari2020}.\\
		
		\begin{comment} 
		Buscar referencias para cada uno
		Añadir quiza STFT ?
		Hablar de la escala de Mel: http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/
		\end{comment}
		
		En \citep{Rashid2018} se ofrece una breve explicación de cada una de las técnicas más comunes analizando sus puntos fuertes y débiles. Así pues, podemos encontrar la Transformada Wavelets Discreta (DWT) que a pesar de mejorar la información que se obtiene del diálogo en la correspondiente banda de frecuencia presenta variaciones indeseadas en los límites debido a que las señales de entrada son de una longitud finita. También podemos encontrar trabajos donde se usen Coeficientes de Predicción Lineal (LPC) los cuales hacen estimaciones bastante precisas al extraer las propiedades del tracto vocal, pero son altamente sensibles al ruido de cuantificación, por lo que demuestran no ser precisos cuando hay ruido de fondo. \\
		
		No obstante, a lo largo de los últimos años se ha popularizado el uso de otros métodos reportando mejores resultados, estos son:
		
		\paragraph{Coeficientes Cepstrales con Predicción Lineal (LPCC)}
		Calcula una envolvente a los Coeficientes de Predicción Lineal (LPC) y luego hace una conversión a coeficientes cepstrales; Esto materializa las características de un canal particular del sonido, teniendo en cuenta que la misma persona con diferentes tonos emocionales tendrá diferentes canales de características, se podrán extraer esos coeficientes para indetificar las emociones contenidas \citep{Sandesara2020}.
		Tiene una baja vulnerabilidad al ruido de fondo y mejora el ratio de error en comparación a LPC, pero sigue teniendo una gran sensibilidad al ruido de cuantificación.\hfill \break
		
		\paragraph{Coeficientes Cepstrales en la escala de Mel (MFCC)}
		Se basa en la desintegración de la señal para tener como resultado un resumen de las características que la forman. La obtención de este conjunto de valores numéricos se basa por un lado, en el rango de frecuencias de Mel, el cual consiste en una adaptación de frecuencias de la señal a aquellas más fácilmente percibidas por el oído humano, y por lo otro lado, la separación de frecuencias mediante lo que llamamos cepstrales (\emph{Cepstrum}) que divide la señal en dos bandas de frecuencias: baja (correspondientes a los fonemas producidos por el tracto vocal) y alta (correspondientes a la excitación de las cuerdas vocales). \\
		Debido a esto, encapsula la mayor parte de energía proveniente del sonido que es generado por humanos, por lo que es frecuentemente usada y sugerida para identificar palabras monosilábicas en un discurso.\hfill \break
		
		Resumiéndolo, los objetivos clave, serían:
		\begin{itemize}
			\item Eliminar la excitación del tracto vocal.
			\item Independizar las características extraídas.
			\item Ajuste a como los humanos percibimos el ruido y la frecuencia del sonido.
			\item Capturar la dinámica fonética, que definirá el contexto.
		\end{itemize}
		MFCC constituye una perfecta representación para el sonido cuando la fuente es estable y consistente \citep{Farouk2014}. 

		
		\subsubsection{Procesamiento de la señal como imagen}
		
		No olvidemos que el tipo de dato con el que se trabajará principalmente serán señales, concretamente, de audio. En las señales de audio hay una cierta presión de aire que varía con respecto al tiempo, y al muestrearlas en un determinado rango de frecuencia, obtendríamos algo como lo siguiente:
		
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.3]{waveform.png}
			\caption{Forma de onda de una señal de audio}
		\end{figure}
		
		Esto que vemos es una representación digital de la onda, de manera que ahora puede ser interpretada y analizada fácilmente.\hfill \break
		El siguiente punto que deberíamos atender, sería preguntarnos cómo extraer características relevantes de esta representación, y para ello encontramos la respuesta en la Transformada de Fourier (FFT \emph{Fast Fourier Transform}), la cual nos permite analizar la cantidad de frecuencia contenida en una señal. La Transformada de Fourier transforma la señal de un dominio de tiempo a un dominio de frencuencia, y el resultado de esta transformación se denomina espectro. \\
		Sin embargo el problema viene cuando en las señales de audio (que son el tipo de señales con las que queremos trabajar), la cantidad de frecuencia varía en el tiempo, por lo que FFT es insuficiente al no poder representar en el espectro resultante esta variación de la señal en el tiempo. La Transformada de Fourier en tiempo reducido, (\emph{short-time Fast Fourier Transform}) resuelve este problema calculando la FFT en segmentos (ventanas de tiempo) superpuestos de la señal. Lo que finalmente obtenemos se denomina \textbf{espectograma}.
		
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.25]{processFFT.png} 
			\caption{Proceso del uso de FFT en una señal. Fuente: \href{https://www.mathworks.com/help/dsp/ref/dsp.stft.html}{Mathworks}}
		\end{figure} 
		
		
		Este espectograma puede ser entendido como una representación tridimensional de la señal donde sus características (tiempo, frecuencia y amplitud de la distribución de energía) pueden ser observadas de manera muy visual. Cuando este espectograma se computa, el eje X representa el tiempo, el eje Y representa la frecuencia, que es convertida a una escala logarítmica, y la gama de colores que se utiliza es para simbolizar la variación de energía expresada (medida decibelios), donde los tonos más oscuros indican unos valores de energía más altos, y viceversa.
		
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.25]{spectogram.png} 
			\caption{Espectograma de una muestra aleatoria. Fuente: \href{https://es.other.wiki/wiki/Spectrogram}{Wiki}}
		\end{figure}
		
		La respuesta a por qué las frecuencias son convertidas a una escala logarítmica es sencillamente porque los humanos no percibimos las frecuencias en una escala lineal \citep{Varshney}, es decir, nuestra habilidad para distinguir entre frecuencias fluctúa a lo largo del rango en el que somos capaces de percibir\citep{StevensVolkmann}. Es por ello que el rango donde se mueve nuestro espectograma, se adapta a lo que se llama \textbf{escala de Mel}, en la cual los armónicos se observan equidistantes, reduciendo como resultado las variantes acústicas que no son significativas.\\
		
		Finalmente nos queda entender el concepto de \emph{Cepstrum} o coeficientes cepstrales, y para ello debemos entender como el sonido (respecto a la articulación de palabras) es producido. Técnicamente, esta producción del sonido en nuestra anatomía se definiría como la combinación de las vibraciones producidas por las cuerdas vocales con las vibraciones producidas por la resonancia del tracto vocal. Nuestras articulaciones controlan la forma del tracto vocal, por lo que la forma de onda de la voz será reprimida o amplificada a diferentes frecuencias por la forma de nuestro tracto vocal \citep{Bao2019}.\\
		El papel de Cepstrum es la separación de frecuencias en el algoritmo de MFCC, atendiendo a como los sonidos son producidos siguiendo un modelo anatómico, de manera que cuando es computado separa la señal de voz y la resonancia del tracto vocal. En \ref{fig:mfcc_process} observamos el paso IDFT, donde tras ajustar la señal a la escala de Mel, se calcula una variante de FFT (\emph{Inverse Discrete Fourier Transform}) y se obtienen los \textbf{coeficientes de MFCC} \cite{Nair}.
		
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.3]{waveform_process.jpeg} 
			\caption{Visualización de los coeficientes cepstrales de una muestra aleatoria. Aquí se puede observar el resultado de los pasos que hemos estado discutiendo hasta ahora. Fuente: \href{https://www.ee.columbia.edu/~stanchen/spring16/e6870/slides/lecture3.pdf}{Columbia University}}
		\end{figure}
		
		Dado que hemos convertido una señal de audio en una imagen, ahora se podrá proceder a usar un modelo basado en redes convolucionales, aprovechando sus ventajas en el campo donde mejor rendimiento reporta: el procesamiento de imágenes.
		
		
		\subsubsection{Algoritmos de Clasificación}
		
		\begin{comment} 
		TODO
		Este parrafo debería ser referenciado
		\end{comment}
		Convencionalmente, el estudio del Reconocimiento de Emociones en el Habla incluye el uso de diferentes tipos de clasificadores para distinguir entre emociones: Suport Vector Machines (SVNs) los cuales se han usado extensamente
		para el reconocimiento de emociones y pueden llegar a presentar un buen rendimiento en comparación con otros clasificadores tradicionales, el algoritmo K-NN es de los enfoques más simples, Hidden Markov Model(HMM) es a menudo utilizado para lidiar con los cambios temporales en la señal y por último, Gaussian Mixture Model (GMM) el cual es útil para representar las unidades de sonido en características acústicas.%\cite{Farooq2020}.
		
		No obstante,en estudios más recientes, se han propuesto clasificadores basados en aprendizaje profundo, los cuales han superado a los enfoques tradicionales resultando ser más eficientes además de tener la capacidad de aprender las características emocionales en el reconocimiento de emociones a través del audio.
		
		El aprendizaje profundo es un conjunto de algoritmos de aprendizaje automático que modela abstracciones de alto nivel construyendo conceptos complejos a partir de otros más sencillos mediante el uso de una arquitectura jerárquica.
		Esta arquitectura jerárquica es lo que denominamos redes neuronales que son estructuras lógicas cuyo diseño se basa en mayor medida en la organización del sistema nervioso de los mamíferos. % referencia 
		Las neuronas artificiales que la componen son unidades de proceso especializadas en detectar determinadas características de aquello que es percibido (señal).
		
		
		\begin{figure}[!htb]
			\begin{minipage}{0.48\textwidth}
				\centering
				\includegraphics[width=.8\textwidth]{neurona.JPG}
				\caption{Representación de una neurona real}
				\label{ref:imgNeuron}
			\end{minipage}\hfill
			\begin{minipage}{0.48\textwidth}
				\centering
				\includegraphics[width=.6\textwidth]{neuronaArtificial.JPG}\hfill
				\caption{Representación de una neurona artificial.}
				\label{ref:imgArtNeuron}
			\end{minipage}
		\end{figure}
		
		La imagen \ref{ref:imgArtNeuron} muestra el funcionamiento de estas neuronas artificiales claramente inspirado en el diseño de la figura \ref{ref:imgNeuron}. Al nodo le llegan unas entradas $x_i$ que tienen asignadas unos pesos $w_i$ y unos sesgos $b_i$ (\emph{biases}), que son procesadas por la función de activación. El proceso de aprendizaje se consigue por transferencia de información de unas capas a otras gracias al algoritmo de retropropagación (\emph{backpropagation}) cuyo objetivo es encontrar la distribución adecuada a cada una de las variables de entrada.
		
		
		\paragraph{Redes Neuronales Recurrentes}\hfill \break
		Las Redes Neuronales Recurrentes (RNN) son convenientes en tareas en las que los datos son procesados secuencialmente. Esto puede ser especialmente una ventaja ya que las distintas entradas de la señal no se tratan de manera independiente como se explica en \citep{Lim2017}. En \citep{Lee2015} se propone un sistema basado en RNN aprovechando esta particularidad donde cada nodo toma en cuenta la información recogida en los anteriores; Esto hace que cubra un espectro más amplio de información creando una especie de contexto.
		
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.30]{rnn_structure.JPG} 
			\caption{Fuente:  \href{http://personal.cimat.mx:8181/~mrivera/cursos/aprendizaje_profundo/RNN_LTSM/introduccion_rnn.html}{Estructura de RNN}}
			\label{fig:rnn_structure}
		\end{figure}
		
		La figura \ref{fig:rnn_structure} muestra la estructura básica de una Red Neuronal Recurrente donde podemos observar como la red actualiza el peso de las entradas a través del algoritmo Descenso de Gradientes. En algunos casos, estos gradientes se irán haciendo más pequeños a medida que la red avanza, evitando así que los pesos cambien su valor y por lo tanto, la red siga aprendiendo. 
		
		\paragraph{Redes LSTM}\hfill \break
		Como podemos ver, los trabajos anteriormente mencionados que implementan este tipo de arquitectura RNN son relativamente antiguos (2017, y 2015 respectivamente) y en estudios más recientes , a destacar \citep{Wang2020} y \citep{Atmaja2019}, los retos que presenta la clasificación de emociones en el habla, son comúnmente abordados a través de una red de Memoria a Largo Corto-Plazo (LSTM) la cual es capaz de retener información de entradas anteriores en el tiempo y tener en cuenta dependencias temporales largas, ya que cada nodo es una célula de memoria. Esto a su vez, resuelve el problema de desvanecimiento de gradientes que presenta RNN.\\
		Las redes LSTM son un tipo de red recurrente que fueron diseñadas para resolver el problema de la dependencia a largo plazo del que sufre RNN. Como podemos ver en la figura \ref{fig:netLSTM}, estas presentan una estructura en cadena al igual que las redes recurrentes, pero el modulo de repetición en lugar de tener una única capa de red neuronal, tiene 4 que interactúan.
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.25]{LSTM3-chain.png}
			\caption{Estructura de una red LSTM. Fuente: \href{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}{Colah}}
			\label{fig:netLSTM} 
		\end{figure}
		
		\paragraph{Redes Neuronales Convolucionales}\hfill \break
		Uno de los mayores avances de los últimos años en el campo de la inteligencia artificial son las redes convolucionales, debido a la alta precisión que proporcionan en el procesamiento de imágenes.
		
		La tendencia de los modelos basados en redes neuronales densas en este ámbito, es aprender características específicas desde varios métodos usados en el reconocimiento de emociones a través de la percepción acústica. En el uso de las Redes Neuronales Convolucionales (CNN) la idea principal es tomar ventaja de las propiedades de la señal, como la conectividad local, los pesos compartidos y el uso de varias capas  \citep{Lim2017}. Estas suponen una importante contribución en la Clasificación Emocional de la Voz debido al uso de características significativas, y su uso en recientes estudios se ha incrementado a lo largo de los años donde destacan los trabajos de \citep{AbdulQayyum2019} y \citep{Anvarjon2020}.  %\cite{Farooq2020}.
		
		\subsubsection{Bases de datos}
		Aquí se recogen las bases de datos sobre el reconocimiento de emociones más usadas en los estudios comentados anteriormente.
		
		\begin{itemize}
			\item SAVEE (por sus siglas en inglés, \emph{ Surrey Audio-Visual Expressed Emotion}) es un conjunto de datos aplicado al reconocimiento de emociones que consiste en grabaciones de 480 frases en inglés británico ejecutadas por 4 actores profesionales masculinos modulando 7 emociones distintas (enfado, asco, tristeza, alegría, miedo, sorpresa y neutral). El estudio \cite{SAVEEdataset} explora y detalla esta base de datos que data del 2011, desgraciadamente su acceso está restringido.
			
			\item IEMOCAP (por sus siglas en ingles, \emph{Interactive Emotional Dyadic Motion Capture}) es una base de datos multimodal privada utilizada para el reconocimiento y análisis de emociones. Consiste en 12 horas de contenido audiovisual donde 10 actores (5 hombres y 5 mujeres) mantienen diálogos en ingles previamente transcritos en los que se interpretan 5 emociones distintas (enfado, tristeza, alegría, frustración y neutral).
			
			\item RAVDESS (por sus siglas en inglés, \emph{Ryerson Audio-Visual Database of Emotional Speech and Song}) es un popular conjunto dinámico multimodal (contiene varios formatos) donde 24 actores profesionales vocalizan frases en inglés norteamericano modulando 7 emociones (enfado, asco, tristeza, alegría, miedo, sorpresa y neutral). Tiene un total de 7356 grabaciones entre audio (hablado), audio(canciones) y vídeo. Cuenta con una completa documentacion \citep{Livingstone2018} y es de acceso público.
			
			\item EMO-DB (por sus siglas en inglés \emph{Berlin Database of EMOtional speech}) es una base de datos alemana cuyos detalles se recogen en \citep{emodb2005}, data del 2005 y es de acceso público. La conforma una colección de 800 grabaciones interpretadas por 10 actores (5 hombres y 5 mujeres) matizando 7 emociones (enfado, asco, tristeza, alegría, miedo, sorpresa y neutral) y son llevadas a cabo en una cámara anecoica (capaz de absorber las ondas sonoras o electromagnéticas sin reflejarlas).
			
		\end{itemize}
		
		
		\begin{comment} 
		TODO
		Añadir más adelante las otras bases de datos
		\end{comment}
		
		\section{Estado del Arte}
		\label{lb_estado_arte}
		Aunque la mayoría de estudios se basan en análisis que evalúan la precisión del clasificador sobre la propia lengua, el auge de las técnicas basadas en aprendizaje profundo ha permitido aumentar la capacidad de clasificación en los modelos de reconocimiento de emociones. La creación de un mapa de características a partir de diferentes representaciones de la onda sonora con respecto a su frecuencia y tiempo, permiten a las redes neuronales distinguir más atributos para hacer una preidcción más exacta. A partir de este paradigma, el intento de clasificar emociones a través de la voz ha sido objeto de estudio aplicando diferntes técnicas.
		
		En \citep{Atmaja2019} proponen sistema basado en una arquitectura LSTM bidireccional (BLSTM) que aplican a un subconjunto de la base de datos IEMOCAP para distinguir entre cuatro emociones(enfado, excitación, tristeza y neutral). Discuten la incapacidad del un modelo BLSTM para detectar características relevantes, y palian el problema añadiendo un modelo de atención. Teniendo ese modelo como punto de partida (BLSTM + modelo de atención), experimentan escogiendo diferentes valores de duración del silencio para medir la eficacia que tendría el modelo si este se eliminara. Además utilizan un complejo sistema de extracción de características entre las que destacan MFCC. Los resultados muestran un máximo del 70.34\% de precisión en los datos sin necesidad de eliminar el silencio de la señal de audio.
		Un año después, J.Wang \citep{Wang2020} propone un modelo dual LSTM donde cada uterancia, se procesa con características MFCC y espectogramas de Mel simultáneamente. El modelo es entrenado y evaluado en el conjunto de datos IEMOCAP llegando a un 72.7\% de exactitud.
		
		Las redes LSTM cubren en mencionado antes, efecto de contexto, resolviendo el desvanecimiento de gradientes que podemos encontrar en las redes recurrentes, pero de manera aislada no son las que mejores resultados ofrecen y es por ello, que en otros trabajos se han combinado con CNN para aumentar su rendimiento. Por ejemplo en \citep{Lim2017} se lleva a cabo una comparación de tres  arquitecturas (CNN, LSTM y CNN distribuida en el tiempo) donde LSTM (utilizada de manera aislada) es la que puntúa más bajo. Al mismo tiempo, W.Lim y su equipo estudian el resultado de un sistema híbrido que usa una red convolucional distribuida en el tiempo (una combinación de CNN y LSTM) para clasificar emociones en una secuencia de audio, consiguiendo un 88.01\% de precisión. De nuevo, para aprovechar las ventajas que ofrecen las redes convolucionales, la señal es convertida a imagen (espectograma), que es entrenada y probada con el corpus EMO-DB (alemán) distinguiendo entre 7 emociones.
		
		Por otro lado, en \citep{Harar2017} describe un método que utiliza una arquitectura basada en Redes Convolucionales sin selección de características para distinguir únicamente entre tres emociones (enfado, neutral, y tristeza) a través de la voz. Su objetivo es predecir el estado emocional de una persona en una grabación corta de audio donde la mencionada arquitectura consiste en 6 capas convolucionales y 3 densas. Como conjunto de datos utilizan la Base de Datos de Berlín de Discurso Emocional (EMO-DB), un corpus alemán que contiene un total de 800 frases (10 frases distintas re-interpretadas en 7 emociones por 5 mujeres y 5 hombres), del cual extraen un subconjunto de 271 grabaciones etiquetadas. De las señales de audio con las que el sistema es alimentado, se han eliminado los segmentos de silencio después de ser estandarizadas consiguiendo una exactitud de 96.97\%.
		
		Con el fin de eliminar el preprocesado de la señal, en \citep{AbdulQayyum2019}  presenta un modelo de redes convolucionales para una clasificación de emociones en el idioma inglés. Este utiliza la base de datos SAVEE, la cual contiene 480 muestras que distinguen entre 6 emociones, interpretadas por hombres y mujeres angloparlantes, donde obtiene finalmente un 81.63\% de precisión. Este trabajo llega a sus resultados mediante la comparación de 3 enfoques (MVR, SVN, y RNN) en el que a cada uno le aplican tres métodos de extracción de características distintos, con el sistema propuesto basado en CNN sin ningún procesado de la señal, siendo este último el que consigue una mejor capacidad de predicción.
		
		En estudios más recientes, \citep{Anvarjon2020} aborda el problema del Reconocimiento de Emociones en el Habla con una red CNN computacionalmente eficiente que es alimentada con los espectogramas de la señal; es decir, se consigue una representación en 2D de la señal de audio aprovechando mejor las ventajas de una red de este tipo. 
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.35]{anvarjon2020_emodb.JPG} 
			\caption{Resultados de T.Anvarjon sobre la base de datos EMO-DB}
			\label{fig:anvarjon_emo_plot}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.35]{anvarjon2020_imeocap.JPG} 
			\caption{Resultados de T.Anvarjon sobre la base de datos IEMOCAP}
			\label{fig:anvarjon_ime_plot}
		\end{figure}
		
		El sistema es probado en con dos datasets distintos independientemente, IEMOCAP (en inglés, y eliminando 'frustración') y EMO-DB (alemán) consiguiendo un 77.01\% y 92.02\%  de precisión respectivamente. En las figuras \ref{fig:anvarjon_emo_plot} y \ref{fig:anvarjon_ime_plot} vemos el rendimiento del modelo en los conjuntos de datos correspondientes comprobando el sólido resultado.
		
		Siguiendo por el enfoque de CNN, en \cite{Mustaqeem2020} exploran una arquitectura basada en CNN compuesta por 7 capas bidimensionales. Paralelamente la red es alimentada con espectrogramas y características extraídas a través de MFCC, y lleva a cabo una clasificación de 5 emociones evaluando el resultado en RAVDESS donde consiguen un 81.0\% de precisión e IEMOCAP con un 84.00\%
		
		% TODO: quiza quieres revisar esa verborrea
		Finalmente, es obligatorio hablar del trabajo de \cite{Tamulevicius2020}, en una línea más cercana al objetivo de este proyecto, lleva a cabo un estudio del reconocimiento emocional empleando el cruce de 6 lenguas (lituano, inglés, serbio, español, alemán y polaco). Aunque sus resultados no son realmente señalables y acaban diseñando un clasificador que es entrenado con todas las lenguas para distinguir las emociones indistintamente, dicha clasificación se lleva a cabo mediante el uso de una red neuronal convolucional bidimensional de 3 capas, e insisten en la importancia del uso de características en 2 dimensiones, ya que proveen información temporal además de las características acústicas de las emociones. En el estudio exploran varias de estas características, siendo el uso de cocleogramas las que consiguen una mayor exactitud.
		
		\section{Conclusiones parciales}
		En esta sección se valorarán las conclusiones que se extraigan del análisis previo sobre los distintas etapas correspondientes al desarrollo de un modelo para la clasificación de emociones en el habla. Vemos que los retos que plantea SER se han abordado anteriormente desde distintos enfoques, pero en su mayoría, desde el punto de vista de un único lenguaje. Las dificultades que presenta esta tarea en la lengua extranjera se deben principalmente a las posibles variaciones de aire para expresar la mismas emociones. Este mismo problema se plantea en una modalidad diferente pero bastante relacionada como es la transcripción de la voz a texto (ASR, Reconocimiento Automático del Discurso), sin embargo estos estudios requieren un estudio más en profundidad de la fonética propia de cada lenguaje.
		
		Los trabajos de Pell se han centrado durante años en el análisis de la prosodia a través de los idiomas. A pesar de su antigüedad y que no entra demasiado en detalles técnicos, merece la pena mencionar que en \citep{Pell2008} lleva a cabo un estudio comparativo entre la detección emocional de la prosodia en la lengua materna y la extranjera, concluyendo que el proceso para entender las emociones vocales en una lengua no aprendida, implica una mayor exposición a esta para familiarizarse con señales prosódicas correspondientes a significados subyacentes.\hfill \break
		
		Bajo estas líneas, podemos ver la tabla donde se resumen los estudios previamente comentados en la sección \ref{lb_estado_arte}.\\
		
		\begin{table}[H]
			\centering
			\begin{center}
				\begin{tabular}{| c | c | c | c| c | c|}
					\hline
					Trabajo & Año & Método &  Datos usados  & Acierto \\ 
					\hline
					Atmaja & 2019 &   BLSTM + Att  & IEMOCAP (4 emociones) & 73.34\% 	\\
					J.Wang & 2020 & LSTM dual + MFCC & IEMOCAP & 72.7\% \\
					W.Lim & 2017 &   LSTM + CNN & EMO-DB &  88.01\%		\\ 
					Harar & 2017 &  CNN & EMOD-DB (3 emociones) &  96.97\%			\\
					Abdul & 2019 & CNN & SAVEE & 81.63\%				\\
					Anvarjon & 2020 & CNN 2D + espectogramas Mel & IEMOCAP (4 emociones) & 77.01\%\\
					Anvarjon & 2020 & CNN 2D + espectogramas Mel & EMOD-DB & 92.02.01\%\\
					Mustaqeem & 2020 & CNN 2D + MFCC & RAVDESS (5 emociones) & 81.01\% \\
					Mustaqeem & 2020 & CNN 2D + MFCC & IEMOCAP (5 emociones) & 84.00\% \\  
					Tamulevicius & 2020 & CNN 2D + cocleogramas & Lithuanian & 97.00\% \\
					%& & & &
					\hline	
				\end{tabular}
				
				\caption{Tabla comparativa y resumida de los trabajos mencionados}
				\label{tab:metod_comp}
			\end{center}
		\end{table}
		
		En la tabla \ref{tab:metod_comp} se muestran un resumen de los estudios relacionados con este campo y sus correspondientes resultados, así como los métodos y datos que han usado para ello. La columna de Método, comprende la arquitectura completa, es decir, el sistema de clasificación usado y el método de extracción de características si lo hubiera. En la tercera columna se describen los datos usados (la base de datos) y el número de clases entre las que el sistema ha tenido que diferenciar. Cuando estas no se especifican, quiere decir que se ha usado el conjunto de datos al completo.
		
		% WARNING No puedo acceder a la base de datos de Lithuanian para referenciarlo
		En el último trabajo, \citep{Tamulevicius2020} se lleva a cabo la comparación de distintos datos, pero es \emph{Lithuanian} en donde se basan para crear el modelo, cuya arquitectura es la que se especifica en la tabla.\\
		
		Cabe destacar que la combinación de métodos, veáse algoritmos de clasificación, filtros para preprocesar la señal, y métodos de extracción de características, así como distintos conjuntos de datos, es realmente diversa, por lo que visualizar una dirección clara para determinar qué línea es la mejor, se diluye. 
		
		No obstante hay observaciones, que pueden llevar a una conclusión general; Por ejemplo, y de manera intuitiva, cuanto mayor es el número de clases (emociones en nuestro caso), peor será la capacidad de clasificación de la red, y por eso en algunos trabajos se extrae un subconjunto reduciendo las opciones entre las que clasificar.  
		
		En la extracción de características el uso de MFCC ha sido amplia y tradicionalmente escogido al reportar resultados más elevados en comparación con otros métodos. En \citep{Langari2020} denota que los métodos de extracción de características son MFCC y LPCC porque las variaciones en la frecuencia del tono están significativamente relacionados con la expresión humana de emociones. Los parámetros que se computan en MFCC como el número de filtros o la escala de frecuencia, son a menudo escogidos de manera experimental y dependen en gran medida del conjunto de datos con el que se pruebe y el clasificador que implemente el sistema.
		
		En cuanto a los métodos usados como clasificadores, observamos también que CNN (con diferentes modificaciones dependiendo del estudio) es la opción más sólida entre los trabajos más recientes, debido principalmente a que reduce la señal de audio a sus características más relevantes, y la combinación de probabilidades resultantes identifica conjuntos de atributos que determinan una clasificación. El uso de espectrogramas, aprovecha las bondades que las redes convolucionales ofrecen
		%https://blog.soprasteria.se/2019/10/07/using-cnn-for-speech-emotion-recognition/
		
		\chapter{Objetivos y metodología de trabajo}
		
		\section{Objetivo General}
		Disponer de al menos 3 conjuntos de datos pertenecientes a 3 idiomas distintos con una estructura de etiquetado similar, de los cuales uno se tomará como referencia para realizar una clasificación emocional con un porcentaje de acierto de al menos  un 80%  
		
		\section{Objetivos específicos}
		Para conseguir el alcance establecido, es necesario que los siguientes puntos sean satisfechos:
		\begin{itemize}
			\item Hacer un estudio del estado del arte sobre diferentes métodos, técnicas, y conjunto de datos utilizados en el reconocimiento de emociones a través de la voz. Aquí también se explorará si se dispone de la documentación necesaria, cómo cada uno de esos métodos pudiera estar relacionado con la lengua que usa para aplicarlo y su fonética.
			
			\item  Conseguir al menos 3 datasets pertenecientes a 3 idiomas diferentes donde uno de ellos será usado como referencia, y además, deberán cumplir las siguientes condiciones: Uno de los conjuntos de datos restantes deberá tener raíces fonéticas distintas al corpus de referencia, y el otro tener raíces fonéticas similares.
			
			\item Diseñar una solución en la que el conjunto de datos de referencia tenga un porcentaje de acierto del 80\% en la clasificación de emociones.
			
			\item Aplicar el modelo diseñado en el paso anterior a los otros conjuntos de datos.
			
			\item Evaluar la tasa de acierto obtenida en cada uno de eso conjuntos y comparar los resultados obtenidos.
			
		\end{itemize}
		\section{Metodología de trabajo}
		Para este proyecto se plantea una metodología de desarrollo iterativa, en las que tras una fase inicial, el proyecto entra en un bucle donde el producto pasa por una serie de etapas que se repiten durante la vida del proyecto. En cada iteración se diseñan unas modificaciones y capacidades funcionales que son añadidas al producto.
		
		\begin{enumerate}
			\item Fase inicial: Identifica el alcance del proyecto y los requisitos de una manera general pero lo suficientemente en detalle para que el tiempo y esfuerzo pueda ser estimado. Este paso inicial tiene como objetivo crear una versión del producto en la que se pueda trabajar como base (o mostrar a un cliente si lo hubiera)
			
			\item Elaboración: Se llevan a cabo los experimentos y la implementación de los componentes mediante unos pasos iterativos, a saber:
			
			\begin{enumerate}
				\item Planificación: Diseño del experimento o estudio que se quiere llevar a cabo teniendo en cuenta el tiempo disponible para ello, los posibles riesgos y el alcance deseado. Para ello se elabora una serie de tareas a implementar
				
				\item Implementación: Una vez identificadas las tareas en el paso anterior, se lleva a cabo la implementación del experimento.
				
				\item Evaluación: Se evalúan los resultados obtenidos de la implementación antes de decidir la iteración por finalizada. En caso de que los resultados no sean los esperados hay dos posibilidades: Si los errores obtenidos no distan demasiado de las especificaciones parciales, se pueden realizar reajustes en el modelo. Si por el contrario, dichos resultados están demasiado lejos del objetivo, se creará una segunda versión con una estrategia diferente, añadiendo más iteaciones al proceso de elaboración.
			\end{enumerate} 
			
			\item Despliegue: Es la etapa final, cuando el proyecto se da por finalizado.
		\end{enumerate}
		
		
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.15]{Iterative_Process_Diagram.png}
			\caption{Proceso iterativo propuesto Fuente: \href{https://en.wikipedia.org/wiki/Iterative_and_incremental_development}{Wikipedia}}
			\label{fig:iterative_process}
		\end{figure}
		
		
		Al contrario que en desarrollos de software más tradicionales donde podrían verse flujos de trabajo basados en metodologías ágiles o en cascada, hemos creído que este modelo se adapta mejor a las necesidades de este proyecto, debido principalmente, al grado de incertidumbre que presenta un proyecto basado en Inteligencia Artificial en comparación con la ingeniería del software estándar. Por ejemplo, una metodología ágil asume que pequeños cambios funcionales hace posible el alcance de los objetivos a bajo coste y alta predictibilidad. Esto no se corresponde con este tipo de trabajo por las siguientes características:
		
		\begin{itemize}
			\item Es difícil conocer los costes y riesgos de la mayoría de los requisitos. Por ejemplo el estudio del conjunto de datos es algo que afecta directamente a la elaboración de los distintos modelos, y por lo tanto el crecimiento funcional es indeterminado.  
			
			\item Los cambios o modificaciones no pueden ser aplicados por diseño, requieren experimentos, además esas modificaciones son realmente complicadas de atomizar, por lo que el coste es impredecible. 
			
			\item Si bien es difícil tener una conclusión final por las estrictas fechas de entrega, un modelo iterativo permite obtener datos presentables a lo largo del proyecto.
		\end{itemize}
	

	\chapter{Planteamiento de la comparativa}
	
	En este capítulo se identificará el problema en concreto a tratar, a la vez que el diseño de los experimentos para acometerlo. Para ello se exponen los datos utilizados así como un análisis en detalle de estos respondiendo a por qué se escogen esos conjuntos. Finalmente las técnicas de procesamiento y el diseño de la red neuronal propuesta que se usan en este trabajo
	
	El objetivo de esta comparativa es contrastar los resultados obtenidos tras aplicar el mismo sistema de reconocimiento de emociones en la voz entrenado con un lenguaje de referencia, con los otros dos lenguajes escogidos. Mediante esta comparativa se pretende responder a la pregunta si es posible reconocer emociones en un idioma que en principio se desconoce.
	
	\section{Conjunto de Datos}
	\label{lb_c4_datos}
	Los datos en un proyecto de inteligencia artificial son clave de cara a la obtención de un resultado coherente en nuestro trabajo. Este estudio pretende analizar si es posible clasificar emociones en la lengua extranjera y para encontrar una respuesta, se seguirá la siguiente estrategia con respecto a los datos.
	
	\paragraph{Idioma de referencia: Inglés} El idioma de referencia será el cuál aprenda nuestra red neuronal, y desde el cual se intenten reconocer emociones en otras lenguas. En este caso se propone el inglés.\\
	
	El conjunto de datos que se usará para el idioma de referencia, será La Base de Datos Audiovisual del Discurso y Canción Emocional RAVDESS (por sus siglas en inglés \emph{Ryerson Audio-Visual Database of Emotional Speech and Song}), el cual contiene 7356 archivos en total, entre los cuales podemos encontrar 3 modalidades: sólo audio (en 16 bit, 48 kHz y en formato wav), audio-video (720p H.264, AAC 48kHz, en formato mp4) y sólo video sin sonido. Esta base de datos contiene 24 actores profesionales vocalizando dos frases en inglés norte americano (\emph{Kids are talking by the door} y \emph{Dogs are sitting by the door}).
	
	Cada uno de estos archivos están nombrados de manera única mediante 7 números a modo de descripción de las características del audio. Éste respeta la siguiente convención:
	\begin{itemize}
		\item Modalidad (01 Audio y vídeo, 02 Sólo vídeo, 03 Sólo audio)
		\item Canal vocal (01 discurso normal, 02 canción)
		\item Emoción que representa
		\item Intensidad Emocional Si es normal o fuerte. La voz neutral no contempla la intensidad fuerte.
		\item Repetición (si es la primera repetición 01, si es la segunda 02)
		\item Actor que ejecuta la acción
	\end{itemize}
	
	Así por ejemplo, el archivo 03-01-03-01-01-01-01.wav dirá que es un archivo de sólo audio (03), donde se vocaliza una frase de manera hablada (01) y con tono alegre (03). La intensidad es normal (01), corresponde a la primera repetición (01) y el actor que la ejecuta es el n.01.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.35]{ravdess_distribucion.png} 
		\caption{Distribución de las emociones en RAVDESS}
		\label{fig:emociones_ravdess}
	\end{figure}
	
	Como se puede apreciar en la figura \ref{fig:emociones_ravdess} las emociones están regularmente distribuidas, sin embargo, a pesar de que en este dataset hay 7356 muestras, para este proyecto sólo podremos usar los que presentan una modalidad de sólo audio, lo que nos deja con un total de 1440 muestras. 
	
	
	\paragraph{Idioma con raíces fonéticas similares: Alemán} Este conjunto de datos pertenecerá a un idioma con unas raíces similares al idioma de referencia, de manera que se espera a priori que se puedan reconocer la mayoría de las emociones.\\
	
	Para este caso el conjunto de datos propuesto es la Base de Datos del Discurso Emocional de Berlín (EMODB, por sus siglas en inglés \emph{Berlin Database of Emotional Speech}). Este corpus contiene 800 grabaciones interpretadas por 10 actores (5 hombres y 5 mujeres) modulando 7 emociones en el idioma alemán. Cada archivo tiene una frecuencia de muestreo de 16 kHz con una resolución de 16 bits, y una duración de 3 segundos de media. Como en el anterior, se utiliza una nomenclatura para nombrar a los archivos que satisface lo siguiente:
	\begin{itemize}
		\item Las dos primeras posiciones determinan el actor que las interpreta.
		\item De la posición 3 a las 5 se define el texto que se pronuncia
		\item La posición 6 indica la emoción.
		\item Versión del audio en caso de que la hubiese (codificado con letras).
	\end{itemize}
	
	Como ejemplo, el archivo \emph{03a01Fa.wav} indica que el actor 03 (hombre de 31 años) cita el texto a01 (\emph{Der Lappen liegt auf dem Eisschrank}, en alemán "El mantel está colgando del frigo"), con la emoción F (felicidad), y es la versión \emph{a} (la primera).
	
	La documentación del corpus también nos ofrece información sobre el género y edad de los actores, lo cual se ha determinado irrelevante, y las distintas frases que pueden aparecer en los archivos.
	Las emociones que clasifica son enfado (W), aburrimiento (L), asco (E), miedo o ansiedad (A), felicidad (F), tristeza (T) y neutral (N) codificadas en el nombre del archivo por su inicial en alemán (especificada entre paréntesis).
	
	
	
	\paragraph{Idioma con raíces fonéticas distintas} Este conjunto de datos pertenecerá a un idioma con unas raíces más distantes al idioma de referencia.\\
	
	
	\section{Extracción de características}
	Teniendo en cuenta el previo estudio de la literatura en el capítulo 2, se concluye que los métodos más prometedores, y que por lo tanto merecen la pena aplicar a este estudio comparativo serían los siguientes:
	\subsubsection{Coeficientes Cepstrales en la escala de Mel}
	Como ya hemos mencionado, MFCC es uno de los mejores algoritmos para capturar características de la señal de audio debido a su similitud a como el sistema auditivo humano procesa el sonido y las frecuencias, así mismo, su efectividad se ha visto reportada y discutida a lo largo de otros estudios.
	La librería usada para la manipulación de audio Librosa ofrece la posibilidad de extraer características MFCC de un archivo de audio. En cuanto a la configuración, se extraerán 13 características MFCC usando el rango de muestreo del propio archivo de audio.
	
	\begin{comment}
	Razon por el numero de características:
	https://dsp.stackexchange.com/questions/28898/mfcc-significance-of-number-of-features
	Mencionar que otros estudios
	\end{comment}
	
	%	\paragraph{Espectogramas de Mel}
	%	La escala de Mel transforma  frecuencias lineales a una escala logarítmica
	
	\section{Configuración}
	En esta sub-sección se muestra los recursos a los que se ha accedido para el desarrollo del estudio, así como su correspondiente configuración.
	\begin{comment}
	\begin{wrapfigure}{l}{0.40\textwidth}
	\vspace{-20pt}
	\centering
	\includegraphics[width=0.4\linewidth]{/logos/colab_icon.png} 
	\vspace{-20pt}
	\end{wrapfigure}
	\end{comment}
	
	
	\paragraph{Google Colab} Para la exploración de los datos así como el desarrollo, y entrenamiento de los modelos se ha hecho uso de la plataforma gratuita desarrollada por Google, Google Colab. Esta plataforma ofrece 12GB de RAM  y 107.77GB de espacio en disco, que será más que suficiente dado el tamaño que nuestro dataset.
	
	\paragraph{Librosa 0.8.1} Librosa es un paquete que ofrece diversas funcionalidades para el análisis de audio y música, cuya información más en detalle se puede encontrar en \cite{librosa082}. Esta librería ha sido esencial para la extracción de características MFCC así como algunas técnicas de aumento de datos.
	
	\paragraph{Tensorflow 2.0} Es una plataforma de código abierto originalmente creada por Google que provee un conjunto de librerías y recursos para el desarrollo de modelos con aprendizaje automático. Tensorflow, que además ofrece soporte de Keras, se ha usado tanto para la estandarización de los datos, como para la compilación y entrenamiento del modelo.
	
	\paragraph{Python 3} Python es un lenguaje interpretado de alto nivel. Todo el código para este proyecto ha sido desarrollado en python 3.
	
	\section{Pre-procesado de los datos}
	Como se ha visto en la sección \ref{lb_c4_datos} no hay una abundante disposición de datos, esto podría convertirse en un problema y perjudicar el rendimiento del modelo en el entrenamiento. 
	Será necesario, antes del entrenamiento, un previo procesado de los datos.
	
	\subsection{División de los datos por género}
	En un primer análisis exploratorio de los datos, se han estudiado las diferencias entre la voz masculina y la voz femenina en las emociones, observando lo siguiente:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.35]{comparative_waveform.png} 
		\caption{Comparativa de los extractos de voz por género en RAVDESS}
		\label{fig:comp_emociones_genero}
	\end{figure}
	En la figura \ref{fig:comp_emociones_genero} podemos ver la comparación de la voz masculina (naranja) y la voz femenina (azul) por cada una de las emociones en el idioma inglés. Ya que es muy distinto, puede ser recomendable dividir el conjunto de datos atendiendo a esta característica.
	
	
	%\subsection{Conversión de la señal a imagen}
	% quiza no sea necesario
	
	%\subsection{Normalización de los datos}
	\begin{comment}
	# Now because we are mixing up a few different data sources, it would be wise to normalise the data. 
	# This is proven to improve the accuracy and speed up the training process. Prior to the discovery of this solution in the embrionic years of neural network, 
	# the problem used to be know as "exploding gradients".
	\end{comment}
	
	
	\subsection{Ténicas de aumento de datos}
	Dado el bajo número de muestras en los distintos conjuntos de datos a los que hemos podido acceder, se ha visto conveniente explorar distintas técnicas de aumento de datos. El aumento de datos es una técnica por la cual, se aumenta el número de muestras en un conjunto mediante la creación de nuevas muestras sintéticas con pequeñas modificaciones a cada uno de los archivos. Esta aumento de los datos se puede traducir por una reducción del overfitting (sobreajuste), ya que el modelo se mantendría invariable mejorando así su capacidad de generalización.
	Esta técnica es ampliamente conocida cuando se procesan imágenes, siendo esas modificaciones rotaciones, transposiciones etc. En nuestro caso, sabemos que el sonido tiene las siguientes características: tono, duración, timbre e intensidad. Por lo que debemos modificar levemente nuestros datos alrededor de esas características de manera que sólo difieran en pequeños factores de la muestra original.
	% VER: https://medium.com/@keur.plkar/audio-data-augmentation-in-python-a91600613e47
	
	\subsubsection{Ruido Blanco}
	Añadir ruido blanco a la pista de audio, implica la inyección de valores aleatorios distribuidos de manera irregular con una media de 0 y una desviación de estándar de 1. Para implementar este método se usará el paquete numpy
	
	
	\subsubsection{Desplazamiento del sonido}
	Desplaza el sonido hacia la izquierda o la derecha una cantidad aleatoria de segundos. Por ejemplo, si el sonido ha sido desplazado hacia delante (izquierda) x segundos, los x primeros segundos se marcan con 0. Si por el contrario han sido desplazados hacia detrás (derecha), los últimos x segundos se marcarán con 0.
	
	\subsubsection{Cambio de tono}
	Se refiere al proceso de cambiar el tono a un sonido sin variar su velocidad. Para implementar este método se usará la librería Librosa que ofrece un método específico.
	
	\section{Arquitectura}
	Como se ha podido ver en la revisión de la literatura del capítulo 2, las redes convolucionales esta una tendencia muy adoptada en los últimos trabajos en esta área de estudio.
	%Por temas de tiempo blablabla [...] se ha decidido por una red base que reporte buenos resultados.
	%blabla...
	
	%Los trabajos como los de \cite{AbdulQayyum2019}, \cite{Anvarjon2020} utilizan CNN en sus trabajos. 
	%Para un caso base en la línea de trabajo se ha optado por una red convolucional inspirada %en el trabajo de \cite{AbdulQayyum2019}, la cual consta de:
	En un principio, se optó por una línea de trabajo inspirada en el estudio de \cite{AbdulQayyum2019} ya que combina buenos resultados y un sistema sencillo. Pero tras varias pruebas esta arquitectura se ha refinado hasta definirse, por ahora, lo siguiente:
	
	\begin{itemize}
		\item 2 capas convolucionales unidimensionales con activación Relu. El número de filtros es de 128 y  y el tamaño del kernel de 5.
		En las dos capas convolucionales se usa regularización de tipo L2 para aplicar una penalización a las capas del kernel con un valor de 0.01 y corregir así el overfitting.
		
		\item La primera capa convolucional está seguida de una capa Dropout del 0.5 y una capa \emph{max pooling} con un tamaño 8.
		
		\item A la segunda capa convolucional se sigue otra capa de Dropout con un valor de 0.25 y una capa \emph{Flatten}.
		
		\item Por último esta arquitectura cierra con una capa densa con 7 nodos (número de clases) con una función de activación 'softmax'.
	\end{itemize}
	% Hablar de las bondades las redes con 1 dimension
	% En que ayudan
	En cuanto al entrenamiento del modelo se usó un optimizador RMSprop con una tasa de aprendizaje de 0.00005, valor de $\rho$ de 0.9 y $\epsilon$ a 'None', por dar mejores resultados frente a Adam del caso inicial desde el que se partió.La función de pérdida utilizada para este propósito es entropía cruzada categórica (categorical crossentropy).\\
	
	Además para intentar afinar el modelo se añadieron como callbacks ReduceLROnPlateau, que reduce la tasa de aprendizaje cuando el modelo ha dejado de mejorar y EarlyStopping, que detiene el entrenamiento si se ha llegado una meseta, es decir, si durante un determinado número de épocas, el modelo ha dejado de mejorar. En ReduceLROnPlateau se monitoriza la \emph{val loss} con el fin de minimizarla, y como configuración se emplea un factor de reducción de la tasa de aprendizaje de 0.9, una paciencia de 20 épocas y una tasa de aprendizaje mínimo de 0.000001.
	Con respecto a EarlyStopping, la variable monitorizada es 'val accuracy' con el fin de maximizarla y una paciencia de 20 épocas.
	
	El modelo es entrenado durante un total de 100 épocas y con un tamaño del batch de 16.
	
	\section{Criterios de éxito}
	El objetivo de esta sección es definir las métricas que se usarán para comparar los distintos modelos en los experimentos parciales, así como los resultados obtenidos al aplicar dichos modelos a los datos mencionados en la sección \ref{lb_c4_datos}.\\
	Las dos principales métricas que se usarán para decidir cómo de buena es la predicción del modelo serán:
	\begin{itemize}
		\item \textbf{Exactitud o \emph{Accuracy} } Establece una comparación entre los resultados predichos y los obtenidos determinando cómo de preciso es el algoritmo cuando se trata de identificar las clases
		
		\[
		Accuracy = \frac{TP + TN}{ TP + FN + TN + FP} 
		\]
		\hfill \break
		
		\item \textbf{F1 score} Siendo el \emph{recall} la fracción de elementos relevantes que son recuperados (el cociente de las predicciones positivas y el número de clases positivas en el conjunto de test), la medida de F1 Score conviene el balance entre la precisión y el \emph{recall}.
		\[
		Recall = \frac{TP}{ TP + FN} 
		\]
		
		\[
		%\begin{equation}
		Precision = \frac{TP}{TP + FP}
		%\end{equation}
		\]
		
		\[
		F1 Score = \frac{2 * (Precision * Recall)}{Precision + Recall}
		\]
		\hfill \break
		
		\item \textbf{Matriz de confusión} Permite una rápida visualización del resultado de la ejecución del algoritmo. Muestra el nivel de predicción por cada clase. En la figura \ref{fig:confusionMatrix} podemos ver una representación de esta.
		
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.3]{confusionMatrix.png}
			\caption{Representación de la matriz de confusión. Fuente: \href{https://rpubs.com/chzelada/275494}{Rpubs}}
			\label{fig:confusionMatrix}
		\end{figure}
		
	\end{itemize}
	
	En algunos casos, la \emph{accuracy} puede ser errónea debido a la paradoja de la exactitud donde puede existir un sesgo debido a una distribución desbalanceada de las clases. Esto hace que pueda ser más inteligente elegir un modelo con menor exactitud pero con mayor poder predictivo.
	Para ver ese poder predictivo por lo tanto, es aconsejable elegir más de una métrica de evaluación. Para ello se contará con F1 Score que es la media armónica entre el \emph{recall} y la precisión, y la matriz de confusión que nos permitirá observar como se comporta un modelo especifico preidiciendo las clases.
	
	
	\section{Diseño de experimentos}
	
	\paragraph{Prueba 1}  %RAVDESS-1DCNN
	Conjunto de datos: RAVDESS con técnicas de aumento de datos
	Método: MFCC + Red convolucional unidimensional 
	
	\paragraph{Prueba 1.A} %RAVDESS+TESS - 1DCNN
	Conjunto de datos: RAVDESS + TESS con técnicas de aumento de datos
	Método: MFCC + Red convolucional unidimensional 
	
	\paragraph{Prueba 1.B}  
	Conjunto de datos: RAVDESS + TESS con técnicas de aumento de datos
	Método: MFCC + Red convolucional bidimensional 
	
	%\paragraph{Prueba 1.C}  %RAVDESS-2DCNN
	
	\chapter{Desarrollo de la comparativa}
	\section{Prueba 1}
	\subsection{Conjunto de datos}
	En un primer momento los datos se han dividido por género como se especifica en el capítulo 4
	\subsection{Extracción de características}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.35]{featuresMFCC.JPG}
		\caption{Características MFCC extraídas del conjunto de datos}
		\label{fig:caract_MFCC}
	\end{figure}
	\subsection{Técnicas de aumento de datos}
	
	\subsubsection{Arquitectura}
	
	
	\subsection{Evaluación}
	Tras varias pruebas el modelo se simplificó considerablemente, ya que presentaba un alto sobreajuste o \emph{overfitting}. En la tabla \ref{tab:augmentationData_1} se presentan los resultados aplicados al conjunto RAVDESS con género femenino:
	
	\begin{table}[H]
		\centering
		\begin{center}
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline
				Ruido Blanco & Desplazamiento & Modulación \\ 
				\hline
				\includegraphics[scale=0.15]{results/white_noise1.png} & \includegraphics[scale=0.15]{results/shiftting_1.png} & \includegraphics[scale=0.15]{results/pitch.png}\\
				
				%& & & &
				\hline	
			\end{tabular}
			\caption{Resultados obtenidos tras aplicar cada uno de los métodos de aumento de datos}
			\label{tab:augmentationData_1}
		\end{center}
	\end{table}
	
	A pesar de que dividir los datos por género parecía una buena idea, se ha comprobado que la variación en el tono provocado por el género no afecta de manera negativa al rendimiento del modelo.
	
	\begin{table}[H]
		\centering
		\begin{center}
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline
				Ruido Blanco & Desplazamiento & Modulación \\ 
				\hline
				\includegraphics[scale=0.15]{results/white_noise2.png} & \includegraphics[scale=0.15]{results/shiftting_2.png} & \includegraphics[scale=0.15]{results/pitch_2.png}\\
				
				%& & & &
				\hline	
			\end{tabular}
			\caption{Resultados obtenidos tras aplicar cada uno de los métodos de aumento de datos}
			\label{tab:augmentationData_2}
		\end{center}
	\end{table}

\printbibliography

\end{document}





















