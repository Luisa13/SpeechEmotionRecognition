\documentclass[11pt,a4paper,spanish]{book}
\usepackage{biblatex}
\usepackage[utf8]{inputenc}

% Para el formato del titulo: numero Nombre Capitulo
\usepackage{titlesec}
\titleformat{\chapter}[block]
{\normalfont\huge\bfseries}{\thechapter.}{1em}{\Huge}
%\titlespacing*{\chapter}{0pt}{-19pt}{0pt}

% Imagenes
\usepackage{graphicx, wrapfig, hyperref}
\graphicspath{ {./.images/} }
\usepackage{float}
\usepackage{comment}

% Tablas
\usepackage{multicol, multirow}
\usepackage{changepage}
\usepackage[dvipsnames]{xcolor}
\addbibresource{./bibLib/mycol_arte.bib}



\begin{document}
	\chapter{Planteamiento de la comparativa}
	
	En este capítulo se identificará el problema en concreto a tratar, a la vez que el diseño de los experimentos para acometerlo. Para ello se exponen los datos utilizados así como un análisis en detalle de estos respondiendo a por qué se escogen esos conjuntos. Finalmente las técnicas de procesamiento y el diseño de la red neuronal propuesta que se usan en este trabajo
	
	El objetivo de esta comparativa es contrastar los resultados obtenidos tras aplicar el mismo sistema de reconocimiento de emociones en la voz entrenado con un lenguaje de referencia, con los otros dos lenguajes escogidos. Mediante esta comparativa se pretende responder a la pregunta si es posible reconocer emociones en un idioma que en principio se desconoce.
	
	\section{Conjunto de Datos}
	\label{lb_c4_datos}
	Los datos en un proyecto de inteligencia artificial son clave de cara a la obtención de un resultado coherente en nuestro trabajo. Este estudio pretende analizar si es posible clasificar emociones en la lengua extranjera y para encontrar una respuesta, se seguirá la siguiente estrategia con respecto a los datos.
	
	\subsection{Idioma de referencia: Inglés} El idioma de referencia será el que aprenda nuestra red neuronal, y desde el cual se intenten reconocer emociones en otras lenguas. En este caso se propone el inglés.\\
	Las bases de datos a las que se ha tenido acceso son limitadas, y no presentan un gran número de muestras, por lo que en previsión de un rendimiento pobre en el modelo, nos hemos decantado por escoger varias del mismo idioma de cara a un entrenamiento más completo.
	
	\paragraph{RAVDESS}
	Como ya se hizo ver en la sección \ref{lb:cap_baseDatos}, RAVDESS (por sus siglas en inglés \emph{Ryerson Audio-Visual Database of Emotional Speech and Song}), contiene 7356 archivos en total, entre los cuales podemos encontrar 3 modalidades: sólo audio (en 16 bit, 48 kHz y en formato wav), audio-video (720p H.264, AAC 48kHz, en formato mp4) y sólo video sin sonido. Esta base de datos contiene 24 actores profesionales vocalizando dos frases en inglés norte americano (\emph{Kids are talking by the door} y \emph{Dogs are sitting by the door}).
	
	Cada uno de estos archivos están nombrados de manera única mediante 7 números a modo de descripción de las características del audio. Éste respeta la siguiente convención:
	\begin{itemize}
		\item Modalidad (01 Audio y vídeo, 02 Sólo vídeo, 03 Sólo audio)
		\item Canal vocal (01 discurso normal, 02 canción)
		\item Emoción que representa
		\item Intensidad Emocional Si es normal o fuerte. La voz neutral no contempla la intensidad fuerte.
		\item Repetición (si es la primera repetición 01, si es la segunda 02)
		\item Actor que ejecuta la acción
	\end{itemize}

	Así por ejemplo, el archivo 03-01-03-01-01-01-01.wav dirá que es un archivo de sólo audio (03), donde se vocaliza una frase de manera hablada (01) y con tono alegre (03). La intensidad es normal (01), corresponde a la primera repetición (01) y el actor que la ejecuta es el n.01.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.25]{ravdess_distribucion.png} 
		\caption{Distribución de las emociones en RAVDESS en la modalidad sólo audio (1440 archivos)}
		\label{fig:emociones_ravdess}
	\end{figure}
	
	A pesar de que este dataset hay 7356 muestras para este proyecto únicamente se usarán aquellas que presentan una modalidad de sólo audio, lo que nos deja con un total de 1440 muestras, sin embargo como se puede apreciar en la figura \ref{fig:emociones_ravdess} las emociones están bien distribuidas.
	
	\paragraph{SAVEE} (por sus siglas en inglés, \emph{ Surrey Audio-Visual Expressed Emotion}), cuenta con 480 archivos de audio en formato wav muestreados a 44.1 kHz, donde 4 actores anglosajones (inglés británico) de 27 a 31 años, modulan 7 emociones con frases específicas a cada una.
	Cada archivo ha sido etiquetado de manera que el primer carácter (o caracteres, antes de un dígito) corresponde a la emoción que representa. Así las letras 'a', 'd', 'f', 'h', 'n', 'sa' y 'su', corresponden a enfado (\emph{angry}), asco (\emph{disgust}), miedo (\emph{fear}), felicidad (\emph{happiness}), neutral (\emph{neutral}), tristeza (\emph{sadness}), sorpresa (\emph{surprise}). El número que le sigue a continuación, se refiere a la frase pronunciada. Por ejemplo, el archivo d03.wav hace referencia a la tercera frase de la emoción asco.
	%QUESTION Deberia incluir algunas de las frases?
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.25]{no-photo.jpg} 
		\caption{Distribución de las emociones en SAVEE}
		\label{fig:emociones_savee}
	\end{figure}
	
	A pesar de que sólo presenta 480 audios, el hecho de que se use una frase distinta para cada emoción, lo convierte en un conjunto de datos muy completo para el entrenamiento. La figura \ref{fig:emociones_savee} muestra la distribución de las emociones, observando que están bien balanceadas.

	\paragraph{TESS}
	Este dataset lo conforman 2800 archivos de audio en formato wav donde dos actrices angloparlantes de 26 y 64 años vocalizan 200 palabras insertadas en la frase \emph{Say the word\_\_\_} donde se interpretan 7 emociones a enfado, asco, miedo, felicidad, neutral, tristeza y sorpresa. Los archivos están organizados en carpetas atendiendo a la actriz y a la emoción que representa, y nombrados mediante 3 cadenas de caracteres separadas por un guión bajo donde la primera indica la actriz, la segunda la palabra que se pronuncia, y el tercero la emoción.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.35]{no-photo.jpg} 
		\caption{Distribución de las emociones en TESS}
		\label{fig:emociones_tess}
	\end{figure}

	En la figura \ref{fig:emociones_tess} vemos la distribución de las emociones en la base de datos TESS.


	
	\subsection{Idioma con raíces fonéticas similares: Alemán} Este conjunto de datos pertenecerá a un idioma con unas raíces similares al idioma de referencia, de manera que se espera a priori que se puedan reconocer la mayoría de las emociones. Para este caso, propondremos el alemán ya que el idioma de referencia (inglés) es una lengua germánica occidental \cite{wikipediaIngles2019}.\\
	
	Para este caso el conjunto de datos propuesto es la Base de Datos del Discurso Emocional de Berlín (EMODB, por sus siglas en inglés \emph{Berlin Database of Emotional Speech}). Este corpus contiene 800 grabaciones interpretadas por 10 actores (5 hombres y 5 mujeres) modulando 7 emociones en el idioma alemán. Cada archivo tiene una frecuencia de muestreo de 16 kHz con una resolución de 16 bits, y una duración de 3 segundos de media. Como en el anterior, se utiliza una nomenclatura para nombrar a los archivos que satisface lo siguiente:
	\begin{itemize}
		\item Las dos primeras posiciones determinan el actor que las interpreta.
		\item De la posición 3 a las 5 se define el texto que se pronuncia
		\item La posición 6 indica la emoción.
		\item Versión del audio en caso de que la hubiese (codificado con letras).
	\end{itemize}

	Como ejemplo, el archivo \emph{03a01Fa.wav} indica que el actor 03 (hombre de 31 años) cita el texto a01 (\emph{Der Lappen liegt auf dem Eisschrank}, en alemán "El mantel está colgando del frigo"), con la emoción F (felicidad), y es la versión \emph{a} (la primera).
	
	La documentación del corpus también nos ofrece información sobre el género y edad de los actores, lo cual se ha determinado irrelevante, y las distintas frases que pueden aparecer en los archivos.
	Las emociones que clasifica son enfado (W), aburrimiento (L), asco (E), miedo o ansiedad (A), felicidad (F), tristeza (T) y neutral (N) codificadas en el nombre del archivo por su inicial en alemán (especificada entre paréntesis).
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.25]{distribucion_emo.JPG} 
		\caption{Distribución de las emociones en EMODB}
		\label{fig:emociones_emodb}
	\end{figure}
	


	
	\subsection{Idioma con raíces fonéticas distintas} Este conjunto de datos pertenecerá a un idioma con unas raíces más distantes al idioma de referencia.\\
	
	
	\section{Extracción de características}
	\label{sec:extraccion_cap4}
	Teniendo en cuenta el previo estudio de la literatura en el capítulo 2, se concluye que los métodos más prometedores, y que por lo tanto merecen la pena aplicar a este estudio comparativo serían los siguientes:
	\subsubsection{Coeficientes Cepstrales en la escala de Mel}
	Como ya hemos mencionado, MFCC es uno de los mejores algoritmos para capturar características de la señal de audio debido a su similitud a como el sistema auditivo humano procesa el sonido y las frecuencias, así mismo, su efectividad se ha visto reportada y discutida a lo largo de otros estudios.
	La librería usada para la manipulación de audio Librosa ofrece la posibilidad de extraer características MFCC de un archivo de audio. En cuanto a la configuración, se extraerán 13 características MFCC usando el rango de muestreo del propio archivo de audio.
	
	\begin{comment}
	Razon por el numero de características:
	https://dsp.stackexchange.com/questions/28898/mfcc-significance-of-number-of-features
		Mencionar que otros estudios
	\end{comment}
	
%	\paragraph{Espectogramas de Mel}
%	La escala de Mel transforma  frecuencias lineales a una escala logarítmica

	\section{Configuración}
	En esta sub-sección se muestra los recursos a los que se ha accedido para el desarrollo del estudio, así como su correspondiente configuración.
	\begin{comment}
	\begin{wrapfigure}{l}{0.40\textwidth}
	\vspace{-20pt}
	\centering
	\includegraphics[width=0.4\linewidth]{/logos/colab_icon.png} 
	\vspace{-20pt}
	\end{wrapfigure}
	\end{comment}
	
	
	\paragraph{Google Colab} Para la exploración de los datos así como el desarrollo, y entrenamiento de los modelos se ha hecho uso de la plataforma gratuita desarrollada por Google, Google Colab. Esta plataforma ofrece 12GB de RAM  y 107.77GB de espacio en disco, que será más que suficiente dado el tamaño que nuestro dataset.
	
	\paragraph{Librosa 0.8.1} Librosa es un paquete que ofrece diversas funcionalidades para el análisis de audio y música, cuya información más en detalle se puede encontrar en \cite{librosa082}. Esta librería ha sido esencial para la extracción de características MFCC así como algunas técnicas de aumento de datos.
	
	\paragraph{OpenCV 3.4.2} OpenCV es una librería de código abierto desarrollada por INTEL en 1999, cuyo principal objetivo es la provisión de funciones y recursos para visión computacional, cubriendo áreas como la reconstrucción 3D, detección de movimiento o reconocimiento de objetos, etc. Si bien en este proyecto no se necesitarán sus recursos más avanzados, será útil en la lectura de espectrogramas como imagen. 
	
	\paragraph{Tensorflow 2.0} Es una plataforma de código abierto originalmente creada por Google que provee un conjunto de librerías y recursos para el desarrollo de modelos con aprendizaje automático. Tensorflow, que además ofrece soporte de Keras, se ha usado tanto para la estandarización de los datos, como para la compilación y entrenamiento del modelo.
	
	\paragraph{Python 3} Python es un lenguaje interpretado de alto nivel. Todo el código para este proyecto ha sido desarrollado en python 3.
	
	\section{Pre-procesado de los datos}
	Como se ha visto en la sección \ref{lb_c4_datos} no hay una abundante disposición de datos, esto podría convertirse en un problema y perjudicar el rendimiento del modelo en el entrenamiento. 
	Será necesario, antes del entrenamiento, un previo procesado de los datos.
	
	\subsection{División de los datos por género}
	\label{cap4:division}
	En un primer análisis exploratorio de los datos, se han estudiado las diferencias entre la voz masculina y la voz femenina en las emociones, observando lo siguiente:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.35]{comparative_waveform.png} 
		\caption{Comparativa de los extractos de voz por género en RAVDESS}
		\label{fig:comp_emociones_genero}
	\end{figure}
	En la figura \ref{fig:comp_emociones_genero} podemos ver la comparación de la voz masculina (naranja) y la voz femenina (azul) por cada una de las emociones en el idioma inglés. Ya que es muy distinto, puede ser recomendable dividir el conjunto de datos atendiendo a esta característica.
	
	
	\subsection{Normalización y estandarización de los datos}
	\label{cap:normalizacion}
	Este proceso se aplicará a todas las pruebas
	\begin{comment}
		# Now because we are mixing up a few different data sources, it would be wise to normalise the data. 
		# This is proven to improve the accuracy and speed up the training process. Prior to the discovery of this solution in the embrionic years of neural network, 
		# the problem used to be know as "exploding gradients".
	\end{comment}
	
	
	\subsection{Ténicas de aumento de datos}
	Dado el bajo número de muestras en los distintos conjuntos de datos a los que hemos podido acceder, se ha visto conveniente explorar distintas técnicas de aumento de datos. El aumento de datos es una técnica por la cual, se aumenta el número de muestras en un conjunto mediante la creación de nuevas muestras sintéticas con pequeñas modificaciones a cada uno de los archivos. Esta aumento de los datos se puede traducir por una reducción del overfitting (sobreajuste), ya que el modelo se mantendría invariable mejorando así su capacidad de generalización.
	Esta técnica es ampliamente conocida cuando se procesan imágenes, siendo esas modificaciones rotaciones, transposiciones etc. En nuestro caso, sabemos que el sonido tiene las siguientes características: tono, duración, timbre e intensidad. Por lo que debemos modificar levemente nuestros datos alrededor de esas características de manera que sólo difieran en pequeños factores de la muestra original.
	% VER: https://medium.com/@keur.plkar/audio-data-augmentation-in-python-a91600613e47
	
		\subsubsection{Ruido Blanco}
		Añadir ruido blanco a la pista de audio, implica la inyección de valores aleatorios distribuidos de manera irregular con una media de 0 y una desviación de estándar de 1. Para implementar este método se usará el paquete numpy.
		
		\begin{figure}[!htb]
			\begin{minipage}{0.45\textwidth}
				\centering
				\includegraphics[width=.6\textwidth]{original-waveform.JPG}
				\caption{Muestra original}
				\label{ref:audio_original_1}
			\end{minipage}\hfill 
			\begin{minipage}{0.45\textwidth}
				\centering
				\includegraphics[width=.6\textwidth]{wn-waveform.JPG}\hfill
				\caption{Misma muestra con ruido blanco}
				\label{ref:audio_wn}
			\end{minipage}
		\end{figure}
				
				
		\subsubsection{Desplazamiento del sonido}
		Desplaza el sonido hacia la izquierda o la derecha una cantidad aleatoria de segundos. Por ejemplo, si el sonido ha sido desplazado hacia delante (izquierda) x segundos, los x primeros segundos se marcan con 0. Si por el contrario han sido desplazados hacia detrás (derecha), los últimos x segundos se marcarán con 0.
		
		\begin{figure}[!htb]
			\begin{minipage}{0.45\textwidth}
				\centering
				\includegraphics[width=.6\textwidth]{original-waveform.JPG}
				\caption{Muestra original}
				\label{ref:audio_original_2}
			\end{minipage}\hfill 
			\begin{minipage}{0.45\textwidth}
				\centering
				\includegraphics[width=.6\textwidth]{shift-waveform.JPG}\hfill
				\caption{Misma muestra con desplazamiento del sonido}
				\label{ref:audio_shiftting}
			\end{minipage}
		\end{figure}
		
		\subsubsection{Cambio de tono}
		Se refiere al proceso de cambiar el tono a un sonido sin variar su velocidad. Para implementar este método se usará la librería Librosa que ofrece un método específico.
		
		\begin{figure}[!htb]
			\begin{minipage}{0.45\textwidth}
				\centering
				\includegraphics[width=.6\textwidth]{original-waveform.JPG}
				\caption{Muestra original}
				\label{ref:audio_original_3}
			\end{minipage}\hfill 
			\begin{minipage}{0.45\textwidth}
				\centering
				\includegraphics[width=.6\textwidth]{pitch-waveform.JPG}\hfill
				\caption{Misma muestra con cambio del tono}
				\label{ref:audio_tunning}
			\end{minipage}
		\end{figure}
	
	
	\subsection{Espectrogramas}
	%Como ya se había explicado en la sección \ref{label}, los espectrogramas son la representación visual de un espectro de frecuencias de una señal que varía con el tiempo \cite{wikipedia2021}
	Como vimos en el capítulo 2, y siguiendo los pasos de los trabajos \cite{Anvarjon2020} y \cite{Mustaqeem2020}, el uso de espectogramas hace referencia a la conversión de la señal a imagen, y el objetivo de esta técnica es aprovechar las fortalezas de las redes convolucionales en las imágenes aplicándolas a un problema de señal de audio. En concreto para este trabajo, se hará uso de los espectrogramas de las características MFCC de la señal, cuyo proceso constará de dos partes: 
	\begin{enumerate}
		\item Generación de espectogramas como imágen
		\item Lectura y procesado de las imágenes que alimentarán la red
	\end{enumerate}

	Para generar estos espectrogramas, se hará con la ayuda del paquete Librosa, especificando en los correspondientes parámetros la extracción 13 características MFCC; Una vez generadas, se guardan en disco recortando el padding 0.05 inches y en formato jpg.
	Finalmente, las imágenes generadas son leídas con la ayuda de OpenCV donde se transforma su canal de color a RGB y son redimensionadas con un tamaño de 40 x 30 píxeles.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.35]{mfcc_espectrogram.jpg} 
		\caption{Epectrograma MFCC de una onda de audio. Fuente propia}
		\label{fig:mfcc_sample}
	\end{figure}
	
	
	\section{Arquitectura}
	\label{sec:arquitectura_cap4}
	Como se ha podido ver en la revisión de la literatura del capítulo 2, las redes convolucionales esta una tendencia muy adoptada en los últimos trabajos en esta área de estudio.
	
	%Por temas de tiempo blablabla [...] se ha decidido por una red base que reporte buenos resultados.
	%blabla...
	
	%Los trabajos como los de \cite{AbdulQayyum2019}, \cite{Anvarjon2020} utilizan CNN en sus trabajos. 
	%Para un caso base en la línea de trabajo se ha optado por una red convolucional inspirada %en el trabajo de \cite{AbdulQayyum2019}, la cual consta de:
	\subsection{Arquitectura principal}
		\subsubsection{Modelo 3 CNN 2D}
		\label{cap4:Modelo3}
		% TODO Describir inicio, inspiracion etc
		Esta arquitectura consiste en:
		\begin{itemize}
			\item 3 capas convolucionales bidimensionales con 32 filtros y un tamaño del kernel de 4 x 10. Como función de activación se usa Relu y pading establecido a 'same'.
			
			\item A las todas las capas convolucionales, les sigue una capa Max Pooling con tamaño para la \emph{pool} de 3, que posteriormente se aplica un Dropout del 20\%
			
			\item Una capa Flatten, seguida de la capa densa de salida con 7 nodos y activación Softmax.
		\end{itemize}
		
		La estrategia de entrenamiento que se siguió fue un optimizador Adam con los parámetros por defecto que ofrece Keras y la entropía cruzada categórica (\emph{categorical crossentropy}) como función de pérdida.
		
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.35]{no-photo.jpg} 
			\caption{Arquitectura propuesta}
			\label{fig:lstm-cnn}
		\end{figure}
	
	
	\subsection{Otras arquitecturas}
	A pesar de que la arquitectura citada en la sección anterior, será con la que se lleven a cabo los principales experimentos y con los resultados más interesantes, consideramos que la mención de otras arquitecturas que han formado parte de los experimentos de este trabajo, ayudará al lector a entender la progresión de los mismos, por lo que se ha visto conveniente incluirlas.
	
		% TODO:
		\subsubsection{Modelo1 CNN}
		\label{cap4:Modelo1}
	
		\subsubsection{Modelo2 CNN}
		\label{cap4:Modelo2}
		En un principio, se optó por una línea de trabajo inspirada en el estudio de \cite{AbdulQayyum2019} ya que combina buenos resultados y un sistema sencillo. Pero tras varias pruebas esta arquitectura se ha refinado hasta definirse, por ahora, lo siguiente:
		
		\begin{itemize}
			\item 2 capas convolucionales unidimensionales con activación Relu. El número de filtros es de 128 y  y el tamaño del kernel de 5.
			En las dos capas convolucionales se usa regularización de tipo L2 para aplicar una penalización a las capas del kernel con un valor de 0.01 y corregir así el overfitting.
			
			\item La primera capa convolucional está seguida de una capa Dropout del 0.5 y una capa Max Pooling con un tamaño 8.
			
			\item A la segunda capa convolucional se sigue otra capa de Dropout con un valor del 25\% y una capa Flatten.
			
			\item Por último esta arquitectura cierra con una capa densa con 7 nodos (número de clases) con una función de activación Softmax.
		\end{itemize}
	
	En cuanto al entrenamiento del modelo se usará un optimizador RMSprop con una tasa de aprendizaje de 0.00005, valor de $\rho$ de 0.9 y $\epsilon$ a 'None', por dar mejores resultados frente a Adam del caso inicial desde el que se partió, mientras que la función de pérdida utilizada para este propósito será entropía cruzada categórica (\emph{categorical crossentropy}).\\
	Se añaden además, para intentar afinar el modelo los callbacks ReduceLROnPlateau, que reduce la tasa de aprendizaje cuando el modelo ha dejado de mejorar y EarlyStopping, que detiene el entrenamiento si se ha llegado una meseta, es decir, si durante un determinado número de épocas, el modelo ha dejado de mejorar. En ReduceLROnPlateau se monitorizará la \emph{val loss} con el fin de minimizarla, y como configuración se empleará un factor de reducción de la tasa de aprendizaje de 0.9, una paciencia de 20 épocas y una tasa de aprendizaje mínimo de 0.000001.
	Con respecto a EarlyStopping, la variable monitorizada será 'val accuracy' con el fin de maximizarla y una paciencia de 20 épocas.\\
	Este entrenamiento se llevará a cabo durante 1000 épocas cpn nun batch de tamaño 16.
		% Hablar de las bondades las redes con 1 dimension
		% En que ayudan


	
		\subsubsection{Modelo4 CNN-LSTM}
		\label{cap4:Modelo4}
		% TODO Describir inicio, inspiracion etc
		\begin{itemize}
			\item 3 capas convolucionales unidimensionales con 64 filtros de 3 x 3 y activación Relu, seguidas de una capa Max Pooling con tamaño 2 para la pool.
			
			\item Una capa Flatten con un Dropout del 25\%
			
			\item 2 capas LSTM unidimensionales con 50 y 20 unidades respectivamente y un Dropout del 50\%. Sólo permitiremos a la primera capa LSTM devolver el estado oculto de salida por cada entrada de tiempo. Ya que a este nivel se cambia a redes unidimensionales, deberemos redimensionar la entrada a 1 x 960.
			
			\item Capa de salida densa de 7 nodos y función Softmax.
			
		\end{itemize}


	
	\section{Criterios de éxito}
	El objetivo de esta sección es definir las métricas que se usarán para comparar los distintos modelos en los experimentos parciales, así como los resultados obtenidos al aplicar dichos modelos a los datos mencionados en la sección \ref{lb_c4_datos}.\\
	Las dos principales métricas que se usarán para decidir cómo de buena es la predicción del modelo serán:
	\begin{itemize}
		\item \textbf{Exactitud o \emph{Accuracy} } Establece una comparación entre los resultados predichos y los obtenidos determinando cómo de preciso es el algoritmo cuando se trata de identificar las clases
		
			\[
				Accuracy = \frac{TP + TN}{ TP + FN + TN + FP} 
			\]
 			\hfill \break

		\item \textbf{F1 score} Siendo el \emph{recall} la fracción de elementos relevantes que son recuperados (el cociente de las predicciones positivas y el número de clases positivas en el conjunto de test), la medida de F1 Score conviene el balance entre la precisión y el \emph{recall}.
		\[
			Recall = \frac{TP}{ TP + FN} 
		\]
		
		\[
		%\begin{equation}
			Precision = \frac{TP}{TP + FP}
		%\end{equation}
		\]
		
		\[
			F1 Score = \frac{2 * (Precision * Recall)}{Precision + Recall}
		\]
		\hfill \break
		
		\item \textbf{Matriz de confusión} Permite una rápida visualización del resultado de la ejecución del algoritmo. Muestra el nivel de predicción por cada clase. En la figura \ref{fig:confusionMatrix} podemos ver una representación de esta.
		
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.3]{confusionMatrix.png}
			\caption{Representación de la matriz de confusión. Fuente: \href{https://rpubs.com/chzelada/275494}{Rpubs}}
			\label{fig:confusionMatrix}
		\end{figure}
	
	\end{itemize}

	En algunos casos, la \emph{accuracy} puede ser errónea debido a la paradoja de la exactitud donde puede existir un sesgo debido a una distribución desbalanceada de las clases. Esto hace que pueda ser más inteligente elegir un modelo con menor exactitud pero con mayor poder predictivo.
	Para ver ese poder predictivo por lo tanto, es aconsejable elegir más de una métrica de evaluación. Para ello se contará con F1 Score que es la media armónica entre el \emph{recall} y la precisión, y la matriz de confusión que nos permitirá observar como se comporta un modelo especifico preidiciendo las clases.
	
	
	\section{Diseño de los experimentos}
	%TODO intro
	% Ahora se detallaran los experimentos blablableble
	%Por cuestiones de espacio, se han incluido unicamente la mejor versión de ese diseño tatata...
	En la presente sección se expondrán los diseños de los experimentos para cumplir nuestro cometido. Ya hablábamos en el capítulo 3, que nuestro propósito general, era doble: por un lado conseguir un modelo que fuera capaz de clasificar satisfactoriamente en el propio idioma con el que se entrena, y por otro, evaluar este modelo en otras lenguas distintas. Por ello, se divide la presente en dos subsecciones, con la intención de que el lector le resulte más fácil seguir la narrativa de este proyecto.
	
	\subsection{Búsqueda de un  modelo óptimo}
	Estos experimentos giran en torno a la búsqueda del modelo en el que más tarde, se validarán los lenguajes extranjeros. Como resumen se ofrece al final, la tabla \ref{tab:resumenTests1} que resume las características más diferenciadoras de cada uno de estas pruebas.
	
	\subsubsection[]{\large Prueba 1 {\normalsize \textcolor{Gray}{RAVDESS con división de los datos por género en una red convolucional unidimensional}}}
	
	\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
		\paragraph{Datos y preprocesado}
		Para esta prueba se usará el conjunto de datos RAVDESS diviendo previamente el conjunto de entrenamiento por género (720 muestras iniciales por cada uno). 
		
		De cada una de las divisiones, se extraerán las carácterísticas MFCC como es descrito en la sección \ref{sec:extraccion_cap4} y se normalizarán como se especifica en \ref{sec:norm_cap4}. Una vez normalizados, se categorizarán usando la técnica \emph{One hot encoding} para que el modelo asigne una importancia equitativa a todas las clases.
		Finalmente los datos son divididos en entrenamiento (70\%), y test(20\%) y validación(10\%).\\
		
		\paragraph{Entrenamiento}
		Para el entrenamiento se usará el modelo Modelo1 CNN descrito en la sección \ref{cap4:Modelo1}. El modelo es entrenado durante un total de 100 épocas y con un tamaño del batch de 16.
	\end{minipage}
	

	\subsubsection[]{\large Prueba 1.B {\normalsize \textcolor{Gray}{RAVDESS en una red convolucional unidimensional}}}
	Se aplicarán los mismos parámetros que en la Prueba 1 con la diferencia que se usará el conjunto de datos RAVDESS al completo.

	%PRUEBA 2 - NOTEBOOK 3
	\subsubsection{\large Prueba 2  {\normalsize \textcolor{Gray}{RAVDESS con técnicas de aumento de datos.}}}

	\hfill\begin{minipage}[t]{\dimexpr\textwidth-1cm}
		\paragraph{Datos y preprocesado}Para esta prueba se usará el conjunto de datos RAVDESS completo (audio sólo, 1440 muestras iniciales), haciendo uso de las técnicas de aumento de datos que se han especificado en la sección \ref{sec:aumento_cap4} de manera independiente, a saber:
		\begin{itemize}
			\item Ruido blanco
			\item Desplazamiento del sonido.
			\item Modulación del tono.
		\end{itemize}
		Para ello se extraerán las características MFCC, normalizándolas y categorizándolas con \emph{One hot encoding}.
		
		\paragraph{Entrenamiento}Para este entrenamiento se usará el modelo descrito en la subsección \ref{cap4:Modelo2}. el cual está basado en una red convolucional unidimensional de tres capas y reluzarización L2 en las dos primeras.
		El modelo se compila con un optimizador RMSprop con un learning rate de 0.00005 y $\rho$ a 0.9, y finalmente se entrena durante 100 épocas con un batch de tamaño 16.\\

	\end{minipage}
	
	%PRUEBA 2.B - NOTEBOOK 5
	\subsubsection[]{\large Prueba 2.B {\normalsize \textcolor{Gray}{Combinación de las técnicas de aumento de datos en RAVDESS}}}
	
	\hfill\begin{minipage}[t]{\dimexpr\textwidth-1cm}
		\paragraph{Datos y preprocesado}Esta prueba implementará los mismos parámetros que la anterior, pero hará uso de las técnicas de aumento de datos combinándolas entre sí.
		\begin{itemize}
			\item Ruido blanco y desplazamiento del sonido, con un total de 4320 características MFCC.
			
			\item Modulación del tono y desplazamiento, con un total de 4320 características MFCC.
			
			\item Ruido blanco, desplazamiento del sonido y modulación del tono con un total de 5760 características MFCC.
		\end{itemize}

		\paragraph{Entrenamiento} Para el entrenamiento, se seguirán los mismos parámetros que en la prueba 2.	
	\end{minipage}

	%PRUEBA 3 - NOTEBOOK 1_CROSSCORPUS
	\subsubsection[]{\large Prueba 3 {\normalsize \textcolor{Gray}{Combinación de datasets}}}
	
	\hfill\begin{minipage}{\dimexpr\textwidth-1cm}

		\paragraph{Datos y preprocesado} Siguiendo una estrategia distinta con el fin de aumentar el número de muestras, se combinarán diferentes bases de datos (RAVDESS, SAVEE y TESS, detalladas en la sección \ref{sec:idioma_ref}). Se extraerán las características MFCC para luego normalizar las muestras y categorizarlas.
		Las subsecciones de esta prueba corresponden a las distintas combinaciones de los datos:
		\begin{itemize}
			\item RAVDESS y TESS, con un total de 4240 muestras.
			\item SAVEE y TESS, con un total de 3280 características MFCC.
			\item RAVDESS, SAVEE y TESS, con un total de 4720 características.
		\end{itemize}
	
		\paragraph{Entrenamiento} Para el entrenamiento se usará el modelo Modelo2 CNN basado en una arquitectura CNN unidimensional, tal y como se detalla en la sección \ref{cap4:Modelo2}, entrenándolo durante 100 épocas con un batch de tamaño 32.
	\end{minipage}

% TODO: explicar estos de abajo y detallar mas lo de arriba (conjunto de datos y entrenamiento??, estrategias de entrenamiento blabla bleble...)
	\subsubsection[]{\large Prueba 3.B {\normalsize \textcolor{Gray}{Combinación de datasets con aumento de datos}}}
	% basandonos en las pruebas anteriores, se aplicara esta tecnica o esta otra
	Se seguirán los mismos parámetros en la Prueba 3, pero aplicando técnicas de aumento de datos (en concreto se aplicará Ruido blanco, por ser la que mejor resultado reportó).
	
	%PRUEBA 3 - NOTEBOOK 2_CROSSCORPUS
	\subsubsection{\large Prueba 4 {\normalsize \textcolor{Gray}{Red CNN bidimensional con espectrogramas}}}
	
	\setlength{\leftskip}{1cm}
	%\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
		\paragraph{Datos y preprocesado} Para esta prueba se ha decidido prescindir de la base de datos RAVDESS y usar en su lugar, la combinación SAVEE y TESS con un total de 3284 muestras. Con respecto al preprocesado de los datos, se extraerán las características MFCC y posteriormente esas características serán convertidas a imagen.

		\paragraph{Entrenamiento} Para el entrenamiento se usará el Modelo 3 basado en una arquitectura CNN bidimensional compilándolo con un optimizador Adam (valores por defecto en keras) tal y como es descrito en la sección \ref{cap4:Modelo3}, entrenándolo durante 100 épocas con un tamaño del batch de 32.
	%\end{minipage}
	%\setlength{\leftskip}{0cm}


	\subsubsection{\large Prueba 5 {\normalsize \textcolor{Gray}{Red LSTM-CNN con espectrogramas}}}
	
	%\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
	\setlength{\leftskip}{1cm}
		\paragraph{Datos y preprocesado} El conjunto de datos usado para esta prueba, será la combinación SAVEE y TESS con un total de 3284 muestras. Con respecto al preprocesado de los datos, se extraerán las características MFCC y posteriormente esas características serán convertidas a imagen, exactamente igual que en la prueba 4.
		\paragraph{Entrenamiento} Para el entrenamiento se usará el Modelo 4 basado en una arquitectura dual CNN-LSTM compilándolo con un optimizador Adam (valores por defecto en keras) tal y como es descrito en la sección \ref{cap4:Modelo4}, entrenándolo durante 100 épocas con un tamaño del batch de 32.
	\setlength{\leftskip}{0cm}	
		
	%\end{minipage}

	Como se menciona en el inicio de esta sección, la tabla \ref{tab:resumenTests1} muestra el resumen con los datos que creemos más característicos de cada una de las pruebas. Así pues, en la columna 'Datos Entr.' se refiere al conjunto de datos que se usarán en el entrenamiento, especificando la naturaleza de estos datos en 'Tipo Dato' (espectrogramas como imagen, o características directamente extraídas de la muestra de audio) mientras que en 'Arquitectura', con qué modelo se llevará a cabo la prueba. En la columna Aumento Datos, se muestra qué técnica de aumento de datos se ha utilizado (en tal caso), donde 'R.B.' corresponde a ruido blanco, 'Desp.' a desplazamiento del sonido y 'Mod.' a modulación del tono.

	\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c | p{3.5cm} | c | c c c | p{3.5cm} |}
			\hline
			\multirow{2}{*}{\textbf{Prueba}} &\multirow{2}{*}{\textbf{Datos Entr.}}& \multirow{2}{*}{\textbf{Arquitectura}} & 
			\multicolumn{3}{|c|}{\textbf{Aumento Datos}} & 
			\multirow{2}{*}{\textbf{Tipo Dato}} \\ \cline{4-6}
			& & & R.B. & Desp. & Mod. & \\ 
			\hline \hline
			
			1 & RAVDESS por genero & CNN 1D simplificado & No &No &No & carac. MFCC	\\ \hline
			1.B & RAVDESS & CNN 1D simplificado &No &No &No & carac. MFCC\\ \hline
			\multirow{3}{*}{2} & \multirow{3}{*}{RAVDESS} & \multirow{3}{*}{CNN 1D} &Sí &No &No  & \multirow{3}{*}{carac. MFCC}\\
			& & & No &Sí &No & 	\\
			& & & No &No &Sí  & 	\\ \hline
			
			\multirow{3}{*}{2.B} & \multirow{3}{*}{RAVDESS} & \multirow{3}{*}{CNN 1D } & Sí &Sí &No & \multirow{3}{*}{carac. MFCC}\\
			
			& &  & No &Sí &Sí & \\
			& &  & Sí &Sí &Sí& \\ \hline
			
			\multirow{3}{*}{3} & RAVDESS y TESS & \multirow{3}{*}{CNN 1D} &  No &No &No & \multirow{3}{*}{carac. MFCC}	\\ 
			\cline{2-2}
			& SAVEE y TESS &  &  No &No &No & 	\\
			\cline{2-2}
			& RAVDESS, SAVEE y TESS &  &  No &No &No & 	\\ \hline
			
			\multirow{3}{*}{3.B} & RAVDESS y TESS & \multirow{3}{*}{CNN 1D} & \multirow{3}{*}{ Sí}&\multirow{3}{*}{ No} & \multirow{3}{*}{ No} & \multirow{3}{*}{carac. MFCC}	\\
			& SAVEE y TESS &  & &&  & 	\\
			& RAVDESS, SAVEE y TESS &  & &&  & 	\\\hline
			
			4 & SAVEE y TESS & CNN 2D &Sí &No &No & espectrogramas MFCC\\ \hline
			5 & SAVEE y TESS & CNN 2D - LSTM &Sí &No &No & espectrogramas MFCC\\ \hline
			
		\end{tabular}
		
		\caption{Resumen de las pruebas para la obtención de un modelo óptimo en la propia lengua (inglés)}
		\label{tab:resumenTests1}
		\end{center}
	\end{table}


	\subsection{Pruebas con lenguajes extranjeros}
	
	\subsubsection{\large Prueba 6 {\normalsize \textcolor{Gray}{Red CNN 1D validada en alemán}}}
	
	\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
		
		\paragraph{Datos y preprocesado}  El conjunto de datos usado para el entrenamiento será la combinación SAVEE y TESS, sin embargo para que las clases del conjunto de entrenamiento y las de validación coincidan, es necesario prescindir de dos clases: sorpresa y miedo.\\ 
		Por otro lado, los datos de validación presentaban una distribución muy irregular, por lo que para que tuvieran una representación más equilibrada y una interpretación más justa en la validación, se han balanceado aumentando el dataset con aumento de datos (ruido blanco) y se han igualado el número de instancias de cada clase a aquella con una proporción menor.\\
		A continuación se extraen 13 características MFCC y después de estandarizar los datos como se detalla en la sección \ref{cap:normalizacion}, se dividen en entrenamiento y test. Debido a que los datos que se usarán para la validación provienen de una fuente externa a la de entrenamiento el balance para esta división ha sido de 75\% para entrenamiento y 25\% para test.\\
		
		\paragraph{Entrenamiento} Para el entrenamiento se utilizará el Modelo 1CNN compilándolo con el optimizador RMSProp como se describe en \ref{cap4:Modelo1}. Posteriormente es entrenado durante 100 épocas con un batch de tamaño 32.
		
	\end{minipage}

	\subsubsection[]{\large Prueba 7 {\normalsize \textcolor{Gray}{Red CNN 2D validada en alemán}}}
	
	%\begin{adjustwidth}{1cm}{}
	\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
		\paragraph{Datos y preprocesado} Al igual que en la prueba anterior, se reducirán de ambos conjuntos de datos el número de emociones a 5, eliminando de la ecuación 'miedo' y 'sorpresa'. Seguidamente, se procederá a la generación de imágenes donde:
		\begin{enumerate}
			\item En primer lugar se aplicará aumento de datos (sólo con ruido blanco).
			\item Se leerán hasta un máximo de 92 para que quede balanceado.
		\end{enumerate}
		Una vez generadas las imágenes, se leerán con las dimensiones 30 x 40.
		
		Ya que que los datos que se usarán para la validación provienen de una fuente externa a la de entrenamiento el balance para esta división ha sido de 75\% para entrenamiento y 25\% para test. 
	
		\paragraph{Entrenamiento} En esta prueba se usará el Modelo 3 basado en una red convolucional bidimensional, entrenándolo durante 100 épocas con un tamaño del batch de 32.
		
	%\end{adjustwidth}
	\end{minipage}

	\subsubsection[]{\large Prueba 8 {\normalsize \textcolor{Gray}{Red CNN-LSTM validada en alemán}}}
	
	\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
		
		\paragraph{Datos y preprocesado}  Este procedimiento, será exactamente igual que el de la prueba 7, tanto para los datos de entrenamiento como para los de validación.
		
		\paragraph{Entrenamiento} Para el entrenamiento de esta prueba se usará el Modelo 4 basado en una red CNN- LSTM, entrenándolo durante 100 épocas con un tamaño del batch de 32.
		
	\end{minipage}



	\begin{table}[H]
		\centering
		\begin{center}
			\begin{tabular}{| c | c | c | c | c |}
				\hline
				\textbf{Prueba} &
				\textbf{Emociones}& 
				\textbf{Arquitectura} & 
				\textbf{Datos Val.} & 
				\textbf{Tipo Dato} \\ 
				\hline\hline
				6 & \multirow{3}{*}{5} & CNN 1D & \multirow{3}{*}{EMO-DB} & caract. MFCC\\ 
				7 &  & CNN 2D &  & espetrog. MFCC\\ 
				8 &  & CNN 2D - LSTM &  & espetrog. MFCC\\ \hline
				9 & \multirow{3}{*}{6} & CNN 2D & \multirow{3}{*}{CAFFE} & caract. MFCC\\  
				10 &  & CNN 2D &  & espetrog. MFCC\\ 
				11 &  & CNN 2D - LSTM &  & espetrog. MFCC\\
				\hline
				
			\end{tabular}
			
			\caption{Resumen de las pruebas en las que se aplican los mejores modelos a distintos lenguajes.}
			\label{tab:resumenTests2}
		\end{center}
	\end{table}
	
	
	
	

		
		
		

	
	
	
		\printbibliography
	
\end{document}