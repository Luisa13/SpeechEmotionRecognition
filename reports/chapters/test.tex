\documentclass[11pt,a4paper,spanish]{book}
\usepackage{estilo_unir}

\usepackage[style=apa, natbib=true, backend=biber]{biblatex}
\usepackage{graphicx, float, hyperref, comment, multicol, enumitem, subcaption, footnote}
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\graphicspath{ {./.images/} }
% Formato
%\usepackage[utf8]{inputenc}
\setlength{\parindent}{0pt} % para evitar las sangrias en todo el doc


% Para el formato del titulo: numero Nombre Capitulo
\usepackage{titlesec}
\titleformat{\chapter}[block]
{\normalfont\huge\bfseries}{\thechapter.}{1em}{\huge}

% Tablas
\usepackage{multicol, multirow, adjustbox, changepage}
\usepackage{changepage}
\usepackage[dvipsnames]{xcolor}

% Define el nivel de profundidad de las secciones para numerarlas
\setcounter{secnumdepth}{3}
\addbibresource{./bibLib/internet.bib}
\addbibresource{./bibLib/referenceLib.bib}



\raggedbottom % para los espacios en blancos de más
%---------------------------
%título del trabajo y autor
%---------------------------
\title{Reconocimiento de Emociones en la Lengua no Aprendida}
\author{Luisa Sánchez Avivar}
\director{Ciro Rodríguez León}
\date{Julio del 2021}
\nombreciudad{Ginebra}

%---------------------------
%marges
%---------------------------
%\usepackage[margin=1.9cm]{geometry}
%---------------------------
%---------------------------
%---------------------------
%---------------------------
% Para los espacios en los titulos
\makeatletter
\usepackage{titlesec} 
\patchcmd{\chapter}{\if@openright\cleardoublepage\else\clearpage\fi}%
{\par\addvspace{\baselineskip}}{}{}
\titlespacing*{\chapter}{0mm}{0em}{1em}
\patchcmd{\ttl@save@mkschap}{*}{}{}{}
\makeatother

\begin{document}
\renewcommand{\listfigurename}{Índice de Ilustraciones}
\renewcommand{\listtablename}{Índice de Tablas}
\renewcommand{\contentsname}{Índice de Contenidos}
\renewcommand{\figurename}{Figura}
\renewcommand{\tablename}{Tabla} 

\maketitle

\frontmatter
\tableofcontents
\listoffigures
\listoftables

\chapter{Resumen}
En este estudio se llevó a cabo un reconocimiento emocional de la voz multi-lingüístico. Para ello, se implementaron tres modelos distintos entrenados en inglés, y posteriormente fueron evaluados en dos lenguas extranjeras que no formaron parte del entrenamiento (francés y alemán). Las características cepstrales de la escala de Mel se extrajeron a partir de las muestras de audio y fueron usadas en los tres clasificadores con una arquitectura basada en redes convolucionales. El uso de espectrogramas en una arquitectura híbrida de redes convolucionales y LSTM se mostró superior frente a los otros, consiguiendo un 92.06 \% de exactitud en una clasificación monolingu?ística. Por otro lado, la clasificación multi-lingüística no arrojó resultados satisfactorios aplicando el mismo método.


{\bf Palabras Clave:} CNN-LSTM, Reconocimiento de emociones en el habla, características espectrales, Lengua extranjera.

\chapter{Abstract}

This work performs a speech emotional recognition through three languages. For this purpose, three different models have been implemented and trained in english, and subsequently tested in other two languages which never took part in the training (french and german). It is assumed that speech audio signals carry emotional information that can be retrieved and hence MFCC features are extracted since they are recognized as best suited to represent emotions through prosody. Different classifiers based on convolutional neural network (CNN) architecture are used (unidimensional CNN, bidimensional CNN and LSTM-CNN). The results show that CNN-LSTM outperforms over the other options with a 92.06\% in a monolinguistic clasification in english, while appliying the same approach in a cross language classification did not deliver satisfactory results.

{\bf Keywords:} CNN-LSTM, foreign language, MFCC, spectral features, Speech Emotion Recognition.



\lfoot{Reconocimiento de Emociones en la Lengua no Aprendida}
\mainmatter


\chapter{Introducción}
La importancia de la comunicación no sólo se basa en qué se dice, sino también en cómo se dice.
Desde hace años, el reconocimiento de emociones a través de la voz ha sido motivo de interés para la investigación, sin embargo siempre se ha estudiado sobre un mismo lenguaje debatiendo la habilidad de reconocer y clasificar las emociones oralmente expresadas. Esta habilidad ha sido respaldada por numerosos artículos donde se concluye que es posible distinguir e identificar entre al menos tres emociones básicas (Felicidad, Tristeza, y Enfado) a través de la voz (sin necesidad del procesamiento del lenguaje natural y por lo tanto de un contexto).

Análogamente, el debate del reconocimiento de emociones en un plano intercultural también se ha enfocado a través del estudio de los gestos faciales en conjunto con la expresión vocal, donde se concluye que los factores sociales tienen un gran impacto, ya que la identificación de las emociones es más fácil para los miembros de la misma cultura que para los de otra distinta \citep{Pell2009a, Pell2009}. A pesar de ello hay una gran carencia de comparativas con respecto a la voz donde se demuestre una sólida influencia cultural, sin embargo parece claro que las dimensiones socio culturales que engloban nuestras interacciones pueden tener un gran impacto en nuestra comunicación dentro de un marco emocional.

\section{Motivación}


El espectro emocional que una persona esconde en su discurso es un factor esencial de la comunicación humana y ofrece información adicional sin alterar el contenido lingüístico. Las tecnologías orientadas a convertir la voz en texto (\emph{speech to text}) no tienen una forma segura de medir la calidad del diálogo de su interlocutor, impactando en negocios que hacen uso de estos avances (por ejemplo, centros de atención al cliente donde miden su grado de satisfacción). La compañía OTO, dedicada a creación de sistemas inteligentes \citep{OTOIntro2018} centrados en la decodificación de la voz, reportó que hay al menos 3000 formas de decir "Gracias" (figura \ref{fig:thankyou}).

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{dimensionThankYou.JPG} 
	\caption{Dimensiones acústicas en la forma de decir "Thank you". Fuente: \href{https://otosystemsinc.medium.com/introducing-oto-99199a9b2c1b}{OTO}}
	\label{fig:thankyou}
\end{figure}

Es indudable el impacto que ha creado la inteligencia artificial en la forma en la que nos comunicamos con las máquinas a día de hoy \citep{Lisetti1998}. La importancia de la interacción con las máquinas a través de comandos de voz, se ha visto acentuada gracias a la aparición de asistentes inteligentes como Siri en Apple \citep{Effron2011} o Alexa en Amazon \citep{Arnold2017} , que han explotado las diferentes áreas del análisis de la voz con el objetivo de mejorar la experiencia de usuario. Otras compañías como OTO han desarrollado modelos de análisis de voz capaces de detectar atributos únicos en la voz del interlocutor, lo que es usando por centros de asistencia telefónica para potenciar y mejorar sus sistemas automáticos \citep{OTOPersonalAssistance}.
En definitiva, desde el primer software de reconocimiento por voz que fue presentado por IBM en 1961 reconociendo dieciséis palabras y dígitos\citep{IBMser}, hasta la aparición de Google Home en 2017 \citep{Hautala2015}, este tipo de asistentes han ido mejorando su alcance y capacidades.\\
% SEGUIR CON ESTO
%https://www.arxiv-vanity.com/papers/1912.10458/

El uso de estos asistentes no sólo se limita al ámbito doméstico, ya que esta tendencia empieza a extenderse hasta en la aplicación de asistentes personalizados para coches automáticos y asistencia de ayuda telefónica en la industria médica.
Por ejemplo, el uso de altavoces inteligentes han demostrado su efectividad en contrarrestar los efectos de la soledad, el aislamiento y la depresión en personas de la tercera edad al ser usadas en residencias \citep{Mizak2017}.
Novel Effect integró sus sistemas con Alexa (Amazon) para crear una tecnología que ofrece una lectura interactiva con fines educativos incrementando la retención de contenidos y su comprensión \citep{Rowe2018}.
De la misma manera compañías como Capital One en la industria financiera, o KAYAK en el sector turístico han integrado también sus sistemas con Alexa para dar soporte a sus clientes \citep{Collier2016} y \citep{John2017}.

%	REF https://hbr.org/2019/04/your-company-needs-a-strategy-for-voice-technology-2
Sin embargo, a pesar de los avances tecnológicos, estos asistentes de voz normalmente carecen de la habilidad de reconocer el estado emocional del usuario, y cerrar esta brecha podría ser un gran avance en las industrias ya mencionadas. 

Cabe pensar que en este tipo de tecnología haya un potencial interés para la asistencia sanitaria, o incluso, para la industria automovilística. Visualicemos por ejemplo, un conductor tratando de resolver una incidencia mientras conduce. Esta incidencia puede variar desde buscar una ruta alternativa a un hospital o servicio de emergencia, y el estado emocional en el que se encuentre, puede afectar limitando su habilidad para resolver el problema. 

De la misma manera el reconocimiento de emociones puede ocupar un lugar en los asistentes virtuales de cualquier servicio al integrarlo con técnicas del procesamiento del lenguaje natural, permitiendo mayor eficiencia del procesamiento de la conversación al detectar -por ejemplo-  irritabilidad o frustración en el usuario. 
SRI Ventures, una central estadounidense centrada en técnicas de procesamiento de la voz para el desarrollo de aplicaciones principalmente en el ámbito de la salud  \citep{Nuance2019} y \citep{Mocherman2015} desarrolla tecnología para analizar síntomas relacionados con enfermedades respiratorias, así como la aplicación de inteligencia artificial para evaluar por voz los sentimientos del cliente con el fin de mejorar el servicio.

Otro equipo de SRI liderado por Elizabeth Shriberg \citep{Shriberg2003} decidieron combinar un sistema para entender la expresión oral con el análisis de emociones en la voz, dando como resultado una tecnología capaz de modelar computacionalmente la entonación de la voz del interlocutor derivando el significado sentimental más allá de las palabras usadas.

Estos ejemplos, impulsan la motivación de crear un sistema capaz de crear una respuesta, no sólo coherente en el plano semántico, sino también sensible al estado emocional del usuario.


\section{Planteamiento del Trabajo}

Este trabajo de fin de máster se centra en el uso de técnicas basadas en redes artificiales para la clasificación de emociones a través del discurso en la lengua extranjera. Para acercarnos a este escenario, se parte del supuesto que dado un modelo entrenado en un lenguaje capaz de reconocer emociones en este, se evalúa en un idioma distinto que nunca ha formado parte del anterior conjunto de datos. 

Para ello, el trabajo se divide en dos objetivos principales: el primero será conseguir un modelo lo suficientemente preciso en una sola lengua, y el segundo comparar su comportamiento con otros lenguajes.

Con respecto a esta disciplina, y como ya se verá en el capítulo 2, existen estudios (en el reconocimiento de emociones en una sola lengua) combinando numerosas variables a tener en cuenta, por lo que se probarán distintos enfoques, analizando sus fortalezas y debilidades.

Una de las limitaciones a las que se hace frente, es el reducido número de muestras en las distintas bases de datos disponibles para entrenar el modelo. Teniendo en cuenta esto, y que el objetivo es incrementar las oportunidades de clasificación en otros conjuntos, este problema se resolverá combinando datos de varias bases de datos distintas, lo que aportará variabilidad a los datos de entrenamiento. Por otro lado, debido a que la mayoría de trabajos encontrados que realizan esta clasificación emocional son en una sola lengua, se parte de un enfoque más ingenuo donde se asume que las señales de audio transportan suficiente información que puede ser extraída. 

La evaluación del modelo resultante en otros idiomas no será de manera arbitraria, sino que, dentro de los conjuntos disponibles que se encuentren, seguirá una estrategia atendiendo al grado de proximidad fonética para ver las diferencias.

Con este estudio se pretende entender mejor la relación entre emoción e idioma y arrojar luz a preguntas como ¿Hay emociones que son más fácilmente reconocibles indistintamente del lenguaje? ¿Hay lenguas donde es más fácil reconocer ciertas emociones? ¿Cómo influye la elección de la base de datos?¿Plantean las técnicas más populares un enfoque adecuado?.


\section{Estructura del trabajo}
	\begin{enumerate}
		\item Se introduce de manera general, el contenido de las distintas partes que componen este trabajo.
		El primer capítulo introduce de manera esquemática el tema principal de este trabajo, justificando su importancia y el impacto en el mundo real. En este mismo capítulo se incluye:
		\begin{itemize}
			\item La motivación, donde se argumenta la relevancia de este trabajo.
			\item El planteamiento del trabajo, que propone de manera general cómo solucionar el problema que se va a encarar.
		\end{itemize}
		
		\item Seguidamente en el Contexto y estado del arte, se realizará una revisión de la literatura actual en relación con el tema a tratar. Se analizarán los resultados conseguidos hasta el momento así como las técnicas y métodos más usados en este ámbito, en concreto tendrá la siguiente estructura:
		\begin{itemize}
			\item Se presentan las características fonéticas del habla y su trasfondo teórico, analizando las posibles limitaciones, o características a tener en cuenta a modo de introducción a las siguientes secciones que son más técnicas.
			
			\item El reconocimiento emocional del discurso propone una forma de modelar el problema haciendo uso de las técnicas que expone.
			
			\item La extracción de características debate sobre qué características encontrar en la voz para reconocer emociones y cómo se pueden extraer.
			
			\item El preprocesado de la señal describe el proceso para convertir la señal de audio a imagen de manera que se aprovechen mejor las ventajas de los clasificadores basados en redes convolucionales.
			
			\item Los algoritmos de clasificación exponen diferentes métodos para categorizar las emociones una vez la señal está procesada.
			
			\item La discusión sobre el estado del arte, revisa otros trabajos donde se han aplicado estas técnicas y cuáles han sido sus resultados.
			
		\end{itemize}
		
		\item La metodología de trabajo, comprende los objetivos generales y específicos que se han marcado para este trabajo así como el proceso que se seguirá para llevarlo a cabo.
		
		\item El capítulo 4, plantea diferentes experimentos con el objetivo de llegar a una conclusión en la comparativa de esta tesis. Describe  todos los componentes que formarán parte de esos experimentos de manera que se puedan reproducir siguiendo los pasos propuestos.
		
		\item El capítulo 5, describe los resultados y el desarrollo de los experimentos planteados en el capítulo anterior, exponiéndolos de manera objetiva.
		
		\item En el capítulo 6 se presenta un análisis de los resultados obtenidos.
		
		\item Finalmente en el capítulo 7 tiene lugar una discusión de la conclusión sobre los resultados, comparándolos con los de otros trabajos, a la vez que se describen posibles líneas futuras
		
	\end{enumerate}


\chapter{Estado del Arte}
\section{Contexto}
El reconocimiento de emociones en el habla es una disciplina en inteligencia artificial que trata de reconocer y clasificar emociones a través de la señal de voz. Este campo de estudio se ha hecho cada vez más popular, pero su origen se remonta a 1996, desde que se presentara el primer trabajo defendible sobre el tema "Reconociendo emociones en el habla" por F.Daellert \citep{Dellaert1996}.
Desde entonces, el reconocimiento de emociones a través de la voz ha sido motivo de interés para la investigación, sin embargo en su gran mayoría, se ha estudiado sobre un mismo lenguaje debatiendo la habilidad de reconocer y clasificar las emociones oralmente expresadas. \\

El presente capítulo está dividido en dos partes, en el primero se hablará de los conceptos inherentes a las características fonéticas en el discurso que es el tema en torno al que gira este trabajo, y por otro lado, cómo se puede orientar ese problema con técnicas de aprendizaje profundo haciendo una revisión de la literatura.



\subsection{Características fonéticas en el habla}
El objetivo del reconocimiento de emociones en el habla es reconocer el trasfondo emocional del mensaje a través de la voz. Esta manifestación sonora posee factores clave para la comunicación humana que ayudan en su interacción sin alterar el contexto del mensaje.\\

La expresión de las emociones están íntimamente relacionadas con las propiedades fonéticas en el habla donde se observan señales y patrones para marcar contrastes lingüísticos en un idioma \citep{Pell2001} por lo tanto, los efectos del lenguaje en la comunicación emocional son evidentes al haber sido observadas y medidas las variaciones en el rango tonal y la frecuencia para expresarlas, cambiando no sólo el tono sino también el patrón lingüístico asociado \citep{Davletcharova2015}.
Por otro lado tanto la proporción de consonantes y vocales (que hacen variar la presión de aire que se necesita), como el ratio de sílabas por palabra en cada idioma, caracterizan la expresión oral de las emociones. Existen muchos factores relacionados con el lenguaje como la morfología o la duración del estímulo que podrían tener un impacto en la decodificación de los matices en la señal vocal, tal y como se explica en \citep{Chen2017}.
Existe una clasificación dependiendo de la velocidad silábica en la expresión de dichos idiomas, sin embargo poco se conoce acerca de los efectos en las medidas respiratorias en el habla \citep{XHuan2001}. Esta observación puede llevar a que se pregunte si en lenguajes que son muy distintos fonéticamente, las emociones expresadas mediante la voz puedan ser reconocidas desde el punto de vista del otro idioma.\\
Normalmente estos estudios se llevan a cabo en un único lenguaje, lo que para este trabajo, se traduciría como el reconocimiento de emociones llevado a cabo en la lengua materna; Mientras este ejercicio puede llegar a ser intuitivo, distinguir las mismas emociones en la lengua extranjera supone un reto ya que implicaría importantes matices culturales. Por ejemplo, no sería lo mismo entender qué emociones intenta expresar un italo parlante desde el punto de vista de una persona que entiende el español (ambas lenguas latinas), que comprender las mismas emociones del discurso desde un germano hablante. Así bien, es importante definir qué idioma se está reconociendo y desde cuál, por lo que analizar las raíces lingüísticas y fonéticas de los idiomas a estudiar es esencial. \hfill \break

\subsection{Reconocimiento emocional del discurso}
Atendiendo a la manera de cómo modelar las emociones de manera que esta información se pueda extraer de señales acústicas \citep{Kumar2019}, se parte de estudios en psicología donde, la clasificación de emociones se ha tratado desde dos enfoques principales:
\begin{itemize}
	\item Las emociones como categorías discretas
	\item Las emociones vistas a través de un modelo dimensional
\end{itemize}

En el primer punto, a todos los humanos se les atribuye un conjunto de emociones básicas que pueden ser reconocidas interculturalmente. El debate se centra en la definición de dichas emociones, y fue Paul Elkman y su equipo en 1992 \citep{Ekman1992} quien estableció que estas eran 6: Enfado, Asco, Miedo , Felicidad, Tristeza y Sorpresa.
En el segundo punto, las emociones se definen respecto a una o más dimensiones, donde normalmente las dimensiones que se comprenden tienden a ser la afectividad, la excitación o la intensidad. Aquí la discusión se centra en encontrar el número de dimensiones que dé lugar a un modelo coherente y pueda incluir las emociones conocidas. El modelo de Plutchik \citep{Plutchik2001} sea quizá el más conocido en este enfoque, y propone un modelo tridimensional que organiza las emociones en círculos concéntricos situando la más básicas en el centro como se representa en la figura \ref{fig:Plutchik}. \\

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Rueda-Plutchik.svg.png} 
	\caption{Rueda de las Emociones de Plutchik. Fuente: \href{https://es.wikipedia.org/wiki/Robert_Plutchik}{Wikipedia}}
	\label{fig:Plutchik}
\end{figure} 

Sin embargo, por cuestiones de simplicidad, la mayoría de los estudios centrados en la clasificación de emociones se centran en las emociones vistas como categorías discretas.



\subsubsection{Extracción de Características}
La extracción de características es una de las secciones más importantes en el reconocimiento de emociones a través de la voz debido a la ambigüedad de las características y la variedad vocal. La extracción de características es el paso principal en el procesamiento del diálogo, y se lleva a cabo para centrarse en la información contenida en la señal y mejorar el grado de similitud y/o diferenciación entre las clases \citep{Hellbernd2016}. Hasta ahora, por lo general hay dos enfoques principales  con respecto al tipo de características usadas en el Reconocimiento de Emociones en el Discurso:\\
Los rasgos prosódicos, los cuales extraen información de la prosodia, en concreto, tono, energía y duración, y por otro lado, las características del tracto vocal que normalmente indican la distribución de la energía en la frecuencia del rango vocal (conocidos como Coeficientes Cepstrales).
La mayoría de los estudios centrados en este tema usan rasgos espectrales como la información extraída del tracto vocal, lo que supone obtener la información derivada del espectro de la señal de la voz y se usan para modelar los patrones de entonación y frecuencia del hablante \citep{Langari2020}.\\

En \citep{Rashid2018} se ofrece una breve explicación de cada una de las técnicas más comunes analizando sus puntos fuertes y débiles: Así pues, se puede encontrar la Transformada Wavelets Discreta (DWT) que a pesar de mejorar la información que se obtiene del diálogo en la correspondiente banda de frecuencia presenta variaciones indeseadas en los límites debido a que las señales de entrada son de una longitud finita. También se encuentran trabajos donde se usen Coeficientes de Predicción Lineal (LPC) \citep{Rana2014} y \citep{Rashmirekha2013} los cuales hacen estimaciones bastante precisas al extraer las propiedades del tracto vocal, pero son altamente sensibles al ruido de cuantificación, por lo que demuestran no ser precisos cuando hay ruido de fondo. \\

No obstante, a lo largo de los últimos años se ha popularizado el uso de otros métodos reportando mejores resultados, estos son:

\paragraph{Coeficientes Cepstrales con Predicción Lineal (LPCC)}
Calcula una envolvente a los Coeficientes de Predicción Lineal (LPC) y luego hace una conversión a coeficientes cepstrales; Esto materializa las características de un canal particular del sonido, teniendo en cuenta que la misma persona con diferentes tonos emocionales tendrá diferentes canales de características, se podrán extraer esos coeficientes para indetificar las emociones contenidas \citep{Sandesara2020}.
Tiene una baja vulnerabilidad al ruido de fondo y mejora el ratio de error en comparación con LPC, pero sigue teniendo una gran sensibilidad al ruido de cuantificación.\hfill \break

\paragraph{Coeficientes Cepstrales en la escala de Mel (MFCC)}
Es la representación compacta del espectro de una señal de audio.
MFCC se basa en la desintegración de la señal para tener como resultado un resumen de las características que la forman. La obtención de este conjunto de valores numéricos se basa por un lado, en el rango de frecuencias de Mel, el cual consiste en una adaptación de frecuencias de la señal a aquellas más fácilmente percibidas por el oído humano, y por lo otro lado, la separación de frecuencias mediante cepstrales (\emph{Cepstrum}, es el resultado de calcular la transformada de Fourier inversa del espectro de la señal estudiada en escala logarítmica \citep{Childers1977}) que divide la señal en dos bandas de frecuencias: baja (correspondientes a los fonemas producidos por el tracto vocal) y alta (correspondientes a la excitación de las cuerdas vocales) \citep{Davis1980} .\\
Debido a esto, encapsula la mayor parte de energía proveniente del sonido que es generado por humanos, por lo que es frecuentemente usada y sugerida para identificar palabras monosilábicas en un discurso \citep{Farouk2014}.\hfill \break

\begin{comment}
	Resumiéndolo, los objetivos clave, serían:
	\begin{itemize}
	\item Eliminar la excitación del tracto vocal.
	\item Independizar las características extraídas.
	\item Ajuste a cómo los humanos percibimos el ruido y la frecuencia del sonido.
	\item Capturar la dinámica fonética, que definirá el contexto.
	\end{itemize}
	MFCC constituye una perfecta representación para el sonido cuando la fuente es estable y consistente. 
\end{comment}

Enumerando los objetivos clave del proceso serían:
\begin{enumerate}
	\item Divide la señal en segmentos de tiempo cortos. Ya que la frecuencia de la señal cambia en la línea temporal, no tendría sentido aplicar la Transformada de Fourier en toda la señal, ya que se perdería parte de la frecuencia a lo largo de ese tiempo produciendo distorsiones.
	
	\item Pasar la señal del dominio de tiempo a dominio de frecuencia. Ya que FFT (\textit{Fast Fourier Transform}) asume que la señal de audio es periódica y continua, al fragmentar la señal se garantiza que es periódica, y para la continuidad se aplica la función de ventana de Hann \citep{Smith2011}. Si no se hiciese este paso se producirían distorsiones en las frecuencias más altas.
	
	\item Aplicación de un filtro de banco para ajustar la señal a la forma en la que los humanos percibimos el sonido y su frecuencia. Concretamente se aplica la escala de Mel, de donde se extraerá la energía en cada banda de frecuencia \citep{Haytham2016}.
	
	\item Finalmente se aplica la Transformada Discreta del Coseno (comúnmente llamada DCT por sus siglas en inglés \textit{Discrete Cosine Transform}) para generar los coeficientes \citep{Syed2003}. Los coeficientes cepstrales de la escala de Mel contienen información sobre los cambios en las diferentes bandas del espectro, así que DCT extraerá esos cambios en las altas y bajas frecuencias de la señal.

\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{MFCCfeatures.PNG}
	\caption{Coeficientes MFCC. Fuente: propia}
	\label{fig:mfccFeat}
\end{figure}

Los valores que se muestran en la figura \ref{fig:mfccFeat} son 10 de los coeficientes resultantes. Cuando un valor es positivo, significa que la mayor parte de la energía espectral está concentrada en las regiones de frecuencia baja. Por el contrario, si el valor es negativo, la mayor parte de la energía espectral está concentrada en frecuencias altas.

Se considera que de 12 a 20 coeficientes cepstrales es una cantidad óptima para el análisis de la voz \citep{post2018}.

\subsubsection{Procesamiento de la señal como imagen}
\label{cap2:spectrograms}

El tipo de dato con el que se trabajará principalmente serán señales, concretamente, de audio. En las señales de audio hay una cierta presión de aire que varía con respecto al tiempo \citep{Koolagudi2012}, y al muestrearlas en un determinado rango de frecuencia, se obtendría algo como lo siguiente:

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.45]{original-waveform.JPG}
	\caption{Forma de onda de una señal de audio. Fuente: propia}
	\label{fig:waveform}
\end{figure}

Los que se muestra en la figura \ref{fig:waveform} es una representación digital de la onda, de manera que ahora puede ser interpretada y analizada fácilmente.\hfill \break
La Transformada de Fourier (FFT \emph{Fast Fourier Transform}) responde a cómo extraer características relevantes de esta representación, ya que permite analizar la cantidad de frecuencia contenida en una señal. Esta transforma la señal de un dominio de tiempo a un dominio de frencuencia (figura \ref{fig:timeFrecuencyView}), y el resultado es el espectro \citep{ntiaudio}. \\
%https://stackoverflow.com/questions/59604595/how-to-extract-features-from-fft

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.7]{FFT-Time-Frequency-View.png}
	\caption{Representación de una señal desde dos planos (frecuencia y tiempo). Fuente: \href{https://commons.wikimedia.org/wiki/File:FFT-Time-Frequency-View.png}{Wikipedia}}
	\label{fig:timeFrecuencyView}
\end{figure}

Sin embargo el problema viene cuando en las señales de audio, la cantidad de frecuencia varía en el tiempo, por lo que FFT es insuficiente al no poder representar en el espectro resultante esta variación de la señal en el tiempo. La Transformada de Fourier en tiempo reducido, (\emph{short-time Fast Fourier Transform}) resuelve este problema calculando la FFT en segmentos (ventanas de tiempo) superpuestos de la señal. Tras aplicar el filtro de banco en la escapa Mel mencionado anteriormente, se obtiene finalmente lo que se denomina \textbf{espectrograma}.
\begin{comment}
	\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{processFFT.png} 
	\caption{Proceso del uso de FFT en una señal. Fuente: \href{https://www.mathworks.com/help/dsp/ref/dsp.stft.html}{Mathworks}}
	\end{figure} 
	
	
	\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{spectogram.png} 
	\caption{Espectrograma de una muestra aleatoria. Fuente: \href{https://es.other.wiki/wiki/Spectrogram}{Wiki}}
	\label{fig:espectrogram}
	\end{figure}
\end{comment}


Este espectrograma (figura \ref{fig:espectrogram}) puede ser entendido como una representación tridimensional de la señal donde sus características (tiempo, frecuencia distribución de energía) pueden ser observadas de manera muy visual. Cuando este espectrograma se computa, el eje X representa el tiempo, el eje Y representa la frecuencia, que es convertida a una escala logarítmica, y la gama de colores que se utiliza es para simbolizar la variación de energía expresada (medida decibelios), donde los tonos más oscuros indican unos valores de energía más altos, y viceversa\citep{Kartik2020}.


\begin{figure}[H]
	\centering
	%\begin{adjustwidth}{-1cm}{}
	\minipage{0.7\textwidth}
	\centering
	\includegraphics[width=\linewidth]{3Dspectogram.png}
	{{\small Representación 3D}}
	\endminipage

	\minipage{0.7\textwidth}
	\centering
	\includegraphics[width=\linewidth]{spectogram.png}
	{{\small Espectrograma resultante}}
	\endminipage
	\caption{Espectrograma de una muestra aleatoria. Fuente: \href{https://www.faberacoustical.com/apps/ios/signalscope_adv_2018/}{faberAcoustical}}
\label{fig:espectrogram}
\end{figure}


La respuesta a por qué las frecuencias son convertidas a una escala logarítmica es sencillamente porque los humanos no percibimos las frecuencias en una escala lineal \citep{Varshney}, es decir, nuestra habilidad para distinguir entre frecuencias fluctúa a lo largo del rango en el que somos capaces de percibir. Es por ello que el rango donde se mueve este espectrograma, se adapta a la \textbf{escala de Mel}, en la cual los armónicos se observan equidistantes, reduciendo como resultado las variantes acústicas que no son significativas \citep{StevensVolkmann}.\\

Finalmente queda entender el concepto de \emph{Cepstrum} o coeficientes cepstrales, y para ello hay que entender como el sonido (respecto a la articulación de palabras) es producido. Técnicamente, esta producción del sonido en la anatomía se definiría como la combinación de las vibraciones producidas por las cuerdas vocales con las vibraciones producidas por la resonancia del tracto vocal. Nuestras articulaciones controlan la forma del tracto vocal, por lo que la forma de onda de la voz será reprimida o amplificada a diferentes frecuencias por la forma de nuestro tracto vocal \citep{Bao2019}.\\
El papel del Cepstrum es la separación de frecuencias en el algoritmo de MFCC, atendiendo a cómo los sonidos son producidos siguiendo un modelo anatómico, de manera que cuando es computado separa la señal de voz y la resonancia del tracto vocal \citep{Nair}. %En \ref{fig:mfcc_process} observamos el paso IDFT, donde tras ajustar la señal a la escala de Mel, se calcula una variante de FFT (\emph{Inverse Discrete Fourier Transform}) y se obtienen los \textbf{coeficientes de MFCC} 

En la figura \ref{fig:cepstralcoef} se pueden observar el resultado de los pasos que se han discutido hasta ahora.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{waveform_process.jpeg} 
	\caption{Visualización de los coeficientes cepstrales de una muestra aleatoria. Fuente: \href{https://www.ee.columbia.edu/~stanchen/spring16/e6870/slides/lecture3.pdf}{Columbia University}}
	\label{fig:cepstralcoef}
\end{figure}

Dado que se ha convertido una señal de audio en una imagen, ahora se podrá proceder a usar un modelo basado en redes convolucionales, aprovechando sus ventajas en el campo donde mejor rendimiento reporta: el procesamiento de imágenes.
% ============================================================================
% REF: https://medium.com/@tanveer9812/mfccs-made-easy-7ef383006040
%	   https://www.kaggle.com/ilyamich/mfcc-implementation-and-tutorial
%https://www.researchgate.net/figure/Block-diagram-for-Mel-Frequency-Cepstral-Coefficient-MFCC-feature-extraction_fig1_260165559
% ============================================================================
\subsubsection{Algoritmos de Clasificación}

Convencionalmente, el estudio del Reconocimiento de Emociones en el Habla incluye el uso de diferentes tipos de clasificadores entre las que destacan las Máquinas de Vector de Soporte (SVNs \emph{Suport Vector Machines}) ya que se han usado extensamente para el reconocimiento de emociones y pueden llegar a presentar un buen rendimiento en comparación con otros clasificadores tradicionales \citep{Africa2020} y \citep{Jain2020}.

No obstante, en estudios más recientes, se han propuesto clasificadores basados en aprendizaje profundo, los cuales han superado a los enfoques tradicionales resultando ser más eficientes además de tener la capacidad de aprender las características emocionales en el reconocimiento de emociones a través del audio\citep{Sandesara2020}.

El aprendizaje profundo es un conjunto de algoritmos de aprendizaje automático que modela abstracciones de alto nivel construyendo conceptos complejos a partir de otros más sencillos mediante el uso de una arquitectura jerárquica.
Esta arquitectura jerárquica es lo que se denomina redes neuronales que son estructuras lógicas cuyo diseño se basa en mayor medida en la organización del sistema nervioso de los mamíferos \citep{Matthew2019}.
Las neuronas artificiales que la componen son unidades de proceso especializadas en detectar determinadas características de aquello que es percibido \citep{Nielsen2015}.


\begin{figure}[!htb]
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=.8\textwidth]{neurona.JPG}
		\caption{Representación de una neurona real. Fuente: \href{https://cebebelgica.es/es_ES/blog/10/que-es-una-red-neuronal-artificial.html}{CeBe}}
		\label{ref:imgNeuron}
	\end{minipage}\hfill
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{neuronaArtificial.JPG}\hfill
		\caption{Representación de una neurona artificial. Fuente: \href{https://futurelab.mx/redes\%20neuronales/inteligencia\%20artificial/2019/06/25/intro-a-redes-neuronales-pt-1/}{futureLab}}
		\label{ref:imgArtNeuron}
	\end{minipage}
\end{figure}

La imagen \ref{ref:imgArtNeuron} muestra el funcionamiento de estas neuronas artificiales claramente inspirado en el diseño de la figura \ref{ref:imgNeuron}. Al nodo le llegan unas entradas $x_i$ que tienen asignadas unos pesos $w_i$ y unos sesgos $b_i$ (\emph{biases}), que son procesadas por la función de activación. El proceso de aprendizaje se consigue por transferencia de información de unas capas a otras gracias al algoritmo de retropropagación (\emph{backpropagation}) cuyo objetivo es encontrar la distribución adecuada a cada una de las variables de entrada \citep{Goodfellow-et-al-2016}.\\


\paragraph{Redes Neuronales Recurrentes}\hfill \break
Las Redes Neuronales Recurrentes (RNN) son convenientes en tareas en las que los datos son procesados secuencialmente. Esto puede ser especialmente una ventaja ya que las distintas entradas de la señal no se tratan de manera independiente \citep{Lim2017}. En \citep{Lee2015} se propone un sistema basado en RNN aprovechando esta particularidad donde cada nodo toma en cuenta la información recogida en los anteriores; Esto hace que cubra un espectro más amplio de información creando una especie de contexto.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{rnn_structure.JPG} 
	\caption{Estructura de RNN. Fuente:  \href{http://personal.cimat.mx:8181/~mrivera/cursos/aprendizaje_profundo/RNN_LTSM/introduccion_rnn.html}{Cimat}}
	\label{fig:rnn_structure}
\end{figure}

La figura \ref{fig:rnn_structure} muestra la estructura básica de una Red Neuronal Recurrente donde se puede observar como la red actualiza el peso de las entradas a través del algoritmo Descenso de Gradientes. En algunos casos, estos gradientes se irán haciendo más pequeños a medida que la red avanza, evitando así que los pesos cambien su valor y por lo tanto, la red siga aprendiendo (lo que se conoce como desvanecimiento de gradientes). \\

\paragraph{Redes LSTM}\hfill \break
Como se puede ver, los trabajos anteriormente mencionados que implementan este tipo de arquitectura RNN son relativamente antiguos (2017, y 2015 respectivamente) y en estudios más recientes , a destacar \citep{Wang2020} y \citep{Atmaja2019}, los retos que presenta la clasificación de emociones en el habla, son comúnmente abordados a través de una red de Memoria a Largo Corto-Plazo (LSTM) la cual es capaz de retener información de entradas anteriores en el tiempo y tener en cuenta dependencias temporales largas, ya que cada nodo es una célula de memoria. Esto a su vez, resuelve el problema de desvanecimiento de gradientes que presenta RNN.\\
Las redes LSTM son un tipo de red recurrente que fueron diseñadas para resolver el problema de la dependencia a largo plazo del que sufre RNN. 
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{LSTM3-chain.png}
	\caption{Estructura de una red LSTM. Fuente: \href{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}{Colah}}
	\label{fig:netLSTM} 
\end{figure}

Como muestra la figura \ref{fig:netLSTM}, estas presentan una estructura en cadena al igual que las redes recurrentes, pero el módulo de repetición en lugar de tener una única capa de red neuronal, tiene cuatro que interactúan.\\

\paragraph{Redes Neuronales Convolucionales}\hfill \break
%https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53
Uno de los mayores avances de los últimos años en el campo de la inteligencia artificial son las redes convolucionales, debido a la alta precisión que proporcionan en el procesamiento de imágenes. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{cnnDiagram.JPG}
	\caption{Estructura de una red CNN. Fuente: \href{https://brilliant.org/wiki/convolutional-neural-network/}{Brilliant}}
	\label{fig:netCNN} 
\end{figure}

La tendencia de los modelos basados en redes neuronales densas en este ámbito, es aprender características específicas desde varios métodos usados en el reconocimiento de emociones a través de la percepción acústica. En el uso de las Redes Neuronales Convolucionales (CNN) la idea principal es tomar ventaja de las propiedades de la señal, como la conectividad local, los pesos compartidos y el uso de varias capas  \citep{Lim2017}. Estas suponen una importante contribución en la Clasificación Emocional de la Voz debido al uso de características significativas, y su uso en recientes estudios se ha incrementado a lo largo de los años donde destacan los trabajos de \citep{AbdulQayyum2019} y \citep{Anvarjon2020}.  %\citep{Farooq2020}.

\subsubsection{Bases de datos}
\label{lb:cap_baseDatos}
Aquí se recogen las bases de datos sobre el reconocimiento de emociones más usadas en los estudios comentados anteriormente.

	\begin{itemize}
		\item SAVEE (por sus siglas en inglés, \emph{ Surrey Audio-Visual Expressed Emotion}) es un conjunto de datos aplicado al reconocimiento de emociones que consiste en grabaciones de 480 frases en total en inglés británico ejecutadas por cuatro actores profesionales masculinos modulando siete emociones distintas (Enfado, Asco, Tristeza, Alegría, Miedo, Sorpresa y Neutral). El estudio \citep{SAVEEdataset} explora y detalla esta base de datos que data del 2011, y es de acceso público para propósitos de investigación.
		
		\item IEMOCAP (por sus siglas en inglés, \emph{Interactive Emotional Dyadic Motion Capture}) es una base de datos multimodal privada utilizada para el reconocimiento y análisis de emociones. Consiste en doce horas de contenido audiovisual donde diez actores (cinco hombres y cinco mujeres) mantienen diálogos en inglés previamente transcritos en los que se interpretan cinco emociones distintas (Enfado, Tristeza, Alegría, Frustración y Neutral).
		
		\item RAVDESS (por sus siglas en inglés, \emph{Ryerson Audio-Visual Database of Emotional Speech and Song}) es un popular conjunto dinámico multimodal (contiene varios formatos) donde veinticuatro actores profesionales vocalizan frases en inglés norteamericano modulando ocho emociones (Enfado, Calma, Asco, Tristeza, Alegría, Miedo, Sorpresa y Neutral). Tiene un total de 7356 grabaciones entre audio (hablado), audio(canciones) y vídeo. Cuenta con una completa documentacion \citep{Livingstone2018} y es de acceso público.
		
		\item TESS (por sus siglas en inglés, \emph{Toronto Emotional Speech Set data}) es un conjunto de datos compuesto por 2800 archivos de audio donde dos actrices de 26 y 64 años cuya lengua materna es el inglés americano, articulan 200 frases cada una modulándolas en siete emociones (Enfado, Asco, Tristeza, Alegría, Miedo, Sorpresa, y Neutral). Este proyecto fue realizado por la universidad de Toronto \citep{E8H2MF-2020} y es de acceso público.
		
		\item EMO-DB es una base de datos alemana cuyos detalles se recogen en \citep{emodb2005}, data del 2005 y es de acceso público. La conforma una colección de 800 grabaciones interpretadas por diez actores (cinco hombres y cinco mujeres) matizando seis emociones (Enfado, Asco, Tristeza, Alegría, Miedo y Neutral) y son llevadas a cabo en una cámara anecoica (capaz de absorber las ondas sonoras o electromagnéticas sin reflejarlas).
		
		\item CaFE (por sus siglas en inglés \emph{Canadian French Emotional Speech Dataset}) es una base de datos canadiense en idioma francés donde seis hombres y seis mujeres, pronuncian un total de seis frases interpretando siete emociones  (Enfado, Asco, Tristeza, Alegría, Miedo, Sorpresa y Neutral). Es de acceso libre y fue introducida en \citep{Gournay2018}
		
	\end{itemize}

\section{Estado del Arte}
\label{lb_estado_arte}
Aunque la mayoría de estudios se basan en análisis que evalúan la precisión del clasificador sobre la propia lengua, el auge de las técnicas basadas en aprendizaje profundo ha permitido aumentar la capacidad de clasificación en los modelos de reconocimiento de emociones. La creación de un mapa de características a partir de diferentes representaciones de la onda sonora con respecto a su frecuencia y tiempo, permiten a las redes neuronales distinguir más atributos para hacer una predicción más exacta. A partir de este paradigma, el intento de clasificar emociones a través de la voz ha sido objeto de estudio aplicando diferentes técnicas.

En \citep{Atmaja2019} proponen sistema basado en una arquitectura LSTM bidireccional (BLSTM) que aplican a un subconjunto de la base de datos IEMOCAP para distinguir entre cuatro emociones(Enfado, excitación, Tristeza y Neutral). Discuten la incapacidad del un modelo BLSTM para detectar características relevantes, y palían el problema añadiendo un modelo de atención. Teniendo ese modelo como punto de partida (BLSTM + modelo de atención), experimentan escogiendo diferentes valores de duración del silencio para medir la eficacia que tendría el modelo si este se eliminara. Además utilizan un complejo sistema de extracción de características entre las que destacan MFCC. Los resultados muestran un máximo del 70.34\% de precisión en los datos sin necesidad de eliminar el silencio de la señal de audio.
Un año después, J.Wang \citep{Wang2020} propone un modelo dual LSTM donde cada uterancia, se procesa con características MFCC y espectrogramas de MEL simultáneamente. El modelo es entrenado y evaluado en el conjunto de datos IEMOCAP llegando a un 72.7\% de exactitud.

Las redes LSTM cubren en mencionado antes, efecto de contexto, resolviendo el desvanecimiento de gradientes que que se puede encontrar en las redes recurrentes, pero de manera aislada no son las que mejores resultados ofrecen y es por ello, que en otros trabajos se han combinado con CNN para aumentar su rendimiento. Por ejemplo en \citep{Lim2017} se lleva a cabo una comparación de tres  arquitecturas (CNN, LSTM y CNN distribuida en el tiempo) donde LSTM (utilizada de manera aislada) es la que puntúa más bajo. Al mismo tiempo, W.Lim y su equipo estudian el resultado de un sistema híbrido que usa una red convolucional distribuida en el tiempo (una combinación de CNN y LSTM) para clasificar emociones en una secuencia de audio, consiguiendo un 88.01\% de precisión. De nuevo, para aprovechar las ventajas que ofrecen las redes convolucionales, la señal es convertida a imagen (espectrograma), que es entrenada y probada con el corpus EMO-DB (alemán) distinguiendo entre 7 emociones.

Por otro lado, en \citep{Harar2017} describe un método que utiliza una arquitectura basada en Redes Convolucionales sin selección de características para distinguir únicamente entre tres emociones (Enfado, Neutral, y Tristeza) a través de la voz. Su objetivo es predecir el estado emocional de una persona en una grabación corta de audio donde la mencionada arquitectura consiste en 6 capas convolucionales y 3 densas. Como conjunto de datos utilizan la Base de Datos de Berlín de Discurso Emocional (EMO-DB), un corpus alemán que contiene un total de 800 frases (10 frases distintas re-interpretadas en 7 emociones por 5 mujeres y 5 hombres), del cual extraen un subconjunto de 271 grabaciones etiquetadas. De las señales de audio con las que el sistema es alimentado, se han eliminado los segmentos de silencio después de ser estandarizadas consiguiendo una exactitud de 96.97\%.

Con el fin de eliminar el preprocesado de la señal, en \citep{AbdulQayyum2019}  presenta un modelo de redes convolucionales para una clasificación de emociones en el idioma inglés. Este utiliza la base de datos SAVEE, la cual contiene 480 muestras que distinguen entre 6 emociones, interpretadas por hombres y mujeres angloparlantes, donde obtiene finalmente un 81.63\% de precisión. Este trabajo llega a sus resultados mediante la comparación de 3 enfoques (MVR, SVN, y RNN) en el que a cada uno le aplican tres métodos de extracción de características distintos, con el sistema propuesto basado en CNN sin ningún procesado de la señal, siendo este último el que consigue una mejor capacidad de predicción.

En estudios más recientes, \citep{Anvarjon2020} aborda el problema del Reconocimiento de Emociones en el Habla con una red CNN computacionalmente eficiente que es alimentada con los espectrogramas de la señal; es decir, se consigue una representación en 2D de la señal de audio aprovechando mejor las ventajas de una red de este tipo. 
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{anvarjon2020_emodb.JPG} 
	\caption{Resultados de T.Anvarjon sobre la base de datos EMO-DB. Fuente: \citep{Anvarjon2020}}
	\label{fig:anvarjon_emo_plot}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{anvarjon2020_imeocap.JPG} 
	\caption{Resultados de T.Anvarjon sobre la base de datos IEMOCAP. Fuente: \citep{Anvarjon2020}}
	\label{fig:anvarjon_ime_plot}
\end{figure}

El sistema es probado en con dos datasets distintos independientemente, IEMOCAP (en inglés, y eliminando 'frustración') y EMO-DB (alemán) consiguiendo un 77.01\% y 92.02\%  de precisión respectivamente. Las figuras \ref{fig:anvarjon_emo_plot} y \ref{fig:anvarjon_ime_plot} muestran el rendimiento del modelo en los conjuntos de datos correspondientes comprobando el sólido resultado.

Siguiendo por el enfoque de CNN, en \citep{Mustaqeem2020} exploran una arquitectura basada en CNN compuesta por 7 capas bidimensionales. Paralelamente la red es alimentada con espectrogramas y características extraídas a través de MFCC, y lleva a cabo una clasificación de 5 emociones evaluando el resultado en RAVDESS donde consiguen un 81.0\% de precisión e IEMOCAP con un 84.00\%

% TODO: quiza quieres revisar esa verborrea
Finalmente, es obligatorio hablar del trabajo de \citep{Tamulevicius2020}, en una línea más cercana al objetivo de este proyecto, lleva a cabo un estudio del reconocimiento emocional empleando el cruce de 6 lenguas (lituano, inglés, serbio, español, alemán y polaco). Aunque sus resultados no son realmente señalables y acaban diseñando un clasificador que es entrenado con todas las lenguas para distinguir las emociones indistintamente, dicha clasificación se lleva a cabo mediante el uso de una red neuronal convolucional bidimensional de 3 capas, e insisten en la importancia del uso de características en dos dimensiones, ya que proveen información temporal además de las características acústicas de las emociones. En el estudio exploran varias de estas características, siendo el uso de cocleogramas las que consiguen una mayor exactitud.


Bajo estas líneas, la tabla resume los estudios previamente comentados en la sección \ref{lb_estado_arte}.\\

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c | c | c | c| c | c|}
			\hline
			\textbf{Trabajo} & \textbf{Año} & \textbf{Método} &  \textbf{Datos usados}  & \textbf{Acierto} \\ 
			\hline\hline
			J.Wang & 2020 & LSTM dual + MFCC & IEMOCAP & 72.7\% \\
			Atmaja & 2019 &   BLSTM + Att  & IEMOCAP (4 emociones) & 73.34\% 	\\
			Anvarjon & 2020 & CNN 2D + espectrogramas Mel & IEMOCAP (4 emociones) & 77.01\%\\
			Mustaqeem & 2020 & CNN 2D + MFCC & RAVDESS (5 emociones) & 81.01\% \\
			Abdul & 2019 & CNN & SAVEE & 81.63\%				\\
			Mustaqeem & 2020 & CNN 2D + MFCC & IEMOCAP (5 emociones) & 84.00\% \\
			W.Lim & 2017 &   LSTM + CNN & EMO-DB &  88.01\%		\\ 
			Anvarjon & 2020 & CNN 2D + espectrogramas Mel & EMOD-DB & 92.01\%\\  
			Harar & 2017 &  CNN & EMOD-DB (3 emociones) &  96.97\%			\\
			Tamulevicius & 2020 & CNN 2D + cocleogramas & Lithuanian & 97.00\% \\
			%& & & &
			\hline	
		\end{tabular}
		
		\caption{Tabla comparativa y resumida de los trabajos mencionados}
		\label{tab:metod_comp}
	\end{center}
\end{table}

En la tabla \ref{tab:metod_comp} se muestra un resumen de los estudios relacionados con este campo y sus correspondientes resultados (ordenados de manera ascendente por porcentaje de acierto), así como los métodos y datos que se han usado para ello. La columna de Método, comprende la arquitectura completa, es decir, el sistema de clasificación usado y el método de extracción de características si lo hubiera. En la tercera columna se describen los datos usados (la base de datos) y el número de clases entre las que el sistema ha tenido que diferenciar. Cuando estas no se especifican, quiere decir que se ha usado el conjunto de datos al completo.
% WARNING No puedo acceder a la base de datos de Lithuanian para referenciarlo
En el último trabajo, \citep{Tamulevicius2020} se lleva a cabo la comparación de distintos datos, pero es \emph{Lithuanian} la base de datos lituana en la que se basan para crear el modelo, cuya arquitectura es la que se especifica en la tabla.\\

\section{Conclusiones parciales}
En esta sección se valorarán las conclusiones que se extraigan del análisis previo sobre los distintas etapas correspondientes al desarrollo de un modelo para la clasificación de emociones en el habla. Se observa que los retos que plantea SER se han abordado anteriormente desde distintos enfoques, pero en su mayoría, desde el punto de vista de un único lenguaje. Las dificultades que presenta esta tarea en la lengua extranjera se deben principalmente a las posibles variaciones de aire para expresar la mismas emociones. Este mismo problema se plantea en una modalidad diferente pero bastante relacionada como es la transcripción de la voz a texto (ASR, Reconocimiento Automático del Discurso), sin embargo estos estudios requieren un análisis más profundo de la fonética propia de cada lenguaje.

Los trabajos de Pell se han centrado durante años en el análisis de la prosodia a través de los idiomas. A pesar de su antigüedad y que no entra demasiado en detalles técnicos, merece la pena mencionar que en \citep{Pell2008} lleva a cabo un estudio comparativo entre la detección emocional de la prosodia en la lengua materna y la extranjera, concluyendo que el proceso para entender las emociones vocales en una lengua no aprendida, implica una mayor exposición a esta para familiarizarse con señales prosódicas correspondientes a significados subyacentes.\hfill \break


Cabe destacar que la combinación de métodos, véase algoritmos de clasificación, filtros para preprocesar la señal, y métodos de extracción de características, así como distintos conjuntos de datos, es realmente diversa, por lo que visualizar una dirección clara para determinar qué línea es la mejor, se diluye. 

No obstante hay observaciones, que pueden llevar a una conclusión general; Por ejemplo, y de manera intuitiva, cuanto mayor es el número de clases (emociones en nuestro caso), peor será la capacidad de clasificación de la red, y por eso en algunos trabajos se extrae un subconjunto reduciendo las opciones entre las que clasificar.  

En la extracción de características el uso de MFCC ha sido amplia y tradicionalmente escogido al reportar resultados más elevados en comparación con otros métodos. En \citep{Langari2020} denota que los métodos de extracción de características son MFCC y LPCC porque las variaciones en la frecuencia del tono están significativamente relacionados con la expresión humana de emociones. Los parámetros que se computan en MFCC como el número de filtros o la escala de frecuencia, son a menudo escogidos de manera experimental y dependen en gran medida del conjunto de datos con el que se pruebe y el clasificador que implemente el sistema.

En cuanto a los métodos usados como clasificadores, se observa también que CNN (con diferentes modificaciones dependiendo del estudio) es la opción más sólida entre los trabajos más recientes, debido principalmente a que reduce la señal de audio a sus características más relevantes, y la combinación de probabilidades resultantes identifica conjuntos de atributos que determinan una clasificación. El uso de espectrogramas, aprovecha las bondades que las redes convolucionales ofrecen.
%https://blog.soprasteria.se/2019/10/07/using-cnn-for-speech-emotion-recognition/


\chapter{Objetivos y metodología de trabajo}

\section{Objetivo General}

\begin{enumerate}
	%\item Conseguir un modelo que realice una clasificación emocional en el idioma de referencia escogido con un porcentaje de acierto por encima de un 80\%.
	\item Hacer un estudio comparativo del reconocimiento de emociones por voz, a través de lenguajes no aprendidos (lenguajes que no hayan formado parte del entrenamiento), una vez se haya conseguido un modelo capaz de clasificar en una lengua conocida con un porcentaje de acierto superior al 81\%.
\end{enumerate} 

\section{Objetivos específicos}
Para conseguir el alcance establecido, es necesario que los siguientes puntos sean satisfechos:
\begin{itemize}
	\item Hacer un estudio del estado del arte sobre diferentes métodos, técnicas, y conjunto de datos utilizados en el reconocimiento de emociones a través de la voz. Aquí también se explorará si se dispone de la documentación necesaria, cómo cada uno de esos métodos pudiera estar relacionado con la lengua que usa para aplicarlo y su fonética.
	
	\item  Conseguir al menos 3 datasets pertenecientes a 3 idiomas diferentes donde uno de ellos será usado como referencia, y además, deberán cumplir las siguientes condiciones: Uno de los conjuntos de datos restantes deberá tener raíces fonéticas distintas al corpus de referencia, y el otro tener raíces fonéticas similares.
	
	\item Diseñar una solución en la que el conjunto de datos de referencia tenga un porcentaje de acierto superior al 81\% en la clasificación de emociones. Esta referencia ha sido marcada por los resultados reportados en la revisión del estado del arte.
	
	\item Aplicar el modelo diseñado en el paso anterior a los otros conjuntos de datos.
	
	\item Evaluar la tasa de acierto obtenida en cada uno de eso conjuntos y comparar los resultados obtenidos.
	
\end{itemize}
\section{Metodología de trabajo}
Para este proyecto se plantea una metodología de desarrollo iterativa (ver figura \ref{fig:iterative_process}), en las que tras una fase inicial, el proyecto entra en un bucle donde el trabajo pasa por una serie de etapas que se repiten durante la vida del proyecto. 


Al contrario que en desarrollos de software más tradicionales donde podrían verse flujos de trabajo basados en metodologías ágiles o en cascada, se ha considerado que este modelo se adapta mejor a las necesidades de este proyecto, debido principalmente, al grado de incertidumbre que presenta un proyecto basado en Inteligencia Artificial en comparación con la ingeniería del software estándar. Por ejemplo, una metodología ágil asume que pequeños cambios funcionales hace posible el alcance de los objetivos a bajo coste y alta predictibilidad. Esto no se corresponde con este tipo de trabajo por las siguientes características:

\begin{itemize}
	\item Es difícil conocer los costes y riesgos de la mayoría de los requisitos. Por ejemplo el estudio del conjunto de datos es algo que afecta directamente a la elaboración de los distintos modelos, y por lo tanto el crecimiento funcional es indeterminado.  
	
	\item Los cambios o modificaciones no pueden ser aplicados por diseño, requieren experimentos, además esas modificaciones son realmente complicadas de atomizar, por lo que el coste es impredecible. 
	
	\item Si bien es difícil tener una conclusión final por las estrictas fechas de entrega, un modelo iterativo permite obtener datos presentables a lo largo del proyecto.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.15]{Iterative_Process_Diagram.png}
	\caption{Proceso iterativo propuesto Fuente: \href{https://en.wikipedia.org/wiki/Iterative_and_incremental_development}{Wikipedia}}
	\label{fig:iterative_process}
\end{figure}

Dentro de la metodología propuesta, en cada iteración se diseñan unas modificaciones y capacidades funcionales que son añadidas en función de la etapa anterior.\\
Este tipo de metodología es normalmente adoptada en desarrollo de producto \citep{Larman2003}, pero para esta comparación se ha modificado mediante la extracción de unos pasos iniciales del bucle que la caracteriza, adaptándola mejor a nuestras necesidades.

\begin{enumerate}
	\item Fase inicial: 
	\begin{enumerate}
		\item Revisión de la literatura sobre el reconocimiento de emociones en el habla, así como los métodos usados y los resultados obtenidos. Este paso permite una mayor comprensión del problema, y su alcance.
		
		\item Análisis y recolección de posibles conjuntos de datos en diferentes idiomas, aptos para los experimentos que se quieren realizar.
		
	\end{enumerate}
	
	\item Elaboración: Se llevan a cabo los experimentos y la implementación de los componentes mediante unos pasos iterativos, a saber:
	
	\begin{enumerate}
		
		\item Identificación y redacción de una serie de pruebas iniciales con los diferentes métodos y técnicas de la revisión de la literatura, aplicados según el análisis de las bases de datos.
		
		\item Implementación en Python de las pruebas diseñadas con las técnicas y arquitecturas identificadas.
		
		\item Ajuste de los parámetros así como el balance de los datos con el fin de conseguir un mejor resultado.
		
		\item Evaluación: Se evalúan los resultados obtenidos de la implementación antes de decidir la iteración por finalizada. En caso de que los resultados no sean los esperados hay dos posibilidades: Si los errores obtenidos no distan demasiado de las especificaciones parciales, se pueden realizar reajustes en el modelo. Si por el contrario, dichos resultados están demasiado lejos del objetivo, se creará una segunda versión con una estrategia diferente, añadiendo más iteraciones al proceso de elaboración.
	\end{enumerate} 
	
	\item Evaluación y comparación de los resultados.
	
\end{enumerate}



\chapter{Planteamiento de la comparativa}

En este capítulo se identificará el problema en concreto a tratar, a la vez que el diseño de los experimentos para acometerlo. Para ello se exponen los datos utilizados junto con las técnicas de procesamiento y el diseño de las redes neuronales que se usan en este trabajo.

El objetivo de esta comparativa es contrastar los resultados obtenidos tras aplicar el mismo sistema de reconocimiento de emociones en la voz entrenado con un lenguaje de referencia, con los otros dos lenguajes escogidos. Mediante esta comparativa se pretende responder a la pregunta si es posible reconocer emociones en un idioma que en principio se desconoce.

\section{Conjunto de Datos}
\label{lb_c4_datos}
Los datos en un proyecto de inteligencia artificial son clave de cara a la obtención de un resultado coherente en nuestro trabajo. Este estudio pretende analizar si es posible clasificar emociones en la lengua extranjera y para encontrar una respuesta, se seguirá la estrategia que se presenta a continuación respecto a los datos.
Es importante mencionar, que por cuestiones de coherencia entre las bases de datos (garantizar que todas las bases de datos con las que se va a trabajar comparten exactamente las mismas emociones), se extraerán de los conjuntos originales, seis emociones para clasificar en este trabajo: Enfado, Asco, Tristeza, Miedo, Felicidad y Neutral. En las figuras correspondientes a cada conjunto se ha marcado en un color más claro la emoción que se suprime.

\subsection{Idioma de referencia: Inglés} El idioma de referencia será el  que aprenda el modelo, y desde el cual se intenten reconocer emociones en otras lenguas, es decir, el idioma que se use en el entrenamiento. En este caso se propone el inglés.\\
Las bases de datos a las que se ha tenido acceso son limitadas, y no presentan un gran número de muestras, por lo que en previsión de un rendimiento pobre en el modelo, se decidió escoger varias del mismo idioma de cara a un entrenamiento más completo.

\paragraph{RAVDESS}
Como ya se hizo ver en la sección \ref{lb:cap_baseDatos}, RAVDESS (por sus siglas en inglés \emph{Ryerson Audio-Visual Database of Emotional Speech and Song}), contiene 7356 archivos en total, entre los cuales se pueden encontrar tres modalidades: sólo audio (en 16 bit, 48 kHz y en formato wav), audio-video (720p H.264, AAC 48kHz, en formato mp4) y sólo video sin sonido. Esta base de datos contiene veinticuatro actores profesionales vocalizando dos frases en inglés norte americano (\emph{Kids are talking by the door} y \emph{Dogs are sitting by the door}).

Cada uno de estos archivos están nombrados de manera única mediante siete números a modo de descripción de las características del audio. Éste respeta la siguiente convención:
\begin{itemize}
	\item Modalidad (01 Audio y vídeo, 02 Sólo vídeo, 03 Sólo audio).
	\item Canal vocal (01 discurso normal, 02 canción).
	\item Emoción que representa.
	\item Intensidad Emocional Si es normal o fuerte. La voz Neutral no contempla la intensidad fuerte.
	\item Repetición (si es la primera repetición 01, si es la segunda 02).
	\item Actor que ejecuta la acción.
\end{itemize}

Así por ejemplo, el archivo 03-01-03-01-01-01-01.wav dirá que es un archivo de sólo audio (03), donde se vocaliza una frase de manera hablada (01) y con tono alegre (03). La intensidad es normal (01), corresponde a la primera repetición (01) y el actor que la ejecuta es el n.01.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{ravdess_distribucion.png} 
	\caption{Distribución de las emociones en RAVDESS en la modalidad sólo audio (1440 archivos)}
	\label{fig:emociones_ravdess}
\end{figure}

A pesar de que este dataset hay 7356 muestras para este proyecto únicamente se usarán aquellas que presentan una modalidad de sólo audio, lo que deja un total de 1056 muestras (tras eliminar Sorpresa y Calma), sin embargo como se puede apreciar en la figura \ref{fig:emociones_ravdess} las emociones están bien distribuidas.

\paragraph{SAVEE} (por sus siglas en inglés, \emph{ Surrey Audio-Visual Expressed Emotion}), cuenta con 480 archivos de audio en formato wav muestreados a 44.1 kHz, donde cuatro actores anglosajones (inglés británico) de 27 a 31 años, modulan siete emociones con frases específicas a cada una.
Cada archivo ha sido etiquetado de manera que el primer carácter (o caracteres, antes de un dígito) corresponde a la emoción que representa. Así las letras 'a', 'd', 'f', 'h', 'n', 'sa' y 'su', corresponden a Enfado (\emph{angry}), Asco (\emph{disgust}), Miedo (\emph{fear}), Felicidad (\emph{happiness}), Neutral (\emph{neutral}), Tristeza (\emph{sadness}), Sorpresa (\emph{surprise}). El número que le sigue a continuación, se refiere a la frase pronunciada. Por ejemplo, el archivo d03.wav hace referencia a la tercera frase de la emoción Asco.
%QUESTION Deberia incluir algunas de las frases?

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{savee_distribucion.png} 
	\caption{Distribución de las emociones en SAVEE}
	\label{fig:emociones_savee}
\end{figure}

A pesar de que sólo presenta 480 audios, el hecho de que se use una frase distinta para cada emoción, lo convierte en un conjunto de datos muy completo para el entrenamiento. La figura \ref{fig:emociones_savee} muestra la distribución de las emociones, observando que están bien balanceadas.

\paragraph{TESS}
Este dataset lo conforman 2800 archivos de audio en formato wav donde dos actrices angloparlantes de 26 y 64 años vocalizan 200 palabras insertadas en la frase \emph{Say the word\_\_\_} donde se interpretan siete emociones a Enfado, Asco, Miedo, Felicidad, Neutral, Tristeza y Sorpresa. Los archivos están organizados en carpetas atendiendo a la actriz y a la emoción que representa, y nombrados mediante 3 cadenas de caracteres separadas por un guión bajo donde la primera indica la actriz, la segunda la palabra que se pronuncia, y el tercero la emoción.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{tess_distribucion.png} 
	\caption{Distribución de las emociones en TESS}
	\label{fig:emociones_tess}
\end{figure}

En la figura \ref{fig:emociones_tess} se puede apreciar la distribución de las emociones en la base de datos TESS.



\subsection{Idioma con raíces fonéticas similares: Alemán} Este conjunto de datos pertenecerá a un idioma con unas raíces similares al idioma de referencia, de manera que se espera al menos haya un porcentaje mayor de reconocimiento que en el segundo conjunto de datos de validación. Para este caso, se propone el alemán ya que el idioma de referencia (inglés) es una lengua germánica occidental \citep{Fennell2001}.\\

Para este caso el conjunto de datos propuesto es la Base de Datos del Discurso Emocional de Berlín (EMODB, por sus siglas en inglés \emph{Berlin Database of Emotional Speech}). Este corpus contiene 800 grabaciones interpretadas por diez actores (cinco hombres y cinco mujeres) modulando siete emociones en el idioma alemán. Cada archivo tiene una frecuencia de muestreo de 16 kHz con una resolución de 16 bits, y una duración de 3 segundos de media. Como en el anterior, se utiliza una nomenclatura para nombrar a los archivos que satisface lo siguiente:
\begin{itemize}
	\item Las dos primeras posiciones determinan el actor que las interpreta.
	\item De la posición 3 a las 5 se define el texto que se pronuncia
	\item La posición 6 indica la emoción.
	\item Versión del audio en caso de que la hubiese (codificado con letras).
\end{itemize}

Como ejemplo, el archivo \emph{03a01Fa.wav} indica que el actor 03 (hombre de 31 años) cita el texto a01 (\emph{Der Lappen liegt auf dem Eisschrank}, en alemán "El mantel está colgando del frigo"), con la emoción F (Felicidad), y es la versión \emph{a} (la primera).

La documentación del corpus también nos ofrece información sobre el género y edad de los actores, lo cual se ha determinado irrelevante, y las distintas frases que pueden aparecer en los archivos.
Las emociones que clasifica son Enfado (W), Aburrimiento (L), Asco (E), Miedo o Ansiedad (A), Felicidad (F), Tristeza (T) y Neutral (N) codificadas en el nombre del archivo por su inicial en alemán (especificada entre paréntesis).

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{distribucion_emo.JPG} 
	\caption{Distribución de las emociones en EMO-DB}
	\label{fig:emociones_emodb}
\end{figure}

En la figura \ref{fig:emociones_emodb} se puede apreciar la distribución de las clases en EMO-DB que como puede verse, están muy desajustadas.

\subsection{Idioma con raíces fonéticas distintas: Francés} Este conjunto de datos pertenecerá a un idioma con unas raíces más distantes al idioma de referencia.\\
Con este conjunto se espera que haya una diferencia notable con respecto al idioma con raíces fonéticas similares, reportando un menor porcentaje de reconocimiento. Para este caso, se propone el idioma francés (que es una lengua romance), utilizando para ello el conjunto de datos en francés canadiense para el reconocimiento de emociones, CaFE, el cual contiene seis frases diferentes pronunciadas por seis hombres y seis mujeres en seis emociones básicas además de Neutral, grabadas cada una en dos intensidades distintas. Este conjunto fue grabado a alta resolución en formato aiff con una frecuencia de muestreo de 192 kHz  y 24 bits por muestra.
Los archivos están organizados en una jerarquía de carpetas, de manera que no es necesario escanear el archivo para saber el genero del actor, la intensidad, o la emoción representada en el audio, sino que basta irse a la carpeta correspondiente.
Las emociones que se incluyen aquí son Enfado (\emph{colere}), Asco (\emph{degout}), Alegría (\emph{joie}), Tristeza(\emph{tristesse}), Miedo (\emph{peur}), Sorpresa (\emph{surprise}) y Neutral (\emph{neutre}). En la figura \ref{fig:emociones_cafe}, se ve la distribución original de las mismas.


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{cafe_distribucion.png} 
	\caption{Distribución de las emociones en CaFE}
	\label{fig:emociones_cafe}
\end{figure}



\section{Extracción de características}
\label{sec:extraccion_cap4}
Teniendo en cuenta el previo estudio de la literatura en el capítulo 2, se concluye que los métodos más prometedores, y que por lo tanto merecen la pena aplicar a este estudio comparativo serían los siguientes:
\subsubsection{Coeficientes Cepstrales en la escala de Mel}
Como ya se ha mencionado, MFCC es uno de los mejores algoritmos para capturar características de la señal de audio por su similitud a cómo el sistema auditivo humano procesa el sonido y las frecuencias, de la misma manera, su efectividad se ha visto reportada y discutida a lo largo de otros estudios.
La librería usada para la manipulación de audio Librosa ofrece la posibilidad de extraer características MFCC de un archivo de audio. En cuanto a la configuración, se extraerán 13 características MFCC usando el rango de muestreo del propio archivo de audio \citep{Bao2019}.

\subsection{Espectrogramas}
%Como ya se había explicado en la sección \ref{label}, los espectrogramas son la representación visual de un espectro de frecuencias de una señal que varía con el tiempo \citep{wikipedia2021}
Como se vio en el capítulo 2, y siguiendo los pasos de los trabajos \citep{Anvarjon2020} y \citep{Mustaqeem2020}, el uso de espectrogramas hace referencia a la conversión de la señal a imagen, y el objetivo de esta técnica es aprovechar las fortalezas de las redes convolucionales en las imágenes aplicándolas a un problema de señal de audio. En concreto para este trabajo, se hará uso de los espectrogramas de las características MFCC de la señal, cuyo proceso constará de dos partes: 
\begin{enumerate}
	\item Generación de espectrogramas como imagen
	\item Lectura y procesado de las imágenes que alimentarán la red
\end{enumerate}

Para generar estos espectrogramas, se hará con la ayuda del paquete Librosa, especificando en los correspondientes parámetros la extracción 13 características MFCC; Una vez generadas, se guardan en disco recortando el padding 0.05 pulgadas y en formato jpg.
Finalmente, las imágenes generadas son leídas con la ayuda de OpenCV donde se transforma su canal de color a RGB y son re-dimensionadas con un tamaño de 40 x 30 píxeles. Las figuras generadas tendrán un aspecto como el que se muestra en la figura \ref{fig:mfcc_sample}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{mfcc_espectrogram.jpg} 
	\caption{Espectrograma MFCC de una onda de audio. Fuente propia}
	\label{fig:mfcc_sample}
\end{figure}

\begin{comment}
Razon por el numero de características:
https://dsp.stackexchange.com/questions/28898/mfcc-significance-of-number-of-features
Mencionar que otros estudios
\end{comment}


\section{Configuración}
En esta sub-sección se muestra los recursos a los que se ha accedido para el desarrollo del estudio, así como su correspondiente configuración.


\paragraph{Google Colab} Para la exploración de los datos así como el desarrollo, y entrenamiento de los modelos se ha hecho uso de la plataforma gratuita desarrollada por Google, Google Colab. Esta plataforma ofrece 12GB de RAM  y 107.77GB de espacio en disco, que será más que suficiente dado el tamaño que nuestro dataset.

\paragraph{Librosa 0.8.1} Librosa es un paquete que ofrece diversas funcionalidades para el análisis de audio y música, cuya información más en detalle se puede encontrar en \citep{librosa082}. Esta librería ha sido esencial para la extracción de características MFCC así como algunas técnicas de aumento de datos.

\paragraph{OpenCV 3.4.2} OpenCV es una librería de código abierto desarrollada por INTEL en 1999, cuyo principal objetivo es la provisión de funciones y recursos para visión computacional, cubriendo áreas como la reconstrucción 3D, detección de movimiento o reconocimiento de objetos, etc. Si bien en este proyecto no se necesitarán sus recursos más avanzados, será útil en la lectura de espectrogramas como imagen. 

\paragraph{Tensorflow 2.0} Es una plataforma de código abierto originalmente creada por Google que provee un conjunto de librerías y recursos para el desarrollo de modelos con aprendizaje automático. Tensorflow, que además ofrece soporte de Keras, se ha usado tanto para la estandarización de los datos, como para la compilación y entrenamiento del modelo.

\paragraph{Python 3} Python es un lenguaje interpretado de alto nivel. Todo el código para este proyecto ha sido desarrollado en Python 3.

\section{Pre-procesado de los datos}
Como se ha visto en la sección \ref{lb_c4_datos} no hay una abundante disposición de datos, esto podría convertirse en un problema y perjudicar el rendimiento del modelo en el entrenamiento. 
Será necesario, antes del entrenamiento, un previo procesado de los datos.


\subsection{Normalización, estandarización y balanceado de los datos}
\label{cap:normalizacion}
Los siguientes puntos se aplicarán a todas las pruebas:

%MFCC values are not very robust in the presence of additive noise, and so it is common to normalize their values in speech recognition systems to lessen the in influence of noise.
\begin{itemize}
	\item El proceso de estandarización consistirá en el cociente entre la media artimética de los valores de los datos de entrenamiento, y la desviación normal de los de test. En esta técnica los valores son centrados con respecto a su media con una desviación estándar, consiguiendo una mejora en la estabilidad numérica del modelo. Ya que a lo largo de las pruebas, se usarán combinaciones de aumentos de datos e incluso mezclas entre distintos datasets, es recomendable aplicar este paso.
	%https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/
	
	\item  La categorización cambia el formato de los datos para su uso en el modelo con keras. En nuestro caso se utilizará la codificación \textit{One Hot} que representa los enteros en secuencias de bits. Esto se hace con el fin de evitar que los números enteros no confundan al modelo asignándole una peso proporcional al índice asociado a una clase (categoría).
	
	\item La división de los datos en entrenamiento (70\%), y validación(20\%) y test(10\%), se hará con el algoritmo \emph{StratifiedShuffleSplit} de la librería de Python \emph{sklearn} que además se encarga de barajar de manera aleatoria los datos previamente y asignar cantidades equitativas a las clases cuando se divide en entrenamiento y test.
	
\end{itemize}


\subsection{División de los datos por género}
\label{cap4:division}
En un primer análisis exploratorio de los datos, se han estudiado las diferencias entre la voz masculina y la voz femenina en las emociones, observando lo siguiente:

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{comparative_waveform.png} 
	\caption{Comparativa de los extractos de voz por género en RAVDESS}
	\label{fig:comp_emociones_genero}
\end{figure}
En la figura \ref{fig:comp_emociones_genero} se puede observar la comparación de la voz masculina (naranja) y la voz femenina (azul) por cada una de las emociones en el idioma inglés. Ya que es muy distinto, puede ser recomendable dividir el conjunto de datos atendiendo a esta característica.


\subsection{Técnicas de aumento de datos}
Dado el bajo número de muestras en los distintos conjuntos de datos a los que se pudo acceder, se ha visto conveniente explorar distintas técnicas de aumento de datos. El aumento de datos es una técnica por la cual, se aumenta el número de muestras en un conjunto mediante la creación de nuevas muestras sintéticas con pequeñas modificaciones a cada uno de los archivos. Esta aumento de los datos se puede traducir por una reducción del overfitting (sobreajuste), ya que el modelo se mantendría invariable mejorando así su capacidad de generalización.
Esta técnica es ampliamente conocida cuando se procesan imágenes, siendo esas modificaciones rotaciones, transposiciones etc. 

Se sabe que el sonido tiene las siguientes características: tono, duración, timbre e intensidad\citep{Moataz2011}, por lo que se deben modificar levemente los datos alrededor de esas características de manera que sólo difieran en pequeños factores de la muestra original.


\subsubsection{Ruido Blanco}
Añadir ruido blanco a la pista de audio, implica la inyección de valores aleatorios distribuidos de manera irregular con una media de 0 y una desviación de estándar de 1. Para implementar este método se usará el paquete numpy.
\begin{comment}

\begin{figure}[!htb]
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{original-waveform.JPG}
		\caption{Muestra original}
		\label{ref:audio_original_1}
	\end{minipage}\hfill 
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{wn-waveform.JPG}\hfill
		\caption{Misma muestra con ruido blanco}
		\label{ref:audio_wn}
	\end{minipage}
\end{figure}
\end{comment}

\subsubsection{Desplazamiento del sonido}
Desplaza el sonido hacia la izquierda o la derecha una cantidad aleatoria de segundos. Por ejemplo, si el sonido ha sido desplazado hacia delante (izquierda) x segundos, los x primeros segundos se marcan con 0. Si por el contrario han sido desplazados hacia detrás (derecha), los últimos x segundos se marcarán con 0.
\begin{comment}
\begin{figure}[!htb]
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{original-waveform.JPG}
		\caption{Muestra original}
		\label{ref:audio_original_2}
	\end{minipage}\hfill 
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{shift-waveform.JPG}\hfill
		\caption{Misma muestra con desplazamiento del sonido}
		\label{ref:audio_shiftting}
	\end{minipage}
\end{figure}
\end{comment}
\subsubsection{Modulación del tono}
Se refiere al proceso de cambiar el tono a un sonido sin variar su velocidad. Para implementar este método se usará la librería Librosa que ofrece un método específico.

\begin{comment}
\begin{figure}[H]
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{original-waveform.JPG}
		\caption{Muestra original}
		\label{ref:audio_original_3}
	\end{minipage}\hfill 
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{pitch-waveform.JPG}\hfill
		\caption{Misma muestra con cambio del tono}
		\label{ref:audio_tunning}
	\end{minipage}
\end{figure}
\end{comment}


\section{Arquitectura}
\label{sec:arquitectura_cap4}
Como se ha podido ver en la revisión de la literatura del capítulo 2, las redes convolucionales esta una tendencia muy adoptada en los últimos trabajos en esta área de estudio.

%Por temas de tiempo blablabla [...] se ha decidido por una red base que reporte buenos resultados.
%blabla...

%Los trabajos como los de \citep{AbdulQayyum2019}, \citep{Anvarjon2020} utilizan CNN en sus trabajos. 
%Para un caso base en la línea de trabajo se ha optado por una red convolucional inspirada %en el trabajo de \citep{AbdulQayyum2019}, la cual consta de:
\subsection{Arquitectura principal}

\subsubsection{Modelo CNN-LSTM}
\label{cap4:Modelo4}
\begin{itemize}
	\item 3 capas convolucionales unidimensionales con 64 filtros de 3 x 3 y activación Relu, seguidas de una capa Max Pooling con tamaño 2 para la pool.
	
	\item Una capa Flatten con un Dropout del 25\%
	
	\item 2 capas LSTM unidimensionales con 50 y 20 unidades respectivamente y un Dropout del 50\%. Sólo se permitirá a la primera capa LSTM devolver el estado oculto de salida por cada entrada de tiempo. Ya que a este nivel se cambia a redes unidimensionales, se deberá redimensionar la entrada a 1 x 960.
	
	\item Capa de salida densa de 7 nodos y función Softmax.
	
\end{itemize}
La figura \ref{fig:archCNNLSTM} muestra un esquema gráfico de los puntos mencionados.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{architectureLSTMCNN.PNG} 
	\caption{Arquitectura propuesta. Fuente propia}
	\label{fig:archCNNLSTM}
\end{figure}


La estrategia de entrenamiento que se siguió fue un optimizador Adam con los parámetros por defecto que ofrece Keras y la entropía cruzada categórica (\emph{categorical crossentropy}) como función de pérdida.


\subsection{Otras arquitecturas}
A pesar de que la arquitectura citada en la sección anterior, será con la que se lleven a cabo los principales experimentos y con los resultados más interesantes, se considera que la mención de otras arquitecturas que han formado parte de los experimentos de este trabajo, ayudará al lector a entender la progresión de los mismos, por lo que se ha visto conveniente incluirlas.

% TODO:
\subsubsection{CNN 1D simplificado}
\label{cap4:Modelo1}
\begin{itemize}
	\item 3 capas convolucionales unidimensionales con activación Relu. El número de filtros es de 128 y  y el tamaño del kernel de 5 con BatchNormalization y MaxPooling de tamaño 8.
	
	
	\item 1 capa convolucional unidimensional con activación Relu. El número de filtros es de 128 y  y el tamaño del kernel de 5 con BatchNormalization y Dropout del 10\%.
	
	\item 1 capa densa de 128 nodos.
	
	\item Por último esta arquitectura cierra con una capa densa con 7 nodos (número de clases) con una función de activación Softmax.
\end{itemize}

\subsubsection{Modelo CNN 1D}
\label{cap4:Modelo2}
En un principio, se optó por una línea de trabajo inspirada en el estudio de \citep{AbdulQayyum2019} ya que combina buenos resultados y un sistema sencillo. Pero tras varias pruebas esta arquitectura se ha refinado hasta definirse, por ahora, lo siguiente:

\begin{itemize}
	\item 2 capas convolucionales unidimensionales con activación Relu. El número de filtros es de 128 y  y el tamaño del kernel de 5.
	En las dos capas convolucionales se usa regularización de tipo L2 para aplicar una penalización a las capas del kernel con un valor de 0.01 y corregir así el overfitting.
	
	\item La primera capa convolucional está seguida de una capa Dropout del 0.5 y una capa Max Pooling con un tamaño 8.
	
	\item A la segunda capa convolucional se sigue otra capa de Dropout con un valor del 25\% y una capa Flatten.
	
	\item Por último esta arquitectura cierra con una capa densa con 7 nodos (número de clases) con una función de activación Softmax.
\end{itemize}

En cuanto al entrenamiento del modelo se usará un optimizador RMSprop con una tasa de aprendizaje de 0.00005, valor de $\rho$ de 0.9 y $\epsilon$ a 'None', por dar mejores resultados frente a Adam del caso inicial desde el que se partió, mientras que la función de pérdida utilizada para este propósito será entropía cruzada categórica (\emph{categorical crossentropy}).\\
Se añaden además, para intentar afinar el modelo los callbacks ReduceLROnPlateau, que reduce la tasa de aprendizaje cuando el modelo ha dejado de mejorar y EarlyStopping, que detiene el entrenamiento si se ha llegado una meseta, es decir, si durante un determinado número de épocas, el modelo ha dejado de mejorar. En ReduceLROnPlateau se monitorizará la \emph{val loss} con el fin de minimizarla, y como configuración se empleará un factor de reducción de la tasa de aprendizaje de 0.9, una paciencia de 20 épocas y una tasa de aprendizaje mínimo de 0.000001.
Con respecto a EarlyStopping, la variable monitorizada será 'val accuracy' con el fin de maximizarla y una paciencia de 20 épocas.\\
Este entrenamiento se llevará a cabo durante 1000 épocas cpn nun batch de tamaño 16.



\subsubsection{Modelo CNN 2D}
\label{cap4:Modelo3}
Esta arquitectura consiste en:
\begin{itemize}
	\item 3 capas convolucionales bidimensionales con 32 filtros y un tamaño del kernel de 4 x 10. Como función de activación se usa Relu y pading establecido a 'same'.
	
	\item A las todas las capas convolucionales, les sigue una capa Max Pooling con tamaño para la \emph{pool} de 3, que posteriormente se aplica un Dropout del 20\%
	
	\item Una capa Flatten, seguida de la capa densa de salida con 7 nodos y activación Softmax.
\end{itemize}

La estrategia de entrenamiento que se siguió fue un optimizador Adam con los parámetros por defecto que ofrece Keras y la entropía cruzada categórica (\emph{categorical crossentropy}) como función de pérdida.






\section{Criterios de éxito}
El objetivo de esta sección es definir las métricas que se usarán para comparar los distintos modelos en los experimentos parciales, así como los resultados obtenidos al aplicar dichos modelos a los datos mencionados en la sección \ref{lb_c4_datos}.\\
Las dos principales métricas que se usarán para decidir cómo de buena es la predicción del modelo serán:
\begin{itemize}
	\item \textbf{Exactitud o \emph{Accuracy} } Establece una comparación entre los resultados predichos y los obtenidos determinando cómo de preciso es el algoritmo cuando se trata de identificar las clases
	
	\[
	Accuracy = \frac{TP + TN}{ TP + FN + TN + FP} 
	\]
	\hfill \break
	
	\item \textbf{F1 score} Siendo el \emph{recall} la fracción de elementos relevantes que son recuperados (el cociente de las predicciones positivas y el número de clases positivas en el conjunto de test), la medida de F1 Score conviene el balance entre la precisión y el \emph{recall}.
	\[
	Recall = \frac{TP}{ TP + FN} 
	\]
	
	\[
	%\begin{equation}
	Precision = \frac{TP}{TP + FP}
	%\end{equation}
	\]
	
	\[
	F1 Score = \frac{2 * (Precision * Recall)}{Precision + Recall}
	\]
	\hfill \break
	
\end{itemize}

En algunos casos, la \emph{accuracy} puede ser engañosa debido a la paradoja de la exactitud donde puede existir un sesgo por a una distribución desbalanceada de las clases \citep{Valverde-Albacete2013}. Esto hace que pueda ser más inteligente elegir un modelo con menor exactitud pero con mayor poder predictivo.
Para ver ese poder predictivo por lo tanto, es aconsejable elegir más de una métrica de evaluación. Para ello se contará con F1 Score que es la media armónica entre el \emph{recall} y la precisión.

Estas métricas se aplicarán a todas las clases en cada uno de los experimentos con el objetivo de determinar cómo de bueno es el modelo a la hora de reconocerlas. Como se ha visto en la presentación de las bases de datos, la clases tienden a estar muy balanceadas con la excepción de EMO-DB en alemán. Por ello, para tener una representación más igualitaria se aplicarán técnicas de aumento de datos a este conjunto y se balanceará el número de sus clases.

Accuracy y F1 se tomarán por cada una de las clases, a la vez que se observa el rendimiento del modelo por cada época para comprobar si reporta un ajuste correcto.

\section{Diseño de los experimentos}
%TODO intro
% Ahora se detallaran los experimentos blablableble
%Por cuestiones de espacio, se han incluido unicamente la mejor versión de ese diseño tatata...
En la presente sección se expondrán los diseños de los experimentos para cumplir los objetivos establecidos en el capítulo 3. Por un lado hay que conseguir un modelo que sea capaz de clasificar satisfactoriamente en el idioma con el que se entrena, para que finalmente se evalúe este modelo en otras lenguas distintas permitiendo hacer una comparación. Por ello, se divide la presente en dos bloques, con la intención de que al lector le resulte más fácil seguir la narrativa de este proyecto. Dentro de estos bloques se muestran los experimentos planteados, que a su vez están formados por varias pruebas con una configuración similar.

\subsection{Búsqueda del mejor modelo}
Estos experimentos giran en torno a la búsqueda del modelo en el que más tarde, se validarán los lenguajes extranjeros. Como resumen se ofrece al final, la tabla \ref{tab:resumenTests1} que resume las características más diferenciadoras de cada uno de estas pruebas.

\subsubsection{Experimento 1}
\begin{enumerate}[label*= Prueba 1.\Alph*, leftmargin=3cm]
	\item RAVDESS subdividido por género femenino.
	\item RAVDESS subdividido por género masculino.
	\item RAVDESS sin subdivisión.
\end{enumerate}

\subsubsection{Experimento 2}
\begin{enumerate}[label*= Prueba 2.\Alph*, leftmargin=3cm]
	\item RAVDESS sin subdivisión con aumento de datos: Ruido Blanco.
	\item RAVDESS sin subdivisión con aumento de datos: Desplazamiento.
	\item RAVDESS sin subdivisión con aumento de datos: Modulación.
	\item RAVDESS sin subdivisión con aumento de datos: Ruido Blanco, Desplazamiento y Modulación.
\end{enumerate}

\subsubsection{Experimento 3}
\begin{enumerate}[label*= Prueba 3.\Alph*, leftmargin=3cm]
	\item Ensamblado con RAVDESS y TESS.
	\item Ensamblado con SAVEE y TESS.
	\item Ensamblado con RAVDESS, SAVEE y TESS.
\end{enumerate}

\subsubsection{Experimento 4}
\begin{enumerate}[label*= Prueba 4.\Alph*, leftmargin=3cm]
	\item Ensamblado con RAVDESS y TESS con aumento de datos.
	\item Ensamblado con SAVEE y TESS con aumento de datos.
	\item Ensamblado con RAVDESS, SAVEE y TESS con aumento de datos.
\end{enumerate}

\subsubsection{Experimento 5}
\begin{enumerate}[label*= Prueba 5.\Alph*, leftmargin=3cm]
	\item Ensamblado con SAVEE y TESS con modelo CNN usando espectrogramas.
\end{enumerate}

\subsubsection{Experimento 6}
\begin{enumerate}[label*= Prueba 6.\Alph*, leftmargin=3cm]
	\item Ensamblado con SAVEE y TESS con modelo CNN-LSTM usando espectrogramas.
\end{enumerate}

\begin{savenotes}
\begin{table}[H]
	\begin{adjustwidth}{-1cm}{}
		\centering
		\begin{center}
			\begin{tabular}{| c | p{3.5cm} | p{3.5cm} | p{3cm}| c |}
				\hline
				\textbf{Prueba} &  
				\textbf{Datos Entr.} \footnote{Se refiere a las bases de datos que se usarán tanto para el entrenamiento, validación y test.}  & 
				\textbf{Método} \footnote{Arquitectura usada en la prueba.}  &  
				\textbf{Aumento} \footnote{Técnica de aumento de datos que se usará.}   &  
				\textbf{Vol. Datos} \footnote{Volumen de datos con los que se trabajará.} \\ 
				\hline\hline
				1.A & RAVDESS (subdivisión femenina) & Modelo CNN 1D simple  &  & 528 muestras\\ 
				\hline
				1.B & RAVDESS (subdivisión masculina) & Modelo CNN 1D simple &  & 528 muestras\\
				\hline
				1.C & RAVDESS & Modelo CNN 1D simple  &   & 1056 muestras\\ 
				\hline
				2.A & RAVDESS & Modelo CNN 1D   &  Ruido Blanco & 2112 muestras \\ 
				\hline
				2.B & RAVDESS & Modelo CNN 1D   &  Desplazamiento & 2112 muestras \\
				\hline
				2.C & RAVDESS & Modelo CNN 1D  &  Modulación & 2112 muestras \\
				\hline
				2.D & RAVDESS & Modelo CNN 1D  &  Ruido Blanco, Desplazamiento y Modulación & 4224  muestras \\
				\hline
				3.A & RAVDESS y TESS & Modelo CNN 1D  &   & 3056 muestras \\
				\hline
				3.B & TESS y SAVEE & Modelo CNN 1D   &   & 2824 muestras \\
				\hline
				3.C & RAVDESS, SAVEE y TESS & Modelo CNN 1D   &   & 3476 muestras \\
				\hline
				4.A & RAVDESS y TESS & Modelo CNN 1D  &  Modulación & 6112 muestras \\
				\hline
				4.B & SAVEE y TESS & Modelo CNN 1D  &  Modulación & 5648 muestras \\
				\hline
				4.C & RAVDESS, SAVEE y TESS & Modelo CNN 1D  &  Modulación & 10112 muestras \\
				\hline
				5 & SAVEE y TESS & Modelo CNN 2D con espectrogramas MFCC &  & 2824 muestras \\
				\hline
				6 & SAVEE y TESS & Modelo CNN-LSTM con espectrogramas MFCC &  & 2824 muestras \\
				\hline
			\end{tabular}
			
			\caption{Resumen de las pruebas para la obtención de un modelo óptimo en la propia lengua (inglés)}
			\label{tab:resumenTests1}
		\end{center}
	\end{adjustwidth}
\end{table}
\end{savenotes}



\subsection{Pruebas con lenguajes extranjeros}
En estas pruebas se evaluarán los mejores modelos de la sección anterior, con lenguajes que no han sido vistos en su entrenamiento. Al final de este bloque se presenta la tabla \ref{tab:resumenTests2} que resume los puntos mas característicos de las pruebas.

\subsubsection{Experimento 7}

\begin{enumerate}[label*= Prueba 7.\Alph*, leftmargin=3cm]
	\item Evaluación del dataset EMO-DB con el tercer mejor modelo.% del conjunto de experimentos anterior.
	\item Evaluación del dataset EMO-DB con el segundo mejor modelo.% del conjunto de experimentos anterior.
	\item Evaluación del dataset EMO-DB con el primer mejor modelo.% del conjunto de experimentos anterior.
\end{enumerate}

\subsubsection{Experimento 8}
\begin{enumerate}[label*= Prueba 8.\Alph*, leftmargin=3cm]
	\item Evaluación del dataset CaFE con el tercer mejor modelo.% del conjunto de experimentos anterior.
	\item Evaluación del dataset CaFE con el segundo mejor modelo.% del conjunto de experimentos anterior.
	\item Evaluación del dataset CaFE con el primer mejor modelo.% del conjunto de experimentos anterior.
\end{enumerate}


\begin{savenotes}
\begin{table}[H]
	\begin{adjustwidth}{-1cm}{}
		\centering
		\begin{center}
			\begin{tabular}{| c | p{3.5cm} | c | p{3cm}| c |}
				\hline
				\textbf{Prueba} & 
				\textbf{Modelo} & 
				\textbf{Datos Test}t&  
				\textbf{Aumento} \footnote{Este aumento se sólo se aplica a los datos de test con el fin de balancearlos.}  
				&  \textbf{Vol. Datos} \\ 
				\hline\hline
				7.A & Tercer mejor modelo  &  EMO-DB & Modulación & 552\\ 
				\hline
				7.B & Segundo mejor modelo  &  EMO-DB & Modulación & 552\\ 
				\hline
				7.C & Primer mejor modelo  &  EMO-DB & Modulación & 552\\ 
				\hline
				8.A & Tercer mejor modelo  &  CaFE & Modulación & 552\\ 
				\hline
				8.B & Segundo mejor modelo  &  CaFE & Modulación & 552\\ 
				\hline
				8.C & Primer mejor modelo  &  CaFE & Modulación & 552\\ 
				
				\hline
			\end{tabular}
			
			\caption{Resumen de las pruebas para la obtención de un modelo óptimo en la propia lengua (inglés)}
			\label{tab:resumenTests2}
		\end{center}
	\end{adjustwidth}
\end{table}
\end{savenotes}

\begin{comment}

\subsection{Búsqueda de un  modelo óptimo}
Estos experimentos giran en torno a la búsqueda del modelo en el que más tarde, se validarán los lenguajes extranjeros. Como resumen se ofrece al final, la tabla \ref{tab:resumenTests1} que resume las características más diferenciadoras de cada uno de estas pruebas.

\subsubsection[]{\large Prueba 1 {\normalsize \textcolor{Gray}{RAVDESS con división de los datos por género en una red convolucional unidimensional}}}

\paragraph{Datos y preprocesado}
Para esta prueba se usará el conjunto de datos RAVDESS dividiendo previamente el conjunto de entrenamiento por género (720 muestras iniciales por cada uno). 

De cada una de las divisiones, se extraerán las características MFCC como es descrito en la sección \ref{sec:extraccion_cap4} y se normalizarán como se especifica en \ref{sec:norm_cap4}. Una vez normalizados, se categorizarán usando la técnica \emph{One hot encoding} para que el modelo asigne una importancia equitativa a todas las clases.
Finalmente los datos son divididos en entrenamiento (70\%), y validación(20\%) y test(10\%).\\

\paragraph{Entrenamiento}
Para el entrenamiento se usará el modelo Modelo1 CNN descrito en la sección \ref{cap4:Modelo1}. El modelo es entrenado durante un total de 100 épocas y con un tamaño del batch de 16.



\subsubsection[]{\large Prueba 1.B {\normalsize \textcolor{Gray}{RAVDESS en una red convolucional unidimensional}}}
Se aplicarán los mismos parámetros que en la Prueba 1 con la diferencia que se usará el conjunto de datos RAVDESS al completo.

%PRUEBA 2 - NOTEBOOK 3
\subsubsection{\large Prueba 2  {\normalsize \textcolor{Gray}{RAVDESS con técnicas de aumento de datos.}}}

\paragraph{Datos y preprocesado}Para esta prueba se usará el conjunto de datos RAVDESS completo (audio sólo, 1440 muestras iniciales), haciendo uso de las técnicas de aumento de datos que se han especificado en la sección \ref{sec:aumento_cap4} de manera independiente, a saber:
\begin{itemize}
	\item Ruido blanco
	\item Desplazamiento del sonido.
	\item Modulación del tono.
\end{itemize}
Para ello se extraerán las características MFCC, normalizándolas y categorizándolas con \emph{One hot encoding}.

\paragraph{Entrenamiento}Para este entrenamiento se usará el modelo descrito en la subsección \ref{cap4:Modelo2}. el cual está basado en una red convolucional unidimensional de tres capas y regularización L2 en las dos primeras.
El modelo se compila con un optimizador RMSprop con un learning rate de 0.00005 y $\rho$ a 0.9, y finalmente se entrena durante 100 épocas con un batch de tamaño 16.\\


%PRUEBA 2.B - NOTEBOOK 5
\subsubsection[]{\large Prueba 2.B {\normalsize \textcolor{Gray}{Combinación de las técnicas de aumento de datos en RAVDESS}}}

\paragraph{Datos y preprocesado}Esta prueba implementará los mismos parámetros que la anterior, pero hará uso de las técnicas de aumento de datos combinándolas entre sí.
\begin{itemize}
	\item Ruido blanco y desplazamiento del sonido, con un total de 4320 características MFCC.
	
	\item Modulación del tono y desplazamiento, con un total de 4320 características MFCC.
	
	\item Ruido blanco, desplazamiento del sonido y modulación del tono con un total de 5760 características MFCC.
\end{itemize}

\paragraph{Entrenamiento} Para el entrenamiento, se seguirán los mismos parámetros que en la prueba 2.	


%PRUEBA 3 - NOTEBOOK 1_CROSSCORPUS
\subsubsection[]{\large Prueba 3 {\normalsize \textcolor{Gray}{Combinación de datasets}}}

\paragraph{Datos y preprocesado} Siguiendo una estrategia distinta con el fin de aumentar el número de muestras, se combinarán diferentes bases de datos (RAVDESS, SAVEE y TESS, detalladas en la sección \ref{sec:idioma_ref}). Se extraerán las características MFCC para luego normalizar las muestras y categorizarlas.
Las subsecciones de esta prueba corresponden a las distintas combinaciones de los datos:
\begin{itemize}
	\item RAVDESS y TESS, con un total de 4240 muestras.
	\item SAVEE y TESS, con un total de 3280 características MFCC.
	\item RAVDESS, SAVEE y TESS, con un total de 4720 características.
\end{itemize}

\paragraph{Entrenamiento} Para el entrenamiento se usará el modelo Modelo2 CNN basado en una arquitectura CNN unidimensional, tal y como se detalla en la sección \ref{cap4:Modelo2}, entrenándolo durante 100 épocas con un batch de tamaño 32.



\subsubsection[]{\large Prueba 3.B {\normalsize \textcolor{Gray}{Combinación de datasets con aumento de datos}}}
% basandonos en las pruebas anteriores, se aplicara esta tecnica o esta otra
Se seguirán los mismos parámetros en la Prueba 3, pero aplicando técnicas de aumento de datos (en concreto se aplicará Ruido blanco, por ser la que mejor resultado reportó).

%PRUEBA 3 - NOTEBOOK 2_CROSSCORPUS
\subsubsection{\large Prueba 4 {\normalsize \textcolor{Gray}{Red CNN bidimensional con espectrogramas}}}

%\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
\paragraph{Datos y preprocesado} Para esta prueba se ha decidido prescindir de la base de datos RAVDESS y usar en su lugar, la combinación SAVEE y TESS con un total de 3284 muestras. Con respecto al preprocesado de los datos, se extraerán las características MFCC y posteriormente esas características serán convertidas a imagen.

\paragraph{Entrenamiento} Para el entrenamiento se usará el Modelo 3 basado en una arquitectura CNN bidimensional compilándolo con un optimizador Adam (valores por defecto en keras) tal y como es descrito en la sección \ref{cap4:Modelo3}, entrenándolo durante 100 épocas con un tamaño del batch de 32.
%\end{minipage}
%\setlength{\leftskip}{0cm}


\subsubsection{\large Prueba 5 {\normalsize \textcolor{Gray}{Red LSTM-CNN con espectrogramas}}}

%\hfill\begin{minipage}{\dimexpr\textwidth-1cm}

\paragraph{Datos y preprocesado} El conjunto de datos usado para esta prueba, será la combinación SAVEE y TESS con un total de 3284 muestras. Con respecto al preprocesado de los datos, se extraerán las características MFCC y posteriormente esas características serán convertidas a imagen, exactamente igual que en la prueba 4.

\paragraph{Entrenamiento} Para el entrenamiento se usará el Modelo 4 basado en una arquitectura dual CNN-LSTM compilándolo con un optimizador Adam (valores por defecto en keras) tal y como es descrito en la sección \ref{cap4:Modelo4}, entrenándolo durante 100 épocas con un tamaño del batch de 32.

%\end{minipage}

Como se menciona en el inicio de esta sección, la tabla \ref{tab:resumenTests1} muestra el resumen con los datos que creemos más característicos de cada una de las pruebas. 
Así pues, en la columna 'Datos Entr.' se refiere al conjunto de datos que se usarán en el entrenamiento, especificando la naturaleza de estos datos en 'Tipo Dato' (espectrogramas como imagen, o características directamente extraídas de la muestra de audio) mientras que en 'Arquitectura', con qué modelo se llevará a cabo la prueba. En la columna Aumento Datos, se muestra qué técnica de aumento de datos se ha utilizado (en tal caso), donde 'R.B.' corresponde a ruido blanco, 'Desp.' a desplazamiento del sonido y 'Mod.' a modulación del tono.


\begin{table}[H]
	\begin{adjustwidth}{-1cm}{}
		\centering
		\begin{center}
			\begin{tabular}{| c | p{3.5cm} | c | c c c | p{3.5cm} |}
				\hline
				\multirow{2}{*}{\textbf{Prueba}} &\multirow{2}{*}{\textbf{Datos Entr.}}& \multirow{2}{*}{\textbf{Arquitectura}} & 
				\multicolumn{3}{|c|}{\textbf{Aumento Datos}} & 
				\multirow{2}{*}{\textbf{Tipo Dato}} \\ \cline{4-6}
				& & & R.B. & Desp. & Mod. & \\ 
				\hline \hline
				1 & RAVDESS por género & CNN 1D simplificado & No &No &No & carac. MFCC	\\ \hline
				
				1.B & RAVDESS & CNN 1D simplificado &No &No &No & carac. MFCC\\ \hline
				
				\multirow{3}{*}{2} & \multirow{3}{*}{RAVDESS} & \multirow{3}{*}{CNN 1D} &Sí &No &No  & \multirow{3}{*}{carac. MFCC}\\
				& & & No &Sí &No & 	\\
				& & & No &No &Sí  & 	\\ \hline
				
				
				\multirow{3}{*}{2.B} & \multirow{3}{*}{RAVDESS} & \multirow{3}{*}{CNN 1D } & Sí &Sí &No & \multirow{3}{*}{carac. MFCC}\\
				
				& &  & No &Sí &Sí & \\
				& &  & Sí &Sí &Sí& \\ \hline
				
				\multirow{3}{*}{3} & RAVDESS y TESS & \multirow{3}{*}{CNN 1D} &  No &No &No & \multirow{3}{*}{carac. MFCC}	\\ 
				\cline{2-2}
				& SAVEE y TESS &  &  No &No &No & 	\\
				\cline{2-2}
				& RAVDESS, SAVEE y TESS &  &  No &No &No & 	\\ \hline
				
				\multirow{3}{*}{3.B} & RAVDESS y TESS & \multirow{3}{*}{CNN 1D} & \multirow{3}{*}{ Sí}&\multirow{3}{*}{ No} & \multirow{3}{*}{ No} & \multirow{3}{*}{carac. MFCC}	\\
				& SAVEE y TESS &  & &&  & 	\\
				& RAVDESS, SAVEE y TESS &  & &&  & 	\\ 
				\hline
				
				4 & SAVEE y TESS & CNN 2D &Sí &No &No & espectrogramas MFCC\\ \hline
				
				5 & SAVEE y TESS & CNN 2D - LSTM &Sí &No &No & espectrogramas MFCC\\ \hline
				
			\end{tabular}
			
			\caption{Resumen de las pruebas para la obtención de un modelo óptimo en la propia lengua (inglés)}
			\label{tab:resumenTests1}
		\end{center}
	\end{adjustwidth}
\end{table}


\subsection{Pruebas con lenguajes extranjeros}
En estas pruebas se evaluarán los mejores modelos de la sección anterior, con lenguajes que no han sido vistos en su entrenamiento. Al final de este bloque se presenta la tabla \ref{tab:resumenTests2} que resume los puntos mas característicos de las pruebas.

\subsubsection{\large Prueba 6 {\normalsize \textcolor{Gray}{Red CNN 1D validada en alemán}}}

\paragraph{Datos y preprocesado}  El conjunto de datos usado para el entrenamiento será la combinación SAVEE y TESS, sin embargo para que las clases del conjunto de entrenamiento y las de validación coincidan, es necesario prescindir de dos clases: sorpresa y miedo.\\ 
Por otro lado, los datos de validación presentaban una distribución muy irregular, por lo que para que tuvieran una representación más equilibrada y una interpretación más justa en la validación, se han balanceado aumentando el dataset con aumento de datos (ruido blanco) y se han igualado el número de instancias de cada clase a aquella con una proporción menor.\\
A continuación se extraen 13 características MFCC y después de estandarizar los datos como se detalla en la sección \ref{cap:normalizacion}, se dividen en entrenamiento y test. Debido a que los datos que se usarán para la validación provienen de una fuente externa a la de entrenamiento el balance para esta división ha sido de 75\% para entrenamiento y 25\% para test.\\

\paragraph{Entrenamiento} Para el entrenamiento se utilizará el Modelo 1CNN compilándolo con el optimizador RMSProp como se describe en \ref{cap4:Modelo1}. Posteriormente es entrenado durante 100 épocas con un batch de tamaño 32.


\subsubsection[]{\large Prueba 7 {\normalsize \textcolor{Gray}{Red CNN 2D validada en alemán}}}

%\begin{adjustwidth}{1cm}{}

\paragraph{Datos y preprocesado} Al igual que en la prueba anterior, se reducirán de ambos conjuntos de datos el número de emociones a 5, eliminando de la ecuación 'miedo' y 'sorpresa'. Seguidamente, se procederá a la generación de imágenes donde:
\begin{enumerate}
	\item En primer lugar se aplicará aumento de datos (sólo con ruido blanco).
	\item Se leerán hasta un máximo de 92 para que quede balanceado.
\end{enumerate}
Una vez generadas las imágenes, se leerán con las dimensiones 30 x 40.

Ya que que los datos que se usarán para la validación provienen de una fuente externa a la de entrenamiento el balance para esta división ha sido de 75\% para entrenamiento y 25\% para test. 

\paragraph{Entrenamiento} En esta prueba se usará el Modelo 3 basado en una red convolucional bidimensional, entrenándolo durante 100 épocas con un tamaño del batch de 32.

%\end{adjustwidth}

\subsubsection[]{\large Prueba 8 {\normalsize \textcolor{Gray}{Red CNN-LSTM validada en alemán}}}

%\hfill\begin{minipage}{\dimexpr\textwidth-1cm}

\paragraph{Datos y preprocesado}  Este procedimiento, será exactamente igual que el de la prueba 7, tanto para los datos de entrenamiento como para los de validación.

\paragraph{Entrenamiento} Para el entrenamiento de esta prueba se usará el Modelo 4 basado en una red CNN- LSTM, entrenándolo durante 100 épocas con un tamaño del batch de 32.	
%\end{minipage}




\subsubsection{\large Prueba 9 {\normalsize \textcolor{Gray}{Red CNN 1D validada en francés}}}

%\hfill\begin{minipage}{\dimexpr\textwidth-1cm}

\paragraph{Datos y preprocesado} El dataset usado para la validación en esta prueba está bien balanceado y además las emociones que contempla coinciden con las del conjunto de entrenamiento, pero presenta pocos datos, por lo que se ha recurrirá al aumento de datos (ruido blanco) para paliar este problema.\\
A continuación se extraen 13 características MFCC y después de estandarizar los datos como se detalla en la sección \ref{cap:normalizacion}, se dividirán en entrenamiento y test. Como ha sido común en esta sección, el balance para esta división ha sido de 75\% para entrenamiento y 25\% para test.

\paragraph{Entrenamiento}  Para el entrenamiento se utilizará el Modelo 1CNN compilándolo con el optimizador RMSProp como se describe en \ref{cap4:Modelo1}. Posteriormente es entrenado durante 100 épocas con un batch de tamaño 32.

%\end{minipage}

\subsubsection{\large Prueba 10 {\normalsize \textcolor{Gray}{Red CNN 2D validada en francés}}}

\paragraph{Datos y preprocesado} Respecto al tratamiento de los datos de entrenamiento, no será diferente del que se hizo en la Prueba 5.
Para los datos de validación con la base de datos CaFE, en cambio, se procederá a lo siguiente:
\begin{enumerate}
	\item Generar las imágenes de los espectrogramas con Librosa y guardarlas.
	\item Leer las imágenes y sus etiquetas(nombre de la emoción) previamente generadas en el mismo orden que fueron leídas para el conjunto de entrenamiento para que coincidan cuando se categoricen.
	\item Normalizar y categorizar el conjunto.
	\item Barajar imágenes y etiquetas de la misma manera.
\end{enumerate}

\paragraph{Entrenamiento}   Para el entrenamiento de esta prueba se usará el Modelo 4 basado en una red CNN- LSTM, entrenándolo durante 100 épocas con un tamaño del batch de 32.



\subsubsection{\large Prueba 11 {\normalsize \textcolor{Gray}{Red CNN 1D validada en francés}}}

\paragraph{Datos y preprocesado} Este procedimiento, será exactamente igual que el de la prueba 10, tanto para los datos de entrenamiento como para los de validación.\\

\paragraph{Entrenamiento} Para el entrenamiento de esta prueba se usará el Modelo 4 basado en una red CNN- LSTM, entrenándolo durante 100 épocas con un tamaño del batch de 32.\\


\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c | c | c | c | c |}
			\hline
			\textbf{Prueba} &
			\textbf{Emociones}& 
			\textbf{Arquitectura} & 
			\textbf{Datos Val.} & 
			\textbf{Tipo Dato} \\ 
			\hline\hline
			6 & \multirow{3}{*}{5} & CNN 1D & \multirow{3}{*}{EMO-DB} & caract. MFCC\\ 
			7 &  & CNN 2D &  & espetrog. MFCC\\ 
			8 &  & CNN 2D - LSTM &  & espetrog. MFCC\\ \hline
			9 & \multirow{3}{*}{6} & CNN 2D & \multirow{3}{*}{CaFE} & caract. MFCC\\  
			10 &  & CNN 2D &  & espetrog. MFCC\\ 
			11 &  & CNN 2D - LSTM &  & espetrog. MFCC\\
			\hline
			
		\end{tabular}
		
		\caption{Resumen de las pruebas en las que se aplican los mejores modelos a distintos lenguajes.}
		\label{tab:resumenTests2}
	\end{center}
\end{table}
\end{comment}

\chapter{Desarrollo de la comparativa}
A continuación se presentan el resultado de los experimentos diseñados en el capítulo anterior. Acorde con la estructura que se le dio entonces, este capítulo se divide en dos partes: La primera sección presenta los datos de los experimentos que buscan un modelo óptimo que funcione en la lengua con la que es entrenado.
Los primeros dos experimentos pueden parecer irrelevantes desde la perspectiva de una conclusión final, pero ayuda a entender el punto de partida y su progresión.
La segunda sección resulta de la evaluación de los tres mejores modelos de la anterior con los idiomas extranjeros.

\section{Búsqueda del mejor modelo}
\label{cap:4ModeloOptimo}

\subsection{Experimento 1}
A continuación se exponen los resultados del experimento 1, que está formado por las pruebas 1.A, 1.B y 1.C.

\paragraph{Procesado de los datos} La distribución de las clases en los conjuntos de datos usados para este experimento queda como se muestra en la tabla \ref{ref:balance1}

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{ c  c | c  c |c  c }
			\hline
			\multicolumn{2}{c|}{Femenino}&
			\multicolumn{2}{|c|}{Masculino} &
			\multicolumn{2}{|c}{RAVDESS sin dividir} \\
			\hline
			enfado 		& 96 & enfado 	& 96 & enfado 	&192 \\
			asco 		& 96 & asco 		& 96 & asco 		&192 \\
			miedo 		& 96 & miedo 		& 96 & miedo 		&192 \\
			felicidad 	& 96 & felicidad 	& 96 & felicidad 	&192 \\
			tristeza 	& 96 & tristeza 	& 96 & tristeza 	&192 \\
			neutral 	& 48 & neutral 	& 48 & neutral 	&96 \\
			\hline
		\end{tabular}
		
		\caption{Distribución de los datos de las pruebas del experimento 1.}
		\label{ref:balance1}
	\end{center}
\end{table}

\paragraph{Entrenamiento} El rendimiento de los distintos modelos se puede visualizar a través de la gráfica de loss y accuracy en la figura \ref{fig:exp1}
\begin{figure}[H]
	\minipage{0.34\textwidth}
	\centering
	\includegraphics[width=\linewidth]{tests1_2/female_plot.jpg}
	{{\small Femenino}}   
	%\caption{(A) Femenino}\label{fig:female_plot}
	\endminipage
	\hfill
	\minipage{0.32\textwidth}
	\centering
	\includegraphics[width=\linewidth]{tests1_2/male_plot_ver.png}
	{{\small Masculino}}   
	%\caption{(B) Masculino}\label{fig:male_plot}
	\endminipage
	\hfill
	\minipage{0.32\textwidth}%
	\centering
	\includegraphics[width=\linewidth]{tests1_2/all_data_plot_ver.png}
	{{\small RAVDESS sin dividir}}   
	%\caption{(C) RAVDESS }\label{fig:ravdess_plot}
	\endminipage
	\caption{Rendimiento en el resultado de las pruebas del experimento 1. }\label{fig:exp1}
\end{figure}



\paragraph{Evaluación} En la tabla \ref{result_Test1} se muestran las métricas de éxito por cada una de las clases.

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c|| c c |  c c |  c c | }
			\hline
			\multicolumn{1}{|c||}{\textbf{Emoción}} & 
			\multicolumn{2}{c|}{\textbf{RAVDES Fem.}}&
			\multicolumn{2}{c|}{\textbf{RAVDES Masc.}} &
			\multicolumn{2}{c|}{\textbf{RAVDES sin dividir}} \\
			\hline
			& 
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}\\
			\hline
			
			enfado 		& 0.24 & 0.38 & 0.23 & 0.35 & 0.21 & 0.33\\
			asco 		& 1.00 & 0.42 & 0.75 & 0.44 & 0.62 & 0.33\\
			miedo 		& 0.80 & 0.55 & 0.60 & 0.53 & 0.76 & 0.59\\
			felicidad 	& 0.79 & 0.65 & 0.47 & 0.43 & 0.77 & 0.52\\
			tristeza 	& 0.17 & 0.08 & 0.00 & 0.00 & 0.42 & 0.14\\
			neutral 	& 1.00 & 0.18 & 0.00 & 0.00 & 0.20 & 0.06\\
			\hline
			\textbf{accuracy} 	& \multicolumn{2}{c|}{\textbf{56.47\%}} & \multicolumn{2}{c|}{\textbf{38.65\%}}  &\multicolumn{2}{c|}{\textbf{50\%}} \\
			\hline
		\end{tabular}
		
		\caption{Resultado de la evaluación de las pruebas en el experimento 1.}
		\label{result_Test1}
	\end{center}
\end{table}

\subsection{Experimento 2}
En este experimento se muestran los resultados de las pruebas 2.A, 2.B, 2.C y 2.D donde se exploran las diferentes técnicas de aumento de datos y cómo estas afectan al rendimiento del modelo.

\paragraph{Procesado de los datos} La distribución de las clases en los conjuntos de datos usados para este experimento queda como se muestra en la tabla \ref{ref:balance2}.

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{ c  c | c  c |c  c | c c}
			\hline
			\multicolumn{2}{c|}{Ruido Blanco}&
			\multicolumn{2}{|c|}{Desplazamiento} &
			\multicolumn{2}{|c|}{Modulación} & 
			\multicolumn{2}{|c}{Los tres métodos}\\
			\hline
			enfado 		& 384 & enfado 	& 384 & enfado 	& 384 & enfado & 384 \\
			asco 		& 384 & asco & 384 & asco 	& 384 & asco  & 384 \\
			miedo 		& 384 & miedo 	& 384 & miedo 	& 384 & miedo  & 384 \\
			felicidad 	& 384 & felicidad & 384 & felicidad & 384 & felicidad & 384 \\
			tristeza 	& 384 & tristeza& 384 & tristeza & 384 & tristeza  & 384 \\
			neutral 	& 192 & neutral & 192 & neutral & 192 & neutral& 192 \\
			\hline
		\end{tabular}
		
		\caption{Distribución de los datos en las pruebas del experimento 2.}
		\label{ref:balance2}
	\end{center}
\end{table}

\paragraph{Entrenamiento} El rendimiento de los distintos modelos se puede visualizar a través de la gráfica de loss y accuracy en la figura \ref{fig:plotsExp2}.


\begin{figure}[H]
		\minipage{0.50\textwidth}
		\centering
		\includegraphics[width=\linewidth]{tests1_2/whitenoise.png}
		{{\small Ruido blanco}}
		\endminipage
		\hfill
		\minipage{0.50\textwidth}
		\centering
		\includegraphics[width=\linewidth]{tests1_2/shiftting.png}
		{{\small Desplazamiento}}
		\endminipage
		%\hfill
		\vskip\baselineskip
		\minipage{0.50\textwidth}%
		\centering
		\includegraphics[width=\linewidth]{tests1_2/modulacion.png}
		{{\small Modulación}}
		\endminipage
	%	\hfill
		\minipage{0.50\textwidth}
		\centering
		\includegraphics[width=\linewidth]{tests1_2/todas.png}
		{{\small Los tres métodos juntos}}
		\endminipage
		\hfill
	\caption{Rendimiento en las pruebas del experimento 2}
	\label{fig:plotsExp2}
	\end{figure}

\paragraph{Evaluación} En la tabla \ref{result_Test2} se muestran las métricas de éxito por cada una de las clases.

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c|| c c |  c c |  c c | c c| }
			\hline
			\multicolumn{1}{|c||}{\textbf{Emoción}} & 
			\multicolumn{2}{c|}{\textbf{Ruido Blanco}}&
			\multicolumn{2}{c|}{\textbf{Desplazamiento}} &
			\multicolumn{2}{c|}{\textbf{Modulación}} &
			\multicolumn{2}{c|}{\textbf{Tres métodos}} \\
			\hline
			& 
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}\\
			\hline
			
			enfado 		& 0.19 & 0.32 & 0.22 & 0.36 & 1.00 & 0.59& 0.20 & 0.33\\
			asco 		& 1.00 & 0.21 & 0.89 & 0.35 & 1.00 & 0.62& 0.88 & 0.26\\
			miedo 		& 0.91 & 0.23 & 0.79 & 0.64 & 1.00 & 0.76& 0.87& 0.44\\
			felicidad 	& 0.75 & 0.07 & 0.95 & 0.39 & 0.98 & 0.70& 0.73 & 0.21\\
			tristeza 	& 0.00 & 0.00 & 0.75 & 0.14 & 1.00 & 0.76& 0.00 & 0.00\\
			neutral 	& 0.00 & 0.00 & 0.00 & 0.00 & 0.25 & 0.40& 0.00 & 0.00\\
			
			\hline
			\textbf{accuracy} 	& \multicolumn{2}{c|}{\textbf{44.91\%}} & \multicolumn{2}{c|}{\textbf{57.68\%}}  &\multicolumn{2}{c|}{\textbf{43.97\%}}  &\multicolumn{2}{c|}{\textbf{47.45\%}}\\
			\hline
		\end{tabular}
		
		\caption{Resultado de la evaluación de las pruebas del experimento 2.}
		\label{result_Test2}
	\end{center}
\end{table}



\subsection{Experimento 3}
En este experimento se muestran los resultados de las pruebas 3.A (RAVDESS y TESS), 3.B (TESSy SAVEE), y 3.C (RAVDESS, TESS y SAVEE) donde se explora el comportamiento del modelo CNN 1D con diferentes combinaciones de bases de datos para aumentar el volumen de muestras.\\

\paragraph{Procesado de los datos} La distribución de las clases en los conjuntos de datos usados para este experimento queda como se muestra en la tabla \ref{ref:balance3}.

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{ c  c | c  c |c  c }
			\hline
			\multicolumn{2}{c|}{RAVDESS y TESS}&
			\multicolumn{2}{c|}{TESS y SAVEE} &
			\multicolumn{2}{c}{RAVDESS, TESS y SAVEE} \\
			\hline
			enfado 		& 592 & enfado 		& 460 & enfado 		& 652\\
			asco 		& 592 & asco 		& 460 & asco 		& 652\\
			miedo 		& 392 & miedo 		& 260 & miedo 		& 452\\
			felicidad 	& 592 & felicidad 	& 460 & felicidad 	& 652\\
			tristeza 	& 392 & tristeza 	& 260 & tristeza 	& 452\\
			neutral 	& 496 & neutral 	& 520 & neutral 	& 652\\
			\hline
		\end{tabular}
		\caption{Distribución de los datos en las pruebas del experimento 3.}
		\label{ref:balance3}
	\end{center}
\end{table}

\paragraph{Entrenamiento} El rendimiento de los distintos modelos se puede visualizar a través de la gráfica de loss y accuracy en la figura \ref{fig:plotsExp3}.
\begin{figure}[H]
	\minipage{0.32\textwidth}
	\centering
	\includegraphics[width=\linewidth]{tests3/ravdess_tess_ver.png}
	{{\small RAVDESS y TESS}}
	\endminipage
	\hfill
	\minipage{0.34\textwidth}
	\centering
	\includegraphics[width=\linewidth]{tests3/tess_savee_ver.png}
	{{\small SAVEE y TESS}}
	\endminipage
	\hfill
	\minipage{0.34\textwidth}%
	\centering
	\includegraphics[width=\linewidth]{tests3/ravdess_tess_savee_ver.png}
	{{\small RAVDESS, SAVEE y TESS}}
	\endminipage
	\caption{Resultado del rendimiento de los modelos del experimento 3.}
	\label{fig:plotsExp3}
\end{figure}

\paragraph{Evaluación} En la tabla \ref{result_Test3} se muestran las métricas de éxito por cada una de las clases.
\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c|| c c |  c c |  c c | }
			\hline
			\multicolumn{1}{|c||}{\textbf{Emoción}} & 
			\multicolumn{2}{c|}{\textbf{RAVDES y TESS}}&
			\multicolumn{2}{c|}{\textbf{TESS y SAVEE}} &
			\multicolumn{2}{c|}{\textbf{RAVDES, TESS y SAVEE}} \\
			\hline
			& 
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}\\
			\hline
			
			enfado 		& 0.38 & 0.55 & 0.51 & 0.66 & 0.32 & 0.48 \\
			asco 		& 0.97 & 0.75 & 1.00 & 0.85 & 0.97 & 0.62 \\
			miedo 		& 1.00 & 0.53 & 1.00 & 0.79 & 1.00 & 0.52 \\
			felicidad	& 1.00 & 0.74 & 0.97 & 0.89 & 1.00 & 0.80 \\
			tristeza 	& 1.00 & 0.80 & 1.00 & 0.87 & 1.00 & 0.39 \\
			neutral 	& 1.00 & 0.89 & 1.00 & 0.79 & 1.00 & 0.81 \\
			\hline
			\textbf{accuracy} 	& \multicolumn{2}{|c|}{\textbf{71.89\%}} & \multicolumn{2}{|c|}{\textbf{88.22\%}}  &\multicolumn{2}{c|}{\textbf{\textbf{64.08\%}}} \\
			\hline
		\end{tabular}
		
		\caption{Resultados de las pruebas del experimento 3.}
		\label{result_Test3}
	\end{center}
\end{table}


\subsection{Experimento 4}
En este experimento se muestran los resultados de las pruebas 4.A, 4.B y 4.C donde se examina el comportamiento de la arquitectura y configuración del experimento 3 aplicando aumento de datos.

\paragraph{Procesado de los datos} La distribución de las clases en los conjuntos de datos usados para este experimento queda como se muestra en la tabla \ref{ref:balance4}.

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{ c  c | c  c |c  c }
			\hline
			\multicolumn{2}{c|}{RAVDESS y TESS}&
			\multicolumn{2}{c|}{TESS y SAVEE} &
			\multicolumn{2}{c}{RAVDESS, TESS y SAVEE} \\
			\hline
			enfado 		& 1184 & enfado 	& 920 & enfado 		& 1984 \\
			asco 		& 1184 & asco 		& 920 & asco 		& 1984 \\
			miedo 		& 784 & miedo 		& 520 & miedo 		& 1184 \\
			felicidad 	& 1184 & felicidad 	& 920 & felicidad 	& 1984 \\
			tristeza 	& 784 & tristeza 	& 520 & tristeza 	& 1184 \\
			neutral 	& 992 & neutral 	& 1040 & neutral 	& 1792 \\
			\hline
		\end{tabular}
		
		\caption{Distribución de los datos en las pruebas del experimento 4.}
		\label{ref:balance4}
	\end{center}
\end{table}

\paragraph{Entrenamiento} El rendimiento de los distintos modelos se puede visualizar a través de la gráfica de loss y accuracy en la figura \ref{fig:plotsExp4}.

\begin{figure}[H]
	\minipage{0.33\textwidth}
	\centering
	\includegraphics[width=\linewidth]{tests3/ravdess_tess_aug_ver.png}
	{{\small RAVDESS y TESS}}
	\endminipage
	\hfill
	\minipage{0.32\textwidth}
	\centering
	\includegraphics[width=\linewidth]{tests3/tess_savee_aug_ver.png}
	{{\small SAVEE y TESS}}
	\endminipage
	\hfill
	\minipage{0.32\textwidth}%
	\centering
	\includegraphics[width=\linewidth]{tests3/ravdess_tess_savee_aug_ver.png}
	{{\small RAVDESS, SAVEE y TESS}}
	\endminipage
	\caption{Resultado de los modelos en las pruebas del experimento 4.}
	\label{fig:plotsExp4}
\end{figure}

\paragraph{Evaluación} En la tabla \ref{result_Test4} se muestra el accuracy y F1 por cada una de las clases en los tres modelos.
\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c|| c c |  c c |  c c | }
			\hline
			\multicolumn{1}{|c||}{\textbf{Emoción}} & 
			\multicolumn{2}{c|}{\textbf{RAVDES y TESS}}&
			\multicolumn{2}{c|}{\textbf{TESS y SAVEE}} &
			\multicolumn{2}{c|}{\textbf{RAVDES, TESS y SAVEE}} \\
			\hline
			& 
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}\\
			\hline
			
			enfado 		& 0.37 & 0.53 & 0.48 & 0.65 & 0.50 & 0.66 \\
			asco 		& 0.97 & 0.77 & 0.92 & 0.83 & 0.96 & 0.84 \\
			miedo 		& 1.00 & 0.51 & 1.00 & 0.86 & 0.96 & 0.71 \\
			felicidad	& 0.97 & 0.72 & 0.94 & 0.91 & 0.94 & 0.84 \\
			tristeza 	& 1.00 & 0.61 & 1.00 & 0.79 & 1.00 & 0.82 \\
			neutral 	& 1.00 & 0.91 & 1.00 & 0.85 & 1.00 & 0.95 \\
			\hline
			\textbf{accuracy} 	& \multicolumn{2}{|c|}{\textbf{71.56\%}} & \multicolumn{2}{|c|}{\textbf{88.19\%}}  &\multicolumn{2}{c|}{\textbf{81.81\%}} \\
			\hline
		\end{tabular}
		
		\caption{Resultado de los datos en las pruebas del experimento 4.}
		\label{result_Test4}
	\end{center}
\end{table}


\subsection{Experimento 5}
A continuación se muestran los resultados del experimento 5, donde se trabajó con un modelo de redes convolucionales alimentado por espectrogramas.

\paragraph{Procesado de los datos} La distribución de las clases en el conjunto de datos usado (SAVEE y TESS) para este experimento queda como se muestra en la tabla \ref{ref:balance5}.

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{ c  c }
			\hline
			\multicolumn{2}{ c }{TESS y SAVEE} \\
			\hline
			enfado 		& 460 \\
			asco 		& 460 \\
			miedo 		& 460 \\
			felicidad 	& 460 \\
			tristeza 	& 460 \\
			neutral 	& 520 \\
			\hline
		\end{tabular}
		
		\caption{Distribución de los datos en las pruebas del experimento 5.}
		\label{ref:balance5}
	\end{center}
\end{table}

\paragraph{Entrenamiento} El rendimiento del correspondiente modelo se puede visualizar a través de la gráfica de loss y accuracy en la figura \ref{fig:cnn2d}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{test4_5/savee_tess_cnn2d.png} 
	\caption{Rendimiento del modelo CNN 2D usando los datos de SAVEE y TESS}
	\label{fig:cnn2d}
\end{figure}

\paragraph{Evaluación} En la tabla \ref{result_Test5} se muestran las métricas de éxito por cada una de las clases.
\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c|| c c | }
			\hline
			\multicolumn{1}{|c||}{\textbf{Emoción}} & 
			\multicolumn{2}{c|}{\textbf{CNN 2D}} \\
			\hline
			& 
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}\\
			\hline
			enfado 		& 0.86 & 0.89  \\
			asco 		& 0.90 & 0.91  \\
			miedo 		& 0.94 & 0.93  \\
			felicidad 	& 0.94 & 0.94  \\
			tristeza 	& 0.96 & 0.92  \\
			neutral 	& 0.88 & 0.89  \\
			\hline
			\textbf{accuracy} 	& \multicolumn{2}{c|}{\textbf{91.50\%}}\\
			\hline
		\end{tabular}
		
		\caption{Resultados del experimento 5 usando una arquitectura CNN 2D.}
		\label{result_Test5}
	\end{center}
\end{table}

\subsection{Experimento 6}

Se exponen los resultados del experimento 6 donde se trabajó con un modelo basado en una arquitectura de redes convolucionales y LSTM, alimentado por espectrogramas.

\paragraph{Procesado de los datos y Evaluación} La distribución de las clases en el conjunto usado para este experimento (TESS y SAVEE) queda como se muestra en la tabla \ref{ref:balance6}, mientras que en la tabla \ref{result_Test6} se muestran las métricas de éxito por cada una de las clases.


\begin{table}[!htb]
	\centering
	\setlength\tabcolsep{4pt}
	\begin{minipage}[b]{0.48\textwidth}
		\centering
		%\tablewidth=\textwidth
		\begin{tabular}[b]{ c  c }
			\hline
			\multicolumn{2}{ c }{TESS y SAVEE} \\
			\hline
			enfado 		& 460 \\
			asco 		& 460 \\
			miedo 		& 460 \\
			felicidad 	& 460 \\
			tristeza 	& 460 \\
			neutral 	& 520 \\
			\hline
			& \\
		\end{tabular}
		\caption{Distribución de los datos en las pruebas del experimento 6.}
		\label{ref:balance6}
	\end{minipage}%
	\hfill
	\begin{minipage}[b]{0.48\textwidth}
		\centering
			\begin{tabular}[b]{| c|| c c | }
			\hline
			\multicolumn{1}{|c||}{\textbf{Emoción}} & 
			\multicolumn{2}{c|}{\textbf{CNN-LSTM}} \\
			\hline
			& 
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}\\
			\hline
			enfado 		& 0.96 & 0.94  \\
			asco 		& 0.67 & 0.80  \\
			miedo 		& 0.99 & 0.93  \\
			felicidad 	& 1.00 & 0.95  \\
			tristeza 	& 0.99 & 0.91  \\
			neutral 	& 0.96 & 0.91  \\
			\hline
			\textbf{accuracy} 	& \multicolumn{2}{c|}{\textbf{92.06\%}}\\
			\hline
		\end{tabular}
		\caption{Resultados del experimento 6 usando un arquitectura CNN-LSTM.}
		\label{result_Test6}
	\end{minipage}
\end{table}
	

\paragraph{Entrenamiento} El rendimiento del correspondiente modelo se puede visualizar a través de la gráfica de loss y accuracy en la figura \ref{fig:lstm-cnn}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{test4_5/savee_tess_lstmcnn2d.png} 
	\caption{Rendimiento del modelo CNN-LSTM usando los datos de SAVEE y TESS}
	\label{fig:lstm-cnn}
\end{figure}



\section{Pruebas con lenguajes extranjeros}
\label{cap:5IdiomaExtranjero}

\subsection{Experimento 7}
Se exponen los resultados del experimento 7 que evaluó los tres mejores modelos resultantes del bloque anterior en el idioma alemán, usando como conjunto de test la base de datos EMO-DB.

\paragraph{Preprocesado de datos}
La distribución de las clases en el conjunto de test usado para este experimento (EMO-DB) queda como se muestra en la tabla \ref{ref:balance7}.

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{ c  c | c  c |c  c }
			\hline
			\multicolumn{2}{c|}{CNN 1D}&
			\multicolumn{2}{c|}{CNN 2D} &
			\multicolumn{2}{c}{CNN-LSTM} \\
			\hline
			enfado 		& 92 & enfado 		& 92 & enfado 	&  92\\
			asco 		& 92 & asco 		& 92 & asco 	& 92 \\
			miedo 		& 92 & miedo 		& 92 & miedo 	& 92 \\
			felicidad 	& 92 & felicidad 	& 92 & felicidad& 92 \\
			tristeza 	& 92 & tristeza 	& 92 & tristeza & 92 \\
			neutral 	& 92 & neutral 		& 92 & neutral 	& 92\\
			\hline
		\end{tabular}
		
		\caption{Distribución resultante de las clases en la base de datos EMO-DB.}
		\label{ref:balance7}
	\end{center}
\end{table}

\paragraph{Evaluación}
En la tabla \ref{result_Test7} se muestran las métricas de éxito por cada una de las clases, en los tres modelos.

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c|| c c |  c c |  c c | }
			\hline
			\multicolumn{1}{|c||}{\textbf{Emoción}} & 
			\multicolumn{2}{c|}{\textbf{CNN 1D}}&
			\multicolumn{2}{c|}{\textbf{CNN 2D}} &
			\multicolumn{2}{c|}{\textbf{CNN-LSTM}} \\
			\hline
			& 
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}\\
			\hline
			
			enfado 		& 0.16 & 0.26 & 0.17 & 0.18 & 0.18 & 0.21	\\
			asco 		& 0.25 & 0.20 & 0.06 & 0.04 & 0.10 & 0.12	\\
			miedo 		& 0.35 & 0.21 & 0.30 & 0.40 & 0.22 & 0.19	\\
			felicidad	& 0.56 & 0.36 & 0.21 & 0.25 & 0.24 & 0.26	\\
			tristeza 	& 0.36 & 0.08 & 0.26 & 0.10 & 0.38 & 0.22	\\
			neutral 	& 0.26 & 0.17 & 0.21 & 0.17 & 0.16 & 0.16	\\
			\hline
			\textbf{accuracy} 	& \multicolumn{2}{c|}{\textbf{32\%}} & \multicolumn{2}{c|}{\textbf{20\%}}  &\multicolumn{2}{c|}{\textbf{22\%}} \\
			\hline
		\end{tabular}
		
		\caption{Resultados de evaluar los mejores modelos en el idioma alemán.}
		\label{result_Test7}
	\end{center}
\end{table}

\subsection{Experimento 8}
Se exponen los resultados del experimento 8 que evaluó los tres mejores modelos resultantes del bloque anterior en el idioma francés, usando como conjunto de test la base de datos CaFE.


\paragraph{Preprocesado de datos}
La distribución de las clases en el conjunto de test usado para este experimento (CaFE) queda como se muestra en la tabla \ref{ref:balance8}.

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{ c  c | c  c |c  c }
			\hline
			\multicolumn{2}{c|}{CNN 1D}&
			\multicolumn{2}{c|}{CNN 2D} &
			\multicolumn{2}{c}{CNN-LSTM} \\
			\hline
			enfado 		& 92 & enfado 	& 92 & enfado 		& 92 \\
			asco 		& 92 & asco 	& 92 & asco 		& 92 \\
			miedo 		& 92 & miedo 	& 92 & miedo 		& 92 \\
			felicidad 	& 92 & felicidad & 92 & felicidad	& 92 \\
			tristeza 	& 92 & tristeza & 92 & tristeza 	& 92 \\
			neutral 	& 92 & neutral 	& 92 & neutral 		& 92 \\
			\hline
		\end{tabular}
		
		\caption{Distribución resultante de las clases en la base de datos CaFE.}
		\label{ref:balance8}
	\end{center}
\end{table}


\paragraph{Evaluación}
En la tabla \ref{result_Test8} se muestran las métricas de éxito por cada una de las clases, en los tres modelos.

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c|| c c |  c c |  c c | }
			\hline
			\multicolumn{1}{|c||}{\textbf{Emoción}} & 
			\multicolumn{2}{c|}{\textbf{CNN 1D}}&
			\multicolumn{2}{c|}{\textbf{CNN 2D}} &
			\multicolumn{2}{c|}{\textbf{CNN-LSTM}} \\
			\hline
			& 
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}\\
			\hline
			
			enfado 		& 0.14 & 0.22 & 0.20 & 0.30 & 0.20 & 0.26 \\
			asco 		& 0.26 & 0.15 & 0.17 & 0.13 & 0.21 & 0.22 \\
			miedo 		& 0.00 & 0.00 & 0.13 & 0.12 & 0.11 & 0.10 \\
			felicidad	& 0.23 & 0.18 & 0.17 & 0.15 & 0.18 & 0.25 \\
			tristeza 	& 0.35 & 0.11 & 0.14 & 0.05 & 0.21 & 0.11 \\
			neutral 	& 0.10 & 0.04 & 0.18 & 0.10 & 0.35 & 0.11 \\
			\hline
			\textbf{accuracy} 	& \multicolumn{2}{c|}{\textbf{18\%}} & \multicolumn{2}{c|}{\textbf{18\%}}  &\multicolumn{2}{c|}{\textbf{21\%}} \\
			\hline
		\end{tabular}
		
		\caption{Resultados de evaluar los mejores modelos en el idioma francés.}
		\label{result_Test8}
	\end{center}
\end{table}


\chapter{Discusión y análisis de los resultados}


El objetivo de los dos primeros experimentos fue probar el comportamiento del método elegido para el desarrollo inicial de las pruebas con el conjunto de datos RAVDESS. Aunque la observación de los datos llevó a la conclusión de que una separación por género tendría sentido, los datos no fueron consistentes, dando las dos divisiones, porcentajes de exactitud muy diferentes además de un enorme desajuste. Respecto al segundo experimento, formado por las pruebas 2.A, 2B, 2.C y 2.D trataron de comparar el resultado aplicando diferentes técnicas de aumento de datos al conjunto de datos RAVDESS, que si bien los frutos de esta prueba no arrojaron unos resultados satisfactorios, condujeron a una sólida reducción del overfitting. 
No se pasa por alto, el hecho de que el uso de ruido blanco llega a ser contraproducente, debido principalmente a la incapacidad de reconocer la Tristeza y la Neutralidad, y por mostrar una enorme diferencia entre las métricas accuracy y F1. Aquí se comprueba la sensibilidad de las características MFCC al ruido de fondo. De la misma manera cabe mencionar que la Neutralidad en esta prueba tuvo menor representación, lo que claramente impactó en el resultado.

%El uso de aumento de datos es contraproducente.

Comparando estos resultados con una configuración similar (clasificación de emociones en RAVDESS con redes convolucionales unidimensionales) en la revisión del estado del arte, se sabe que \citep{AbdulQayyum2019} consiguen un 81.63\% con una arquitectura de siete capas convolucionales unidimensionales usando una base de datos similar en inglés con siete emociones (SAVEE). Por otro lado, \citep{Mustaqeem2020} implementa una red CNN bidimensional de siete capas usando espectrogramas MFCC para llegar al 81.01\% de accuracy en RAVDESS. \\


Siguiendo por el experimento 3, el objetivo fue probar un enfoque distinto para aumentar el número de elementos con los que la red entrenará teniendo en cuenta que el modelo resultante se probaría finalmente con un idioma que no ha sido visto en el entrenamiento. Se propuso un ensamblado de conjuntos de datos en el mismo idioma, aportando una mayor diversidad a los datos, y más capacidad de aprendizaje a la red. 

Paradójicamente, el que peor resultados logró fue la combinación con mayor número de características RAVEE-TESS-SAVEE, mientras que TESS y SAVEE con 2824 muestras, consiguieron un 87.19\% frente al 64.08\%  de la combinación de los tres conjuntos de datos con 3476 muestras.

El conjunto de datos que se había usado en los experimentos previos (RAVDESS)  únicamente contempla 2 tipos de frases, lo que apunta a que el bajo rendimiento de esas pruebas se deba a la poca diversidad de los datos.


Cabe resaltar que el desempeño del modelo con redes convolucionales unidimensionales compite con los resultados reportados por \citep{AbdulQayyum2019}, \citep{Mustaqeem2020}, y \citep{Anvarjon2020}, los cuales con arquitecturas más complejas consiguen menor exactitud.



%Cabe resaltar el avance con respecto a las otras pruebas, y los resultados sugieren que la diversidad de datos es lo que conlleva un mejor rendimiento del modelo.

%[...]parece claro que el aumento del conjunto de datos es la dirección por la que apostar, ya que ha reducido notablemente el sobre ajuste que presentaban las primeras pruebas.

Analizando los resultados hasta ahora, da la sensación que la técnica de aumento de datos no consigue aportar demasiado, por lo que para corroborar esta hipótesis, el experimento 4  repitió esta técnica en la configuración del experimento anterior. Se pudo ver que los resultados no arrojaron demasiadas diferencias en comparación con su versión no aumentada, salvo en el combinación de los tres conjuntos (RAVDESS, TESS y SAVEE) donde se tuvo que casi triplicar el tamaño para que hubiera una mejoría del 17.73\% en el accuracy: % que marcó una mejora de un noseque% blablabla
 
\begin{itemize}
	\item RAVDESS y TESS pasaron de 3056 muestras con un 71.89\% de precisión, a 6112 instancias con 71.56\% en su versión aumentada.
	
	\item TESS y SAVEE pasaron de sólo 2824 muestras con un 88.22\% de precisión, a 5648 instancias con un 88.19\%.
	
	\item RAVDESS, TESS y SAVEE, sin embargo, pasaron de un 64.08\% con 3476 muestras a un 81.81\% con 10112 instancias.
\end{itemize}

%La importancia de la variabilidad en los datos se hace evidente, donde a la hora de incrementar la accuracy, apostar por esta característica es más eficiente que técnicas más tradicionales de aumento de datos.

Los experimentos 5 y 6 exploraron el uso de espectrogramas MFCC como estrategia. Este enfoque consiguió los mejores resultados: 92.06\% en CNN-LSTM en primer lugar y CNN con un 91.50\% en una clasificación de seis emociones en inglés. \\


\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c|| c c |  c c |  c c | c c | }
			\hline
			\multicolumn{1}{|c||}{\textbf{Emoción}} & 
			\multicolumn{2}{c|}{\textbf{CNN 1D}}&
			\multicolumn{2}{c|}{\textbf{CNN 2D}} &
			\multicolumn{2}{c|}{\textbf{CNN-LSTM}} \\
			\hline
			& 
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}\\
			\hline
			
			enfado 		& 0.51  & 0.66  & 0.86 & 0.89 & \textbf{0.96} & \textbf{0.94}  \\
			asco 		& 0.92  & 0.83  & 0.90 & 0.91 & \textbf{0.67} & \textbf{0.80}  \\
			miedo 		& 1.00  & 0.86  & 0.94 & 0.93 & \textbf{0.99} & \textbf{0.93}  \\
			felicidad	& 0.94  & 0.91  & 0.94 & 0.94 & \textbf{1.00} & \textbf{0.95}  \\
			tristeza 	& 1.00  & 0.79  & 0.96 & 0.92 & \textbf{0.99} & \textbf{0.91}  \\
			neutral 	& 1.00  & 0.85  & 0.88 & 0.89 & \textbf{0.96 }& \textbf{0.91}  \\
			\hline
			\textbf{accuracy} 	& 
			 \multicolumn{2}{c|}{\textbf{88.22\%}} &
			\multicolumn{2}{c|}{\textbf{90.50\%}} &
			\multicolumn{2}{c|}{\textbf{92.06\%}} \\
			\hline
		\end{tabular}
		\caption{Comparación de los tres mejores modelos resultantes en el idioma inglés.}
		\label{comparaTable}
	\end{center}
\end{table}
La tabla \ref{comparaTable} compara los resultados de los tres mejores modelos de los experimentos realizados.
La efectividad del uso de espectrogramas en un clasificador mono-lingüístico también se refleja en cómo de estables son los valores de accuracy y F1, ya que presentan menor variación entre ellos.\\

En la tabla \ref{comparaTable1} se expone una comparación de los principales trabajos que se han llevado a cabo en \textbf{el idioma inglés} con el mejor modelo propuesto (CNN-LSTM). 

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c|| c c |  c c |  c c | c c | }
			\hline
			& 
			\multicolumn{2}{c|}{\textbf{Abdul 2020}} &
			\multicolumn{2}{c|}{\textbf{Mustaqeem 2020}}&
			\multicolumn{2}{c|}{\textbf{Anvarjon 2020}} &
			\multicolumn{2}{c|}{\textbf{Modelo propuesto}} \\
			
			\multicolumn{1}{|c||}{\textbf{Emoción}}& 
			\multicolumn{2}{c|}{SAVEE} &
			\multicolumn{2}{c|}{IEMOCAP}&
			\multicolumn{2}{c|}{IEMOCAP} &
			\multicolumn{2}{c|}{SAVEE y TESS} \\
			\hline
			& 
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}&
			\multicolumn{1}{c|}{Accuracy}&\multicolumn{1}{c|}{F1}\\
			\hline
			
			enfado 		& 0.85  & 0.79  & 0.87 & 0.80 & 0.68 & 0.76 & \textbf{0.96} & \textbf{0.94}  \\
			asco 		& 0.82  & 0.82  &   -  &   -  & -    &  -   & \textbf{0.67} & \textbf{0.80}  \\
			miedo 		& 0.85  & 0.82  &   -  &   -  & -    &  -   & \textbf{0.99} & \textbf{0.93}  \\
			felicidad	& 0.83  & 0.85  & 0.97 & 0.91 & 0.85 & 0.73 & \textbf{1.00} & \textbf{0.95}  \\
			tristeza 	& 0.83  & 0.86  & 0.82 & 0.84 & 0.74 & 0.75 & \textbf{0.99} & \textbf{0.91}  \\
			sorpresa 	&  0.84 &  0.86 &   -  &    -  &  -  &   -  &   -  &   -   \\
			neutral 	& 0.83  & 0.85  & 0.77 & 0.83 & 0.81 & 0.80 & \textbf{0.96} & \textbf{0.91}  \\
			\hline
			\textbf{accuracy} 	& 
			\multicolumn{2}{c|}{\textbf{84.01\%}} & \multicolumn{2}{c|}{\textbf{84\%}}  &
			\multicolumn{2}{c|}{\textbf{77.01\%}} &
			\multicolumn{2}{c|}{\textbf{92.06\%}} \\
			\hline
		\end{tabular}
		\caption{Comparación de los trabajos presentados en la revisión de la literatura en el idioma inglés.}
		\label{comparaTable1}
	\end{center}
\end{table}


\begin{comment}
	Teniendo en cuenta la progresión de los experimentos hasta llegar a este modelo (Prueba 3.B del experimento 3), hay varios aspectos a resaltar:
	\begin{itemize}
	\item El uso del regularizador L2 en las capas de salida junto el uso de \textit{ReduceLROnPlateau} monitorizando la pérdida, redujo el overfitting y aumentó el rendimiento permitiendo al modelo llegar a mejores resultados en menos épocas.
	
	\item La combinación de los dos conjuntos de datos que más diversidad aportaban (SAVEE con siete tipos de frases distintas y TESS con un total de 200 frases donde cambian la última palabra en cada una) fue un factor clave en el aprendizaje del modelo.
	\end{itemize}
\end{comment}

%En relación al modelo resultante del experimento 5, u
Un aspecto que llama la atención, es que en todas las pruebas que se hicieron en un modelo basado únicamente en redes convolucionales entrenado y evaluado en inglés, el Enfado fue la emoción más complicada de distinguir. Esto sugiere un patrón para este lenguaje, lo que encaja con los resultados encontrados en la revisión del estado del arte.

Se han extraído los espectrogramas de Mel de seis emociones pertenecientes a SAVEE (figura \ref{fig:melCompare}) y se comparan con las observaciones hechas en \citep{Huang2013}.



\begin{figure}[H]
	%\begin{adjustwidth}{-1cm}{}
	\minipage{0.33\textwidth}
	\centering
	\includegraphics[width=\linewidth]{/emotions/mel_angry.PNG}
	{{\small Enfado}}
	\endminipage
	\hfill
	\minipage{0.33\textwidth}
	\centering
	\includegraphics[width=\linewidth]{/emotions/mel_disgust.PNG}
	{{\small Asco}}
	\endminipage
	\hfill
	\minipage{0.33\textwidth}%
	\centering
	\includegraphics[width=\linewidth]{/emotions/mel_fear.PNG}
	{{\small Miedo}}
	\endminipage
	\vskip\baselineskip
	
	%================
	%   2a FILA
	%================
	
	\minipage{0.33\textwidth}
	\centering
	\includegraphics[width=\linewidth]{/emotions/mel_happiness.PNG}
	{{\small Felicidad}}
	\endminipage
	\hfill
	\minipage{0.33\textwidth}
	\centering
	\includegraphics[width=\linewidth]{/emotions/mel_sad.PNG}
	{{\small Tristeza}}
	\endminipage
	\hfill
	\minipage{0.33\textwidth}%
	\centering
	\includegraphics[width=\linewidth]{/emotions/mel_neutral.PNG}
	{{\small Neutral}}
	\endminipage
	
	\caption{Espectrogramas de Mel de las emociones pertenecientes a SAVEE. Fuente Propia}
	\label{fig:melCompare}
%	\end{adjustwidth}
\end{figure}



La figura \ref{fig:melCompare} muestra la representación de la banda del espectro de frecuencia de las seis emociones trabajadas en este estudio procedentes de la misma base de datos (SAVEE). Estos espectrogramas visualizan el cambio de frecuencia de una señal no estacionaria \citep{scipySignal}.\\ %Se debe señalar que esta información está contenida en los espectrogramas con los que se alimenta la red, pero 

Posteriormente la figura \ref{fig:plotsEmotions} muestra las variaciones en la intensidad y frecuencia de algunas de las emociones que forman la base de datos TESS.




\begin{figure}[H]
	%\begin{adjustwidth}{-1cm}{}
	\minipage{0.50\textwidth}
		\centering
		\includegraphics[width=\linewidth]{/emotions/plot_anger.jpg}
		{{\small Enfado}}
	\endminipage
	\hfill
	\minipage{0.50\textwidth}
		\centering
		\includegraphics[width=\linewidth]{/emotions/plot_fear.jpg}
		{{\small Miedo}}
	\endminipage
	\hfill
	%================
	%   2a FILA
	%================
	\minipage{0.50\textwidth}
		\centering
		\includegraphics[width=\linewidth]{/emotions/plot_happiness.jpg}
		{{\small Felicidad}}
	\endminipage
	\hfill
	\minipage{0.50\textwidth}
		\centering
		\includegraphics[width=\linewidth]{/emotions/plot_neutral.jpg}
		{{\small Neutral}}
	\endminipage
	\hfill

	\caption{Onda acústica, frecuencia e intensidad de cuatro emociones en TESS. Fuente: \citep{Huang2013}}
	\label{fig:plotsEmotions}
	%	\end{adjustwidth}
\end{figure}
% https://link.springer.com/article/10.1007/s40747-021-00295-z

%https://medium.com/analytics-vidhya/getting-started-with-speech-emotion-recognition-visualising-emotions-704a1dc50d84

%https://www.researchgate.net/post/Why_we_take_only_12-13_MFCC_coefficients_in_feature_extraction
Respecto a las figuras sobre estas líneas se resalta lo siguiente:

\begin{itemize}
	\item El Miedo y la Felicidad tienen una duración más corta, mientras que el Enfado, la Tristeza y la Neutralidad se expanden más en el tiempo (en la figura \ref{fig:melCompare}, menos zonas oscuras en el eje \textit{X}). 
	
	\item El Enfado y el Miedo tienen mayor amplitud, que se puede ver en el en la representación de la onda acústica de cada emoción en la figura \ref{fig:plotsEmotions}.
	
	\item La emoción del Enfado mantiene más constante la frecuencia en la línea de tiempo y su variación en la intensidad es más suave en comparación con las otras emociones (figura \ref{fig:plotsEmotions}).
\end{itemize}

Estas observaciones parecen indicar que el factor más distinguible respecto al Enfado es cómo sus características varían en el tiempo, lo que tiene sentido con el hecho de que este patrón no se continúe en los resultados del experimento 6 con el modelo CNN-LSTM, ya que este sistema permite aprender la relevancia temporal de cada secuencia del habla. %la última capa tiene en cuenta información de un tiempo previo.



Un enfoque basado puramente en redes convolucionales aprende a diferenciar patrones a través del espacio. Sin embargo la señal acústica es continua en el domino de tiempo, de forma que las características emocionales percibidas en cada segmento no pueden ser tratadas de manera aislada. LSTM resuelve ese problema aumentando la información entre esas ventanas de tiempo, lo cual ayuda a reflejar una continuidad temporal de las características.\\

La tabla \ref{comparaTable2} compara los tres mejores trabajos (con cualquier lenguaje) expuestos en la revisión del estado del arte con el mejor modelo propuesto (CNN-LSTM). 

\begin{table}[H]
	\centering
	\begin{center}
		\begin{tabular}{| c | c | c| c |}
			\hline
			\textbf{Trabajo} &  \textbf{Método} &  \textbf{Datos usados}  & \textbf{Acierto} \\ 
			\hline\hline
			Anvarjon, 2020 & CNN 2D + espect. Mel & EMOD-DB & 92.01\%\\  
			Harar, 2017    &  CNN      & EMOD-DB (3 emociones) &  96.97\%	\\
			Tamulevicius, 2020 & CNN 2D + cocleogramas & Lithuanian & 97.00\% \\
		\textbf{Modelo propuesto} & CNN-LSTM + espect. MFCC & TESS+SAVEE & \textbf{92.06\%} \\
			%& & & &
			\hline	
		\end{tabular}
		
		\caption{Comparación de los tres mejores trabajos presentados en la revisión de la literatura con el modelo propuesto.}
		\label{comparaTable2}
	\end{center}
\end{table}




Finalmente, tal y como se predijo para los experimentos 7 y 8, se obtuvo un porcentaje de accuracy un poco mayor en la evaluación con alemán (lengua con raíces fonéticas más próximas al idioma que se usó en el entrenamiento), que en la evaluación con francés (lengua con raíces fonéticas más lejanas al idioma que se usó en el entrenamiento). 
En el experimento 7 se evaluó el rendimiento de los tres mejores modelos en el idioma alemán utilizando como test la base de datos EMO-DB. En un primer momento se pudo observar en el accuracy el mismo patrón sobre la dificultad para reconocer la emoción del Enfado, pero comprobando la métrica de F1 en el modelo CNN 1D, las emociones Neutral y Tristeza bajan de un 26\% a un 17\% y de un 36\% a un 8\% respectivamente. La exactitud general de las tres pruebas, tampoco mantiene la misma lógica que los modelos evaluados en inglés, ya que aquí fue el modelo basado en redes convolucionales unidimensionales, el que mejor porcentaje de exactitud consiguió.

No se pudieron percibir las mismas observaciones en el experimento 8, que evaluaba el idioma francés usando como conjunto de test la base de datos CaFE en los tres mejores modelos entrenados y evaluados en inglés. Por un lado, no fue posible encontrar un patrón sobre la emoción más difícil (o más fácil) de reconocer y por otro el modelo CNN-LSTM tuvo mayor porcentaje de exactitud con un 21\%.


En la tabla \ref{comparaTable3} se comparan los resultados de este trabajo con el de \citep{Tamulevicius2020} tras haber evaluado un modelo entrenado en una lengua con otras distintas (con espectrogramas y con cocleogramas). 
 
\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c||c c|c c|c c|}
			\hline
			\textbf{Idioma Test} & 
			\multicolumn{2}{c|}{\textbf{Tamulevicius 2020}} & 
			\multicolumn{2}{c|}{\textbf{Tamulevicius 2020}} &
			\multicolumn{2}{c|}{\textbf{Modelo propuesto}} \\
			&
			\multicolumn{2}{c|}{espectrogramas} & 
			\multicolumn{2}{c|}{cocleogramas} &
			&\\
			\hline
			&
			\multicolumn{1}{c|}{Accuracy} & \multicolumn{1}{c|}{F1} & 
			\multicolumn{1}{c|}{Accuracy} & \multicolumn{1}{c|}{F1} &
			\multicolumn{1}{c|}{Accuracy} & \multicolumn{1}{c|}{F1} \\
			\hline
			Serbio  & 0.37 & 0.18 & 0.41 & 0.36 &  - &  -  \\
			Polaco  & 0.21 & 0.17 & 0.20 & 0.19 &  - &  -  \\
			Alemán  & 0.49 & 0.27 & 0.42 & 0.40 &  0.19 & 0.19   \\
			Español & 0.3  & 0.19 & 0.35 & 0.20 &  - &  -  \\
			Francés &   -  &   -  &  - &  - & 0.20 &  0.18  \\
			\hline
		\end{tabular}
	\end{center}
\caption{Comparación de los modelos evaluados en lenguas extranjeras.}
\label{comparaTable3}
\end{table}

Tamulevicius evalúa en las lenguas serbio, polaco, alemán y español un modelo que ha aprendido lituano. El lituano, usado como idioma de referencia es una lengua con raíces bálticas, las cuales mantienen cierto grado de similitud fonética con las lenguas eslavas \citep{Kortlandt1983}, y sin embargo el polaco (lengua eslava), no es donde consigue un porcentaje de acierto más elevado.
Atendiendo a los datos que se muestran en la tabla \ref{comparaTable3}, no parece que se pueda establecer una relación de proximidad fonética entre distintos idiomas para la clasificación de emociones en la lengua extranjera siguiendo un enfoque basado en redes convolucionales.


%Este estudio también contempla el uso de cocleogramas, que mejoran levemente la puntuación en cada lenguaje, pero en la tabla \ref{comparaTable3} sólo se muestran los resultados de haber usado espectrogramas, ya que por los ajustados tiempos de entrega y falta de apoyo por parte de la literatura
Por último, se desea analizar el hecho de que el modelo no fuera capaz de abstraer la información extraída de las características para clasificar emociones en otras lenguas. Haciendo una revisión de lo aprendido en este trabajo, se hacen algunas observaciones sobre por qué, usando espectrogramas que han resultado ser tan exitosos para la clasificación en una sola lengua, no se comportan de la misma manera en otros idiomas:

\begin{itemize}
	\item Los objetos visuales y los sonidos no se agrupan en una imagen de la misma manera. Cuando en una imagen hay un píxel de un determinado color se puede asumir que pertenece a un determinado objeto. Por el contrario, los cambios instantáneos en las características del sonido como el tono o la intensidad, no se separan en capas distinguibles en un espectrograma. Esto quiere decir que no se puede asumir que una determinada frecuencia representada en un espectrograma pertenezca a un determinado tipo de sonido (por ejemplo, podría estar producida por la interacción de varias ondas). En definitiva, esto hace que separar sonidos simultáneos en espectrogramas tal y como lo se hace con objetos opacos en imágenes, sea especialmente difícil \citep{Wyse2017}. %https://towardsdatascience.com/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd
	En la figura \ref{fig:spectroEmo} se muestran tres emociones pertenecientes a las bases de datos que se han usado en los experimentos. Como puede verse, es difícil establecer un patrón común entre los idiomas para la misma emoción.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{spectrogramsEmotions.JPG} 
		\caption{Espectrogramas de tres emociones en tres bases de datos distintas usadas en este trabajo. Fuente: Propia.}
		\label{fig:spectroEmo}
	\end{figure}
	
	\item Los ejes \textit{X} e \textit{Y} de una imagen no tienen el mismo significado que en un espectrograma. Por ejemplo, un perro en una foto, seguiría siendo un perro independientemente si se mueve horizontal o verticalmente en la imagen; Es decir, la información representada \textbf{no cambia su significado}. Sin embargo esto no ocurre de la misma manera en un espectrograma donde estas dos dimensiones representan unidades distintas: frecuencia y tiempo \citep{Verma2018}.
	%\citep{spectrograms2021}.
	
	\item Desde el punto de vista humano, la forma en la que se procesan esas señales no es comparable. A la hora de localizar un objeto de manera visual en el entorno, se escanea lo que hay alrededor varias veces, ya que los objetos que lo conforman son estáticos. 
	Por otro lado, un sonido toma la forma física de la presión manifestada en la onda, y desde el punto de vista de quien la percibe, esa determinada onda con un determinado estado sólo existe en un momento específico. Dicho de otro modo, la información que contiene una onda de audio está dispuesta de manera secuencial
	\citep{JHui2019}.
	
\end{itemize} 


\chapter{Conclusiones y Trabajo Futuro}

Para finalizar esta tesis de máster, se reúnen a continuación las conclusiones en base a los objetivos establecidos en el capítulo 3.

Respecto al primer objetivo específico, el cual consistía en una revisión en profundidad de la literatura actual, se abordó en el capítulo 2. Se llegó a la conclusión de que no existía una línea definida sobre qué métodos, técnicas y datos  eran los más adecuados, para abordar este problema. Añadir además que, si bien el número de artículos sobre el reconocimiento de emociones en una lengua es abundante, no existe un oferta tan extensa para abordar el problema del reconocimiento de emociones en la lengua extranjera. Debido a esto, se decidió apostar por las técnicas que más apoyo tenían por parte de la literatura.


De esta manera, una vez se reunieron los conjuntos de datos que atendían a las condiciones establecidas en el capítulo 3, se pasó al siguiente punto de los objetivos específicos donde se debía diseñar una solución cuyo porcentaje de exactitud fuese superior a un 81\%. Los tres mejores modelos desarrollados en este trabajo consiguieron una puntuación por encima de la marca que se propuso, donde la diversidad de datos jugó un papel determinante. Esta estrategia no sólo redujo el overfitting, si no que hizo posible un modelo más rentable, ya que en comparación a las arquitecturas de otros trabajos, los tres modelos propuestos conseguían mejor porcentaje de acierto con diseños más simples.
En el desarrollo de los modelos se comprobó que la generación de datos sintéticos podía ser contraproducente (especialmente con ruido blanco). Por otra parte quedó clara la superioridad del uso de espectrogramas para un clasificador de emociones mono-lenguaje, especialmente con una arquitectura híbrida CNN-LSTM ya que tiene en cuenta la información temporal inherente a la señal de audio.

Por lo que respecta a los experimentos hechos evaluando una lengua extranjera y atendiendo al último punto de los objetivos, se ha llegado a las mismas conclusiones que Tamulevicius \citep{Tamulevicius2020} donde tras hacer diversas pruebas con diferentes idiomas, únicamente puede reconocer aquellos con los que entrena su modelo. No obstante, extrayendo lo aprendido en este trabajo, se desaconseja tratar la señal acústica con imágenes estáticas (espectrogramas) por no adaptarse correctamente a cómo estas funcionan y perder información importante.

La estrategia que se planteó donde las lenguas extranjeras se dividían según su similitud fonética parece no ser muy relevante coincidiendo con \citep{Pell2008} donde afirma que para reconocer el estado emocional en una lengua no aprendida, se necesita mayor exposición a esta.\\



Debido a los ajustados tiempos de entrega, el alcance de este proyecto se ha debido simplificar, por lo que se plantean líneas de trabajo futuras:

En este trabajo se han explorado el reconocimiento de emociones en la voz usando características cepstrales, ya que por la revisión del estado del arte, se asumió que eran de las que mejor funcionaban. Aunque los resultados del clasificador sobre una sola lengua, estuvieron a la altura de otros estudios, atendiendo al análisis previo en este mismo capítulo, se considera que esta no es la mejor manera de abordar ese problema.

Por un lado se cree que sería recomendable no tratar las emociones como categorías discretas, ya que varían enormemente de un lenguaje a otro. Esto requiere un análisis más en profundidad sobre la fonética en el idioma en cuanto a la expresión emocional para conseguir una mayor independencia cultural.

Por otro lado, y aunque el tiempo dedicado a la tesis no lo ha permitido, sería interesante explorar la combinación mecanismos de atención junto al punto anterior, ya que podrían computar segmentos relevantes de la señal de audio y conseguir así mejor generalización.% \citep{Yoon2019}. 







%\begin{thebibliography}{a}
%\bibitem{etiqueta} \textsc{Autores},
%\textit{nombre referencia.}
%Información addicional
%\end{thebibliography}
%\bibliographystyle{plain} 
%\bibliography{bibliografia}


\printbibliography

%\bibliographystyle{apacite}
%\bibliography{./bibLib/referenceLib.bib}

\appendix
\chapter{Apéndices}
%Atención, deberá generar un pdf con la plantilla de artículo y añadirla como anexo utilizando includepdf.
\setboolean{@twoside}{false}
\includepdf[pages=-,  offset=75 -75]{articuloLuisa.pdf} %descomentar cuándo esté el artículo terminado.
\end{document}





















