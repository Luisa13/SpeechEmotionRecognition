@article{Pell2001,
abstract = {Preliminary data were collected on how emotional qualities of the voice (sad, happy, angry) influence the acoustic underpinnings of neutral sentences varying in location of intra-sentential focus (initial, final, no) and utterance "modality" (statement, question). Short (six syllable) and long (ten syllable) utterances exhibiting varying combinations of emotion, focus, and modality characteristics were analyzed for eight elderly speakers following administration of a controlled elicitation paradigm (story completion) and a speaker evaluation procedure. Duration and fundamental frequency (f0) parameters of recordings were scrutinized for "keyword" vowels within each token and for whole utterances. Results generally re-affirmed past accounts of how duration and f0 are encoded on key content words to mark linguistic focus in affectively neutral statements and questions for English. Acoustic data on three "global" parameters of the stimuli (speech rate, mean f0, f0 range) were also largely supportive of previous descriptions of how happy, sad, angry, and neutral utterances are differentiated in the speech signal. Important interactions between emotional and linguistic properties of the utterances emerged which were predominantly (although not exclusively) tied to the modulation of f0; speakers were notably constrained in conditions which required them to manipulate f0 parameters to express emotional and nonemotional intentions conjointly. Sentence length also had a meaningful impact on some of the measures gathered.},
author = {Pell, Marc D.},
doi = {10.1121/1.1352088},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Influence{\_}of{\_}emotion{\_}and{\_}focus{\_}location.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {1668--1680},
title = {{Influence of emotion and focus location on prosody in matched statements and questions}},
volume = {109},
year = {2001}
}
@article{Pell2011,
abstract = {To inform how emotions in speech are implicitly processed and registered in memory, we compared how emotional prosody, emotional semantics, and both cues in tandem prime decisions about conjoined emotional faces. Fifty-two participants rendered facial affect decisions (Pell, 2005a), indicating whether a target face represented an emotion (happiness or sadness) or not (a facial grimace), after passively listening to happy, sad, or neutral prime utterances. Emotional information from primes was conveyed by: (1) prosody only; (2) semantic cues only; or (3) combined prosody and semantic cues. Results indicated that prosody, semantics, and combined prosody-semantic cues facilitate emotional decisions about target faces in an emotion-congruent manner. However, the magnitude of priming did not vary across tasks. Our findings highlight that emotional meanings of prosody and semantic cues are systematically registered during speech processing, but with similar effects on associative knowledge about emotions, which is presumably shared by prosody, semantics, and faces. {\textcopyright} 2010 Psychology Press.},
author = {Pell, Marc D. and Jaywant, Abhishek and Monetta, Laura and Kotz, Sonja A.},
doi = {10.1080/02699931.2010.516915},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Emotional{\_}speech{\_}processing{\_}Disentanglin.pdf:pdf},
issn = {02699931},
journal = {Cognition and Emotion},
keywords = {Audio-visual priming,Emotions,Facial expression,Speech perception,Vocal cues},
number = {5},
pages = {834--853},
title = {{Emotional speech processing: Disentangling the effects of prosody and semantic cues}},
volume = {25},
year = {2011}
}
@article{Pell2009,
abstract = {To understand how language influences the vocal communication of emotion, we investigated how discrete emotions are recognized and acoustically differentiated in four language contexts-English, German, Hindi, and Arabic. Vocal expressions of six emotions (anger, disgust, fear, sadness, happiness, pleasant surprise) and neutral expressions were elicited from four native speakers of each language. Each speaker produced pseudo-utterances ("nonsense speech") which resembled their native language to express each emotion type, and the recordings were judged for their perceived emotional meaning by a group of native listeners in each language condition. Emotion recognition and acoustic patterns were analyzed within and across languages. Although overall recognition rates varied by language, all emotions could be recognized strictly from vocal cues in each language at levels exceeding chance. Anger, sadness, and fear tended to be recognized most accurately irrespective of language. Acoustic and discriminant function analyses highlighted the importance of speaker fundamental frequency (i.e., relative pitch level and variability) for signalling vocal emotions in all languages. Our data emphasize that while emotional communication is governed by display rules and other social variables, vocal expressions of 'basic' emotion in speech exhibit modal tendencies in their acoustic and perceptual attributes which are largely unaffected by language or linguistic similarity. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Pell, Marc D. and Paulmann, Silke and Dara, Chinar and Alasseri, Areej and Kotz, Sonja A.},
doi = {10.1016/j.wocn.2009.07.005},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Factors{\_}in{\_}the{\_}recognition{\_}of{\_}emotional.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
number = {4},
pages = {417--435},
title = {{Factors in the recognition of vocally expressed emotions: A comparison of four languages}},
volume = {37},
year = {2009}
}
@article{Sarkania2013,
abstract = {Android is a linux based operating system which uses linux kernel. In this paper we will see how the boot up process of android is different from linux and how the different applications in an Android System communicate with each other. As an Android Application is made up of different activities so through this paper we will also come to know about how the activity changes their states and the whole lifecycle of an activity.},
author = {Sarkania, Vaibhav Kumar and Bhalla, Vinod Kumar},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarkania, Bhalla - 2013 - International Journal of Advanced Research in.pdf:pdf},
journal = {Android Internals},
keywords = {- android boot up,activity lifecycle,dalvik vm,zygote},
number = {6},
pages = {143--147},
title = {{International Journal of Advanced Research in}},
volume = {3},
year = {2013}
}
@article{Cheang2011,
abstract = {The goal of the present research was to determine whether certain speaker intentions conveyed through prosody in an unfamiliar language can be accurately recognized. English and Cantonese utterances expressing sarcasm, sincerity, humorous irony, or neutrality through prosody were presented to English and Cantonese listeners unfamiliar with the other language. Listeners identified the communicative intent of utterances in both languages in a crossed design. Participants successfully identified sarcasm spoken in their native language but identified sarcasm at near-chance levels in the unfamiliar language. Both groups were relatively more successful at recognizing the other attitudes when listening to the unfamiliar language (in addition to the native language). Our data suggest that while sarcastic utterances in Cantonese and English share certain acoustic features, these cues are insufficient to recognize sarcasm between languages; rather, this ability depends on (native) language experience.},
author = {Cheang, Henry S and Pell, Marc D},
doi = {10.1075/pc.19.2.02che},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Recognizing{\_}sarcasm{\_}without{\_}language{\_}A{\_}c.pdf:pdf},
issn = {0929-0907},
journal = {Pragmatics {\&} CognitionPragmatics and Cognition},
keywords = {cantonese,communicative intentions,cross-linguistic,sarcasm},
number = {2},
pages = {203--223},
title = {{Recognizing sarcasm without language: A cross-linguistic study of English and Cantonese}},
volume = {19},
year = {2011}
}
@article{BAKIR2018,
abstract = {In several application, emotion recognition from the speech signal has been research topic since many years. To determine the emotions from the speech signal, many systems have been developed. To solve the speaker emotion recognition problem, hybrid model is proposed to classify five speech emotions, including anger, sadness, fear, happiness and neutral. The aim this study of was to actualize automatic voice and speech emotion recognition system using hybrid model taking Turkish sound forms and properties into consideration. Approximately 3000 Turkish voice samples of words and clauses with differing lengths have been collected from 25 males and 25 females. In this study, an authentic and unique Turkish database has been used. Features of these voice samples have been obtained using Mel Frequency Cepstral Coefficients (MFCC) and Mel Frequency Discrete Wavelet Coefficients (MFDWC). Moreover, spectral features of these voice samples have been obtained using Support Vector Machine (SVM). Feature vectors of the voice samples obtained have been trained with such methods as Gauss Mixture Model( GMM), Artifical Neural Network (ANN), Dynamic Time Warping (DTW), Hidden Markov Model (HMM) and hybrid model(GMM with combined SVM). This hybrid model has been carried out by combining with SVM and GMM. In first stage of this model, with SVM has been performed subsets obtained vector of spectral features. In the second phase, a set of training and tests have been formed from these spectral features. In the test phase, owner of a given voice sample has been identified taking the trained voice samples into consideration. Results and performances of the algorithms employed in the study for classification have been also demonstrated in a comparative manner.},
author = {BAKIR, CIGDEM and YUZKAT, MECIT},
doi = {10.17694/bajece.419557},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Speech Emotion Classification and Recognition with different methods for Turkish Language, .pdf:pdf},
issn = {2147-284X},
journal = {Balkan Journal of Electrical and Computer Engineering},
number = {2},
pages = {54--60},
title = {{Speech Emotion Classification and Recognition with different methods for Turkish Language}},
volume = {6},
year = {2018}
}
@article{Kaminska2012,
abstract = {Machine recognition of human emotional states is an essential part in improving man-machine interaction. During expressive speech the voice conveys semantic message as well as the information about emotional state of the speaker. The pitch contour is one of the most significant properties of speech, which is affected by the emotional state. Therefore pitch features have been commonly used in systems for automatic emotion detection. In this work different intensities of emotions and their influence on pitch features have been studied. This understanding is important to develop such a system. Intensities of emotions are presented on Plutchik's cone-shaped 3D model. The k Nearest Neighbor algorithm has been used for classification. The classification has been divided into two parts. First, the primary emotion has been detected, then its intensity has been specified. The results show that the recognition accuracy of the system is over 50{\%} for primary emotions, and over 70{\%} for its intensities.},
author = {Kami{\'{n}}ska, Dorota and Pelikant, Adam},
doi = {10.2478/v10177-012-0024-4},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kami{\'{n}}ska, Pelikant - 2012 - Recognition of human emotion from a speech signal based on plutchik's model.pdf:pdf},
issn = {20818491},
journal = {International Journal of Electronics and Telecommunications},
keywords = {Plutchik's wheel of emotion,emotion detection,speech signal},
number = {2},
pages = {165--170},
title = {{Recognition of human emotion from a speech signal based on plutchik's model}},
volume = {58},
year = {2012}
}
@article{Pell2009a,
abstract = {Expressions of basic emotions (joy, sadness, anger, fear, disgust) can be recognized pan-culturally from the face and it is assumed that these emotions can be recognized from a speaker's voice, regardless of an individual's culture or linguistic ability. Here, we compared how monolingual speakers of Argentine Spanish recognize basic emotions from pseudo-utterances ("nonsense speech") produced in their native language and in three foreign languages (English, German, Arabic). Results indicated that vocal expressions of basic emotions could be decoded in each language condition at accuracy levels exceeding chance, although Spanish listeners performed significantly better overall in their native language ("in-group advantage"). Our findings argue that the ability to understand vocally-expressed emotions in speech is partly independent of linguistic ability and involves universal principles, although this ability is also shaped by linguistic and cultural variables.},
author = {Pell, Marc D. and Monetta, Laura and Paulmann, Silke and Kotz, Sonja A.},
doi = {10.1007/s10919-008-0065-7},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Recognizing{\_}Emotions{\_}in{\_}a{\_}Foreign{\_}Langua.pdf:pdf},
issn = {01915886},
journal = {Journal of Nonverbal Behavior},
keywords = {Affective prosody,Cross-linguistic group study,Cultural factors,Emotional speech processing,Vocal expression},
number = {2},
pages = {107--120},
title = {{Recognizing Emotions in a Foreign Language}},
volume = {33},
year = {2009}
}
@article{Lugovic2016,
abstract = {Affective computing opens a new area of research in computer science with the aim to improve the way how humans and machines interact. Recognition of human emotions by machines is becoming a significant focus in recent research in different disciplines related to information sciences and Human-Computer Interaction (HCI). In particular, emotion recognition in human speech is important, as it is the primary communication tool of humans. This paper gives a brief overview of the current state of the research in this area with the aim to underline different techniques that are being used for detecting emotional states in vocal expressions. Furthermore, approaches for extracting speech features from speech datasets and machine learning methods with special emphasis on classifiers are analysed. In addition to the mentioned techniques, this paper also gives an outline of the areas where emotion recognition could be utilised such as healthcare, psychology, cognitive sciences and marketing.},
author = {Lugovic, S. and Dunder, I. and Horvat, M.},
doi = {10.1109/MIPRO.2016.7522336},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lugovic, Dunder, Horvat - 2016 - Techniques and applications of emotion recognition in speech.pdf:pdf},
isbn = {9789532330885},
journal = {2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2016 - Proceedings},
keywords = {acoustic signal processing,affective computing,emotion recognition,human-computer interaction,linguistic speech features,machine learning,speech analysis},
number = {November 2017},
pages = {1278--1283},
title = {{Techniques and applications of emotion recognition in speech}},
year = {2016}
}
@article{Pell2015,
abstract = {This study used event-related brain potentials (ERPs) to compare the time course of emotion processing from non-linguistic vocalizations versus speech prosody, to test whether vocalizations are treated preferentially by the neurocognitive system. Participants passively listened to vocalizations or pseudo-utterances conveying anger, sadness, or happiness as the EEG was recorded. Simultaneous effects of vocal expression type and emotion were analyzed for three ERP components (N100, P200, late positive component). Emotional vocalizations and speech were differentiated very early (N100) and vocalizations elicited stronger, earlier, and more differentiated P200 responses than speech. At later stages (450-700. ms), anger vocalizations evoked a stronger late positivity (LPC) than other vocal expressions, which was similar but delayed for angry speech. Individuals with high trait anxiety exhibited early, heightened sensitivity to vocal emotions (particularly vocalizations). These data provide new neurophysiological evidence that vocalizations, as evolutionarily primitive signals, are accorded precedence over speech-embedded emotions in the human voice.},
author = {Pell, M. D. and Rothermich, K. and Liu, P. and Paulmann, S. and Sethi, S. and Rigoulot, S.},
doi = {10.1016/j.biopsycho.2015.08.008},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pell et al. - 2015 - Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody.pdf:pdf},
issn = {18736246},
journal = {Biological Psychology},
keywords = {Anxiety,ERPs,Emotional communication,Speech prosody,Vocal expression},
pages = {14--25},
pmid = {26307467},
publisher = {Elsevier B.V.},
title = {{Preferential decoding of emotion from human non-linguistic vocalizations versus speech prosody}},
url = {http://dx.doi.org/10.1016/j.biopsycho.2015.08.008},
volume = {111},
year = {2015}
}
@article{Davletcharova2015,
abstract = {Recognizing emotion from speech has become one the active research themes in speech processing and in applications based on human-computer interaction. This paper conducts an experimental study on recognizing emotions from human speech. The emotions considered for the experiments include neutral, anger, joy and sadness. The distinuishability of emotional features in speech were studied first followed by emotion classification performed on a custom dataset. The classification was performed for different classifiers. One of the main feature attribute considered in the prepared dataset was the peak-to-peak distance obtained from the graphical representation of the speech signals. After performing the classification tests on a dataset formed from 30 different subjects, it was found that for getting better accuracy, one should consider the data collected from one person rather than considering the data from a group of people.},
author = {Davletcharova, Assel and Sugathan, Sherin and Abraham, Bibia and James, Alex Pappachen},
doi = {10.1016/j.procs.2015.08.032},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/1-s2.0-S1877050915021432-main.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Emotion Analysis,Emotion Classification,Mel-Frequency Cepstral Coefficients,Speech Processing},
pages = {91--96},
publisher = {Elsevier Masson SAS},
title = {{Detection and Analysis of Emotion from Speech Signals}},
url = {http://dx.doi.org/10.1016/j.procs.2015.08.032},
volume = {58},
year = {2015}
}
@article{Smith1975,
abstract = {Using the voices of six subjects, representing various social and educational backgrounds, fifty-four synthetic voices were generated by computer. Each normal voice was both increased and decreased in rate by 121/2, 25, 371/2, and 50 per cent. Judges evaluated the fifty-four voices using a series of adjectives representing two major personality factors of “competence” and “benevolence”. Several statistical analyses were performed, and it was found that the competence factor was much more sensitive to rate manipulations than was the benevolence factor. Ratings of competence were found to increase as rate increases and decrease as rate decreases, in a linear fashion. Benevolence had an inverted U-relationship with speech rate; the highest benevolence ratings occurred with normal speech rate. {\textcopyright} 1975, Sage Publications. All rights reserved.},
author = {Smith, Bruce L. and Brown, Bruce L. and Strong, William J. and Rencher, Alvin C.},
doi = {10.1177/002383097501800203},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/EffectsofSpeechRateonPersonalityPerception.pdf:pdf},
issn = {00238309},
journal = {Language and Speech},
number = {2},
pages = {145--152},
title = {{Effects of speech rate on personality perception}},
volume = {18},
year = {1975}
}
@article{Anggraeni2018,
abstract = {In this paper describe an implementation of speech recognition to pick and place an object using Robot Arm. To get the feature extraction of speech signal used Mel-Frequency Cepstrum Coefficients (MFCC) method and to learn the database of speech recognition used Support Vector Machine (SVM) method, the algorithm based on Python 2.7. The data learning which used to SVM process are 12 features, then the system tested using trained and not trained data show the best agreement to identifying the speech recognition. The speech recognition system has been implemented for control the 5 DoF Robot Arm based Arduino microcontroller to doing task pick and place the object.},
author = {Anggraeni, D. and Sanjaya, W. S.M. and Nurasyidiek, M. Y.S. and Munawwaroh, M.},
doi = {10.1088/1757-899X/288/1/012042},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Anggraeni{\_}2018{\_}IOP{\_}Conf.{\_}Ser.{\_}{\_}Mater.{\_}Sci.{\_}Eng.{\_}288{\_}012042.pdf:pdf},
issn = {1757899X},
journal = {IOP Conference Series: Materials Science and Engineering},
number = {1},
title = {{The Implementation of Speech Recognition using Mel-Frequency Cepstrum Coefficients (MFCC) and Support Vector Machine (SVM) method based on Python to Control Robot Arm}},
volume = {288},
year = {2018}
}
@article{Processing2015,
author = {Processing, Social Signal},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Emotion{\_}recognition{\_}through{\_}voice{\_}analys.pdf:pdf},
title = {{Research paper Social Signal Processing !}},
year = {2015}
}
@article{Dasgupta2017,
abstract = {The ability to modulate vocal sounds and generate speech is one of the features which set humans apart from other living beings. The human voice can be characterized by several attributes such as pitch, timbre, loudness, and vocal tone. It has often been observed that humans express their emotions by varying different vocal attributes during speech generation. Hence, deduction of human emotions through voice and speech analysis has a practical plausibility and could potentially be beneficial for improving human conversational and persuasion skills. This paper presents an algorithmic approach for detection and analysis of human emotions with the help of voice and speech processing. The proposed approach has been developed with the objective of incorporation with futuristic artificial intelligence systems for improving human-computer interactions.},
author = {Dasgupta, Poorna Banerjee},
doi = {10.14445/22312803/ijctt-v52p101},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/1710.10198.pdf:pdf},
journal = {International Journal of Computer Trends and Technology},
keywords = {artificial intelligence,emotions,human,speech processing,voice processing},
number = {1},
pages = {1--3},
title = {{Detection and Analysis of Human Emotions through Voice and Speech Pattern Processing}},
volume = {52},
year = {2017}
}
@article{Pell2008,
abstract = {To test ideas about the universality and time course of vocal emotion processing, 50 English listeners performed an emotional priming task to determine whether they implicitly recognize emotional meanings of prosody when exposed to a foreign language. Arabic pseudo-utterances produced in a happy, sad, or neutral prosody acted as primes for a happy, sad, or 'false' (i.e., non-emotional) face target and participants judged whether the facial expression represents an emotion. The prosody-face relationship (congruent, incongruent) and the prosody duration (600 or 1000 ms) were independently manipulated in the same experiment. Results indicated that English listeners automatically detect the emotional significance of prosody when expressed in a foreign language, although activation of emotional meanings in a foreign language may require increased exposure to prosodic information than when listening to the native language. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Pell, Marc D. and Skorup, Vera},
doi = {10.1016/j.specom.2008.03.006},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/Implicit{\_}processing{\_}of{\_}emotional{\_}prosody.pdf:pdf},
issn = {01676393},
journal = {Speech Communication},
keywords = {Cross-linguistic,Cultural factors,Semantic priming,Speech processing,Vocal expression},
number = {6},
pages = {519--530},
title = {{Implicit processing of emotional prosody in a foreign versus native language}},
volume = {50},
year = {2008}
}
@article{Juslin2003,
abstract = {Many authors have speculated about a close relationship between vocal expression of emotions and musical expression of emotions, but evidence bearing on this relationship has unfortunately been, lacking. This review of 104 studies of vocal expression and 41 studies of music performance reveals similarities between the 2 channels concerning (a) the accuracy with which discrete emotions were communicated to listeners and (b) the emotion-specific patterns of acoustic cues used to communicate each emotion. The patterns are generally consistent with K. R. Scherer's (1986) theoretical predictions. The results can explain why music is perceived as expressive of emotion, and they are consistent with an evolutionary perspective on vocal expression of emotions. Discussion focuses on theoretical accounts and directions for future research.},
author = {Juslin, Patrik N. and Laukka, Petri},
doi = {10.1037/0033-2909.129.5.770},
file = {:C$\backslash$:/Users/Usuario/Documents/Articulos Latex/Articulos/c{\_}510552-l{\_}1-k{\_}juslin{\_}emotion2003.pdf:pdf},
issn = {00332909},
journal = {Psychological Bulletin},
number = {5},
pages = {770--814},
title = {{Communication of Emotions in Vocal Expression and Music Performance: Different Channels, Same Code?}},
volume = {129},
year = {2003}
}


