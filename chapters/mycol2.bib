@article{Langari2020,
abstract = {One of the most important issues in human-computer interaction is to create a system that can hear and respond correctly like a human. This has led to the design of the Automatic Speech Emotion Recognition system (SER) that is able to identify different emotional classes by extracting and selecting effective features from speech signals. For this reason, in this study, we propose a novel feature extraction method based on adaptive time-frequency coefficients to improve the SER. The simulations are performed using the Berlin Emotional Speech Database (EMO-DB), the Surrey Audio-Visual Expressed Emotion Database (SAVEE), and the Persian Drama Radio Emotional Corpus (PDREC). The main contribution of our work is to extract novel features, called Adaptive Time-Frequency features, based on the Fractional Fourier Transform and to combine them with Cepstral features. Experimental results show that the proposed method effectively identifies different emotional classes in EMO-DB (97.57{\%} accuracy), SAVEE (80{\%} accuracy), and PDREC (91.46{\%} accuracy) data sets.},
author = {Langari, Shadi and Marvi, Hossein and Zahedi, Morteza},
doi = {10.1016/j.imu.2020.100424},
file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Langari, Marvi, Zahedi - 2020 - Efficient speech emotion recognition using modified feature extraction.pdf:pdf},
issn = {23529148},
journal = {Informatics in Medicine Unlocked},
keywords = {Evolutionary algorithms,Feature extraction,Feature selection,Human-computer interaction,Speech emotion recognition},
pages = {100424},
publisher = {Elsevier Ltd},
title = {{Efficient speech emotion recognition using modified feature extraction}},
url = {https://doi.org/10.1016/j.imu.2020.100424},
volume = {20},
year = {2020}
}
@article{Hellbernd2016,
	abstract = {Action-theoretic views of language posit that the recognition of others' intentions is key to successful interpersonal communication. Yet, speakers do not always code their intentions literally, raising the question of which mechanisms enable interlocutors to exchange communicative intents. The present study investigated whether and how prosody-the vocal tone-contributes to the identification of "unspoken" intentions. Single (non-)words were spoken with six intonations representing different speech acts-as carriers of communicative intentions. This corpus was acoustically analyzed (Experiment 1), and behaviorally evaluated in two experiments (Experiments 2 and 3). The combined results show characteristic prosodic feature configurations for different intentions that were reliably recognized by listeners. Interestingly, identification of intentions was not contingent on context (single words), lexical information (non-words), and recognition of the speaker's emotion (valence and arousal). Overall, the data demonstrate that speakers' intentions are represented in the prosodic signal which can, thus, determine the success of interpersonal communication.},
	author = {Hellbernd, Nele and Sammler, Daniela},
	doi = {10.1016/j.jml.2016.01.001},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hellbernd, Sammler - 2016 - Prosody conveys speaker's intentions Acoustic cues for speech act perception.pdf:pdf},
	issn = {0749596X},
	journal = {Journal of Memory and Language},
	keywords = {Acoustics,Intention,Pragmatics,Prosody,Speech acts},
	pages = {70--86},
	title = {{Prosody conveys speaker's intentions: Acoustic cues for speech act perception}},
	volume = {88},
	year = {2016}
}
@incollection{Rashid2018,
	abstract = {Speech is a complex naturally acquired human motor ability. It is characterized in adults with the production of about 14 different sounds per second via the harmonized actions of roughly 100 muscles. Speaker recognition is the capability of a software or hardware to receive speech signal, identify the speaker present in the speech signal and recognize the speaker afterwards. Feature extraction is accomplished by changing the speech waveform to a form of parametric representation at a relatively minimized data rate for subsequent processing and analysis. Therefore, acceptable classification is derived from excellent and quality features. Mel Frequency Cepstral Coefficients (MFCC), Linear Prediction Coeffi- cients (LPC), Linear Prediction Cepstral Coefficients (LPCC), Line Spectral Frequencies (LSF), Discrete Wavelet Transform (DWT) and Perceptual Linear Prediction (PLP) are the speech feature extraction techniques that were discussed in these chapter. These methods have been tested in a wide variety of applications, giving them high level of reliability and acceptability. Researchers have made several modifications to the above discussed tech- niques to make them less susceptible to noise, more robust and consume less time. In conclusion, none of the methods is superior to the other, the area of application would determine which method to select.},
	author = {Rashid, Sabur Ajibola Alim and Nahrul Khair Alang},
	booktitle = {From Natural to Artificial Intelligence: Algorithms and Applications},
	chapter = {Chapter 1},
	file = {:C$\backslash$:/Users/Usuario/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rashid - 2018 - Some Commonly Used Speech Feature Extraction Algorithms.pdf:pdf},
	keywords = {discrete wavelet transform (DWT),human speech,line spectral frequencies (LSF),linear prediction cepstral coefficients (LPCC),linear prediction coefficients (LPC),mel frequency cepstral coefficients (MFCC),perceptual linear prediction (PLP),speech features},
	number = {tourism},
	pages = {13},
	title = {{Some Commonly Used Speech Feature Extraction Algorithms}},
	url = {https://www.intechopen.com/books/advanced-biometric-technologies/liveness-detection-in-biometrics},
	year = {2018}
}

